/*! For license information please see main.ad437247.js.LICENSE.txt */
(()=>{"use strict";var e={730:(e,t,n)=>{var a=n(43),i=n(853);function o(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var s=new Set,r={};function l(e,t){c(e,t),c(e+"Capture",t)}function c(e,t){for(r[e]=t,e=0;e<t.length;e++)s.add(t[e])}var d=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),u=Object.prototype.hasOwnProperty,p=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,m={},g={};function h(e,t,n,a,i,o,s){this.acceptsBooleans=2===t||3===t||4===t,this.attributeName=a,this.attributeNamespace=i,this.mustUseProperty=n,this.propertyName=e,this.type=t,this.sanitizeURL=o,this.removeEmptyString=s}var f={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach((function(e){f[e]=new h(e,0,!1,e,null,!1,!1)})),[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach((function(e){var t=e[0];f[t]=new h(t,1,!1,e[1],null,!1,!1)})),["contentEditable","draggable","spellCheck","value"].forEach((function(e){f[e]=new h(e,2,!1,e.toLowerCase(),null,!1,!1)})),["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach((function(e){f[e]=new h(e,2,!1,e,null,!1,!1)})),"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture disableRemotePlayback formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach((function(e){f[e]=new h(e,3,!1,e.toLowerCase(),null,!1,!1)})),["checked","multiple","muted","selected"].forEach((function(e){f[e]=new h(e,3,!0,e,null,!1,!1)})),["capture","download"].forEach((function(e){f[e]=new h(e,4,!1,e,null,!1,!1)})),["cols","rows","size","span"].forEach((function(e){f[e]=new h(e,6,!1,e,null,!1,!1)})),["rowSpan","start"].forEach((function(e){f[e]=new h(e,5,!1,e.toLowerCase(),null,!1,!1)}));var y=/[\-:]([a-z])/g;function v(e){return e[1].toUpperCase()}function b(e,t,n,a){var i=f.hasOwnProperty(t)?f[t]:null;(null!==i?0!==i.type:a||!(2<t.length)||"o"!==t[0]&&"O"!==t[0]||"n"!==t[1]&&"N"!==t[1])&&(function(e,t,n,a){if(null===t||"undefined"===typeof t||function(e,t,n,a){if(null!==n&&0===n.type)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return!a&&(null!==n?!n.acceptsBooleans:"data-"!==(e=e.toLowerCase().slice(0,5))&&"aria-"!==e);default:return!1}}(e,t,n,a))return!0;if(a)return!1;if(null!==n)switch(n.type){case 3:return!t;case 4:return!1===t;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}(t,n,i,a)&&(n=null),a||null===i?function(e){return!!u.call(g,e)||!u.call(m,e)&&(p.test(e)?g[e]=!0:(m[e]=!0,!1))}(t)&&(null===n?e.removeAttribute(t):e.setAttribute(t,""+n)):i.mustUseProperty?e[i.propertyName]=null===n?3!==i.type&&"":n:(t=i.attributeName,a=i.attributeNamespace,null===n?e.removeAttribute(t):(n=3===(i=i.type)||4===i&&!0===n?"":""+n,a?e.setAttributeNS(a,t,n):e.setAttribute(t,n))))}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach((function(e){var t=e.replace(y,v);f[t]=new h(t,1,!1,e,null,!1,!1)})),"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach((function(e){var t=e.replace(y,v);f[t]=new h(t,1,!1,e,"http://www.w3.org/1999/xlink",!1,!1)})),["xml:base","xml:lang","xml:space"].forEach((function(e){var t=e.replace(y,v);f[t]=new h(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1,!1)})),["tabIndex","crossOrigin"].forEach((function(e){f[e]=new h(e,1,!1,e.toLowerCase(),null,!1,!1)})),f.xlinkHref=new h("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0,!1),["src","href","action","formAction"].forEach((function(e){f[e]=new h(e,1,!1,e.toLowerCase(),null,!0,!0)}));var A=a.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,I=Symbol.for("react.element"),w=Symbol.for("react.portal"),k=Symbol.for("react.fragment"),_=Symbol.for("react.strict_mode"),S=Symbol.for("react.profiler"),x=Symbol.for("react.provider"),C=Symbol.for("react.context"),P=Symbol.for("react.forward_ref"),E=Symbol.for("react.suspense"),T=Symbol.for("react.suspense_list"),R=Symbol.for("react.memo"),z=Symbol.for("react.lazy");Symbol.for("react.scope"),Symbol.for("react.debug_trace_mode");var D=Symbol.for("react.offscreen");Symbol.for("react.legacy_hidden"),Symbol.for("react.cache"),Symbol.for("react.tracing_marker");var q=Symbol.iterator;function M(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=q&&e[q]||e["@@iterator"])?e:null}var G,L=Object.assign;function F(e){if(void 0===G)try{throw Error()}catch(n){var t=n.stack.trim().match(/\n( *(at )?)/);G=t&&t[1]||""}return"\n"+G+e}var N=!1;function U(e,t){if(!e||N)return"";N=!0;var n=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{if(t)if(t=function(){throw Error()},Object.defineProperty(t.prototype,"props",{set:function(){throw Error()}}),"object"===typeof Reflect&&Reflect.construct){try{Reflect.construct(t,[])}catch(c){var a=c}Reflect.construct(e,[],t)}else{try{t.call()}catch(c){a=c}e.call(t.prototype)}else{try{throw Error()}catch(c){a=c}e()}}catch(c){if(c&&a&&"string"===typeof c.stack){for(var i=c.stack.split("\n"),o=a.stack.split("\n"),s=i.length-1,r=o.length-1;1<=s&&0<=r&&i[s]!==o[r];)r--;for(;1<=s&&0<=r;s--,r--)if(i[s]!==o[r]){if(1!==s||1!==r)do{if(s--,0>--r||i[s]!==o[r]){var l="\n"+i[s].replace(" at new "," at ");return e.displayName&&l.includes("<anonymous>")&&(l=l.replace("<anonymous>",e.displayName)),l}}while(1<=s&&0<=r);break}}}finally{N=!1,Error.prepareStackTrace=n}return(e=e?e.displayName||e.name:"")?F(e):""}function O(e){switch(e.tag){case 5:return F(e.type);case 16:return F("Lazy");case 13:return F("Suspense");case 19:return F("SuspenseList");case 0:case 2:case 15:return e=U(e.type,!1);case 11:return e=U(e.type.render,!1);case 1:return e=U(e.type,!0);default:return""}}function H(e){if(null==e)return null;if("function"===typeof e)return e.displayName||e.name||null;if("string"===typeof e)return e;switch(e){case k:return"Fragment";case w:return"Portal";case S:return"Profiler";case _:return"StrictMode";case E:return"Suspense";case T:return"SuspenseList"}if("object"===typeof e)switch(e.$$typeof){case C:return(e.displayName||"Context")+".Consumer";case x:return(e._context.displayName||"Context")+".Provider";case P:var t=e.render;return(e=e.displayName)||(e=""!==(e=t.displayName||t.name||"")?"ForwardRef("+e+")":"ForwardRef"),e;case R:return null!==(t=e.displayName||null)?t:H(e.type)||"Memo";case z:t=e._payload,e=e._init;try{return H(e(t))}catch(n){}}return null}function B(e){var t=e.type;switch(e.tag){case 24:return"Cache";case 9:return(t.displayName||"Context")+".Consumer";case 10:return(t._context.displayName||"Context")+".Provider";case 18:return"DehydratedFragment";case 11:return e=(e=t.render).displayName||e.name||"",t.displayName||(""!==e?"ForwardRef("+e+")":"ForwardRef");case 7:return"Fragment";case 5:return t;case 4:return"Portal";case 3:return"Root";case 6:return"Text";case 16:return H(t);case 8:return t===_?"StrictMode":"Mode";case 22:return"Offscreen";case 12:return"Profiler";case 21:return"Scope";case 13:return"Suspense";case 19:return"SuspenseList";case 25:return"TracingMarker";case 1:case 0:case 17:case 2:case 14:case 15:if("function"===typeof t)return t.displayName||t.name||null;if("string"===typeof t)return t}return null}function W(e){switch(typeof e){case"boolean":case"number":case"string":case"undefined":case"object":return e;default:return""}}function j(e){var t=e.type;return(e=e.nodeName)&&"input"===e.toLowerCase()&&("checkbox"===t||"radio"===t)}function V(e){e._valueTracker||(e._valueTracker=function(e){var t=j(e)?"checked":"value",n=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),a=""+e[t];if(!e.hasOwnProperty(t)&&"undefined"!==typeof n&&"function"===typeof n.get&&"function"===typeof n.set){var i=n.get,o=n.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return i.call(this)},set:function(e){a=""+e,o.call(this,e)}}),Object.defineProperty(e,t,{enumerable:n.enumerable}),{getValue:function(){return a},setValue:function(e){a=""+e},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}(e))}function $(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var n=t.getValue(),a="";return e&&(a=j(e)?e.checked?"true":"false":e.value),(e=a)!==n&&(t.setValue(e),!0)}function K(e){if("undefined"===typeof(e=e||("undefined"!==typeof document?document:void 0)))return null;try{return e.activeElement||e.body}catch(t){return e.body}}function Q(e,t){var n=t.checked;return L({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=n?n:e._wrapperState.initialChecked})}function J(e,t){var n=null==t.defaultValue?"":t.defaultValue,a=null!=t.checked?t.checked:t.defaultChecked;n=W(null!=t.value?t.value:n),e._wrapperState={initialChecked:a,initialValue:n,controlled:"checkbox"===t.type||"radio"===t.type?null!=t.checked:null!=t.value}}function X(e,t){null!=(t=t.checked)&&b(e,"checked",t,!1)}function Y(e,t){X(e,t);var n=W(t.value),a=t.type;if(null!=n)"number"===a?(0===n&&""===e.value||e.value!=n)&&(e.value=""+n):e.value!==""+n&&(e.value=""+n);else if("submit"===a||"reset"===a)return void e.removeAttribute("value");t.hasOwnProperty("value")?ee(e,t.type,n):t.hasOwnProperty("defaultValue")&&ee(e,t.type,W(t.defaultValue)),null==t.checked&&null!=t.defaultChecked&&(e.defaultChecked=!!t.defaultChecked)}function Z(e,t,n){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var a=t.type;if(!("submit"!==a&&"reset"!==a||void 0!==t.value&&null!==t.value))return;t=""+e._wrapperState.initialValue,n||t===e.value||(e.value=t),e.defaultValue=t}""!==(n=e.name)&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,""!==n&&(e.name=n)}function ee(e,t,n){"number"===t&&K(e.ownerDocument)===e||(null==n?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+n&&(e.defaultValue=""+n))}var te=Array.isArray;function ne(e,t,n,a){if(e=e.options,t){t={};for(var i=0;i<n.length;i++)t["$"+n[i]]=!0;for(n=0;n<e.length;n++)i=t.hasOwnProperty("$"+e[n].value),e[n].selected!==i&&(e[n].selected=i),i&&a&&(e[n].defaultSelected=!0)}else{for(n=""+W(n),t=null,i=0;i<e.length;i++){if(e[i].value===n)return e[i].selected=!0,void(a&&(e[i].defaultSelected=!0));null!==t||e[i].disabled||(t=e[i])}null!==t&&(t.selected=!0)}}function ae(e,t){if(null!=t.dangerouslySetInnerHTML)throw Error(o(91));return L({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function ie(e,t){var n=t.value;if(null==n){if(n=t.children,t=t.defaultValue,null!=n){if(null!=t)throw Error(o(92));if(te(n)){if(1<n.length)throw Error(o(93));n=n[0]}t=n}null==t&&(t=""),n=t}e._wrapperState={initialValue:W(n)}}function oe(e,t){var n=W(t.value),a=W(t.defaultValue);null!=n&&((n=""+n)!==e.value&&(e.value=n),null==t.defaultValue&&e.defaultValue!==n&&(e.defaultValue=n)),null!=a&&(e.defaultValue=""+a)}function se(e){var t=e.textContent;t===e._wrapperState.initialValue&&""!==t&&null!==t&&(e.value=t)}function re(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function le(e,t){return null==e||"http://www.w3.org/1999/xhtml"===e?re(t):"http://www.w3.org/2000/svg"===e&&"foreignObject"===t?"http://www.w3.org/1999/xhtml":e}var ce,de,ue=(de=function(e,t){if("http://www.w3.org/2000/svg"!==e.namespaceURI||"innerHTML"in e)e.innerHTML=t;else{for((ce=ce||document.createElement("div")).innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=ce.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}},"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(e,t,n,a){MSApp.execUnsafeLocalFunction((function(){return de(e,t)}))}:de);function pe(e,t){if(t){var n=e.firstChild;if(n&&n===e.lastChild&&3===n.nodeType)return void(n.nodeValue=t)}e.textContent=t}var me={animationIterationCount:!0,aspectRatio:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},ge=["Webkit","ms","Moz","O"];function he(e,t,n){return null==t||"boolean"===typeof t||""===t?"":n||"number"!==typeof t||0===t||me.hasOwnProperty(e)&&me[e]?(""+t).trim():t+"px"}function fe(e,t){for(var n in e=e.style,t)if(t.hasOwnProperty(n)){var a=0===n.indexOf("--"),i=he(n,t[n],a);"float"===n&&(n="cssFloat"),a?e.setProperty(n,i):e[n]=i}}Object.keys(me).forEach((function(e){ge.forEach((function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),me[t]=me[e]}))}));var ye=L({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function ve(e,t){if(t){if(ye[e]&&(null!=t.children||null!=t.dangerouslySetInnerHTML))throw Error(o(137,e));if(null!=t.dangerouslySetInnerHTML){if(null!=t.children)throw Error(o(60));if("object"!==typeof t.dangerouslySetInnerHTML||!("__html"in t.dangerouslySetInnerHTML))throw Error(o(61))}if(null!=t.style&&"object"!==typeof t.style)throw Error(o(62))}}function be(e,t){if(-1===e.indexOf("-"))return"string"===typeof t.is;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var Ae=null;function Ie(e){return(e=e.target||e.srcElement||window).correspondingUseElement&&(e=e.correspondingUseElement),3===e.nodeType?e.parentNode:e}var we=null,ke=null,_e=null;function Se(e){if(e=bi(e)){if("function"!==typeof we)throw Error(o(280));var t=e.stateNode;t&&(t=Ii(t),we(e.stateNode,e.type,t))}}function xe(e){ke?_e?_e.push(e):_e=[e]:ke=e}function Ce(){if(ke){var e=ke,t=_e;if(_e=ke=null,Se(e),t)for(e=0;e<t.length;e++)Se(t[e])}}function Pe(e,t){return e(t)}function Ee(){}var Te=!1;function Re(e,t,n){if(Te)return e(t,n);Te=!0;try{return Pe(e,t,n)}finally{Te=!1,(null!==ke||null!==_e)&&(Ee(),Ce())}}function ze(e,t){var n=e.stateNode;if(null===n)return null;var a=Ii(n);if(null===a)return null;n=a[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(a=!a.disabled)||(a=!("button"===(e=e.type)||"input"===e||"select"===e||"textarea"===e)),e=!a;break e;default:e=!1}if(e)return null;if(n&&"function"!==typeof n)throw Error(o(231,t,typeof n));return n}var De=!1;if(d)try{var qe={};Object.defineProperty(qe,"passive",{get:function(){De=!0}}),window.addEventListener("test",qe,qe),window.removeEventListener("test",qe,qe)}catch(de){De=!1}function Me(e,t,n,a,i,o,s,r,l){var c=Array.prototype.slice.call(arguments,3);try{t.apply(n,c)}catch(d){this.onError(d)}}var Ge=!1,Le=null,Fe=!1,Ne=null,Ue={onError:function(e){Ge=!0,Le=e}};function Oe(e,t,n,a,i,o,s,r,l){Ge=!1,Le=null,Me.apply(Ue,arguments)}function He(e){var t=e,n=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do{0!==(4098&(t=e).flags)&&(n=t.return),e=t.return}while(e)}return 3===t.tag?n:null}function Be(e){if(13===e.tag){var t=e.memoizedState;if(null===t&&(null!==(e=e.alternate)&&(t=e.memoizedState)),null!==t)return t.dehydrated}return null}function We(e){if(He(e)!==e)throw Error(o(188))}function je(e){return null!==(e=function(e){var t=e.alternate;if(!t){if(null===(t=He(e)))throw Error(o(188));return t!==e?null:e}for(var n=e,a=t;;){var i=n.return;if(null===i)break;var s=i.alternate;if(null===s){if(null!==(a=i.return)){n=a;continue}break}if(i.child===s.child){for(s=i.child;s;){if(s===n)return We(i),e;if(s===a)return We(i),t;s=s.sibling}throw Error(o(188))}if(n.return!==a.return)n=i,a=s;else{for(var r=!1,l=i.child;l;){if(l===n){r=!0,n=i,a=s;break}if(l===a){r=!0,a=i,n=s;break}l=l.sibling}if(!r){for(l=s.child;l;){if(l===n){r=!0,n=s,a=i;break}if(l===a){r=!0,a=s,n=i;break}l=l.sibling}if(!r)throw Error(o(189))}}if(n.alternate!==a)throw Error(o(190))}if(3!==n.tag)throw Error(o(188));return n.stateNode.current===n?e:t}(e))?Ve(e):null}function Ve(e){if(5===e.tag||6===e.tag)return e;for(e=e.child;null!==e;){var t=Ve(e);if(null!==t)return t;e=e.sibling}return null}var $e=i.unstable_scheduleCallback,Ke=i.unstable_cancelCallback,Qe=i.unstable_shouldYield,Je=i.unstable_requestPaint,Xe=i.unstable_now,Ye=i.unstable_getCurrentPriorityLevel,Ze=i.unstable_ImmediatePriority,et=i.unstable_UserBlockingPriority,tt=i.unstable_NormalPriority,nt=i.unstable_LowPriority,at=i.unstable_IdlePriority,it=null,ot=null;var st=Math.clz32?Math.clz32:function(e){return e>>>=0,0===e?32:31-(rt(e)/lt|0)|0},rt=Math.log,lt=Math.LN2;var ct=64,dt=4194304;function ut(e){switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return 4194240&e;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return 130023424&e;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 1073741824;default:return e}}function pt(e,t){var n=e.pendingLanes;if(0===n)return 0;var a=0,i=e.suspendedLanes,o=e.pingedLanes,s=268435455&n;if(0!==s){var r=s&~i;0!==r?a=ut(r):0!==(o&=s)&&(a=ut(o))}else 0!==(s=n&~i)?a=ut(s):0!==o&&(a=ut(o));if(0===a)return 0;if(0!==t&&t!==a&&0===(t&i)&&((i=a&-a)>=(o=t&-t)||16===i&&0!==(4194240&o)))return t;if(0!==(4&a)&&(a|=16&n),0!==(t=e.entangledLanes))for(e=e.entanglements,t&=a;0<t;)i=1<<(n=31-st(t)),a|=e[n],t&=~i;return a}function mt(e,t){switch(e){case 1:case 2:case 4:return t+250;case 8:case 16:case 32:case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;default:return-1}}function gt(e){return 0!==(e=-1073741825&e.pendingLanes)?e:1073741824&e?1073741824:0}function ht(){var e=ct;return 0===(4194240&(ct<<=1))&&(ct=64),e}function ft(e){for(var t=[],n=0;31>n;n++)t.push(e);return t}function yt(e,t,n){e.pendingLanes|=t,536870912!==t&&(e.suspendedLanes=0,e.pingedLanes=0),(e=e.eventTimes)[t=31-st(t)]=n}function vt(e,t){var n=e.entangledLanes|=t;for(e=e.entanglements;n;){var a=31-st(n),i=1<<a;i&t|e[a]&t&&(e[a]|=t),n&=~i}}var bt=0;function At(e){return 1<(e&=-e)?4<e?0!==(268435455&e)?16:536870912:4:1}var It,wt,kt,_t,St,xt=!1,Ct=[],Pt=null,Et=null,Tt=null,Rt=new Map,zt=new Map,Dt=[],qt="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset submit".split(" ");function Mt(e,t){switch(e){case"focusin":case"focusout":Pt=null;break;case"dragenter":case"dragleave":Et=null;break;case"mouseover":case"mouseout":Tt=null;break;case"pointerover":case"pointerout":Rt.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":zt.delete(t.pointerId)}}function Gt(e,t,n,a,i,o){return null===e||e.nativeEvent!==o?(e={blockedOn:t,domEventName:n,eventSystemFlags:a,nativeEvent:o,targetContainers:[i]},null!==t&&(null!==(t=bi(t))&&wt(t)),e):(e.eventSystemFlags|=a,t=e.targetContainers,null!==i&&-1===t.indexOf(i)&&t.push(i),e)}function Lt(e){var t=vi(e.target);if(null!==t){var n=He(t);if(null!==n)if(13===(t=n.tag)){if(null!==(t=Be(n)))return e.blockedOn=t,void St(e.priority,(function(){kt(n)}))}else if(3===t&&n.stateNode.current.memoizedState.isDehydrated)return void(e.blockedOn=3===n.tag?n.stateNode.containerInfo:null)}e.blockedOn=null}function Ft(e){if(null!==e.blockedOn)return!1;for(var t=e.targetContainers;0<t.length;){var n=Qt(e.domEventName,e.eventSystemFlags,t[0],e.nativeEvent);if(null!==n)return null!==(t=bi(n))&&wt(t),e.blockedOn=n,!1;var a=new(n=e.nativeEvent).constructor(n.type,n);Ae=a,n.target.dispatchEvent(a),Ae=null,t.shift()}return!0}function Nt(e,t,n){Ft(e)&&n.delete(t)}function Ut(){xt=!1,null!==Pt&&Ft(Pt)&&(Pt=null),null!==Et&&Ft(Et)&&(Et=null),null!==Tt&&Ft(Tt)&&(Tt=null),Rt.forEach(Nt),zt.forEach(Nt)}function Ot(e,t){e.blockedOn===t&&(e.blockedOn=null,xt||(xt=!0,i.unstable_scheduleCallback(i.unstable_NormalPriority,Ut)))}function Ht(e){function t(t){return Ot(t,e)}if(0<Ct.length){Ot(Ct[0],e);for(var n=1;n<Ct.length;n++){var a=Ct[n];a.blockedOn===e&&(a.blockedOn=null)}}for(null!==Pt&&Ot(Pt,e),null!==Et&&Ot(Et,e),null!==Tt&&Ot(Tt,e),Rt.forEach(t),zt.forEach(t),n=0;n<Dt.length;n++)(a=Dt[n]).blockedOn===e&&(a.blockedOn=null);for(;0<Dt.length&&null===(n=Dt[0]).blockedOn;)Lt(n),null===n.blockedOn&&Dt.shift()}var Bt=A.ReactCurrentBatchConfig,Wt=!0;function jt(e,t,n,a){var i=bt,o=Bt.transition;Bt.transition=null;try{bt=1,$t(e,t,n,a)}finally{bt=i,Bt.transition=o}}function Vt(e,t,n,a){var i=bt,o=Bt.transition;Bt.transition=null;try{bt=4,$t(e,t,n,a)}finally{bt=i,Bt.transition=o}}function $t(e,t,n,a){if(Wt){var i=Qt(e,t,n,a);if(null===i)Wa(e,t,a,Kt,n),Mt(e,a);else if(function(e,t,n,a,i){switch(t){case"focusin":return Pt=Gt(Pt,e,t,n,a,i),!0;case"dragenter":return Et=Gt(Et,e,t,n,a,i),!0;case"mouseover":return Tt=Gt(Tt,e,t,n,a,i),!0;case"pointerover":var o=i.pointerId;return Rt.set(o,Gt(Rt.get(o)||null,e,t,n,a,i)),!0;case"gotpointercapture":return o=i.pointerId,zt.set(o,Gt(zt.get(o)||null,e,t,n,a,i)),!0}return!1}(i,e,t,n,a))a.stopPropagation();else if(Mt(e,a),4&t&&-1<qt.indexOf(e)){for(;null!==i;){var o=bi(i);if(null!==o&&It(o),null===(o=Qt(e,t,n,a))&&Wa(e,t,a,Kt,n),o===i)break;i=o}null!==i&&a.stopPropagation()}else Wa(e,t,a,null,n)}}var Kt=null;function Qt(e,t,n,a){if(Kt=null,null!==(e=vi(e=Ie(a))))if(null===(t=He(e)))e=null;else if(13===(n=t.tag)){if(null!==(e=Be(t)))return e;e=null}else if(3===n){if(t.stateNode.current.memoizedState.isDehydrated)return 3===t.tag?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null);return Kt=e,null}function Jt(e){switch(e){case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 1;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"toggle":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 4;case"message":switch(Ye()){case Ze:return 1;case et:return 4;case tt:case nt:return 16;case at:return 536870912;default:return 16}default:return 16}}var Xt=null,Yt=null,Zt=null;function en(){if(Zt)return Zt;var e,t,n=Yt,a=n.length,i="value"in Xt?Xt.value:Xt.textContent,o=i.length;for(e=0;e<a&&n[e]===i[e];e++);var s=a-e;for(t=1;t<=s&&n[a-t]===i[o-t];t++);return Zt=i.slice(e,1<t?1-t:void 0)}function tn(e){var t=e.keyCode;return"charCode"in e?0===(e=e.charCode)&&13===t&&(e=13):e=t,10===e&&(e=13),32<=e||13===e?e:0}function nn(){return!0}function an(){return!1}function on(e){function t(t,n,a,i,o){for(var s in this._reactName=t,this._targetInst=a,this.type=n,this.nativeEvent=i,this.target=o,this.currentTarget=null,e)e.hasOwnProperty(s)&&(t=e[s],this[s]=t?t(i):i[s]);return this.isDefaultPrevented=(null!=i.defaultPrevented?i.defaultPrevented:!1===i.returnValue)?nn:an,this.isPropagationStopped=an,this}return L(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var e=this.nativeEvent;e&&(e.preventDefault?e.preventDefault():"unknown"!==typeof e.returnValue&&(e.returnValue=!1),this.isDefaultPrevented=nn)},stopPropagation:function(){var e=this.nativeEvent;e&&(e.stopPropagation?e.stopPropagation():"unknown"!==typeof e.cancelBubble&&(e.cancelBubble=!0),this.isPropagationStopped=nn)},persist:function(){},isPersistent:nn}),t}var sn,rn,ln,cn={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},dn=on(cn),un=L({},cn,{view:0,detail:0}),pn=on(un),mn=L({},un,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:Sn,button:0,buttons:0,relatedTarget:function(e){return void 0===e.relatedTarget?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==ln&&(ln&&"mousemove"===e.type?(sn=e.screenX-ln.screenX,rn=e.screenY-ln.screenY):rn=sn=0,ln=e),sn)},movementY:function(e){return"movementY"in e?e.movementY:rn}}),gn=on(mn),hn=on(L({},mn,{dataTransfer:0})),fn=on(L({},un,{relatedTarget:0})),yn=on(L({},cn,{animationName:0,elapsedTime:0,pseudoElement:0})),vn=L({},cn,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),bn=on(vn),An=on(L({},cn,{data:0})),In={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},wn={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},kn={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function _n(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):!!(e=kn[e])&&!!t[e]}function Sn(){return _n}var xn=L({},un,{key:function(e){if(e.key){var t=In[e.key]||e.key;if("Unidentified"!==t)return t}return"keypress"===e.type?13===(e=tn(e))?"Enter":String.fromCharCode(e):"keydown"===e.type||"keyup"===e.type?wn[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:Sn,charCode:function(e){return"keypress"===e.type?tn(e):0},keyCode:function(e){return"keydown"===e.type||"keyup"===e.type?e.keyCode:0},which:function(e){return"keypress"===e.type?tn(e):"keydown"===e.type||"keyup"===e.type?e.keyCode:0}}),Cn=on(xn),Pn=on(L({},mn,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0})),En=on(L({},un,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:Sn})),Tn=on(L({},cn,{propertyName:0,elapsedTime:0,pseudoElement:0})),Rn=L({},mn,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),zn=on(Rn),Dn=[9,13,27,32],qn=d&&"CompositionEvent"in window,Mn=null;d&&"documentMode"in document&&(Mn=document.documentMode);var Gn=d&&"TextEvent"in window&&!Mn,Ln=d&&(!qn||Mn&&8<Mn&&11>=Mn),Fn=String.fromCharCode(32),Nn=!1;function Un(e,t){switch(e){case"keyup":return-1!==Dn.indexOf(t.keyCode);case"keydown":return 229!==t.keyCode;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function On(e){return"object"===typeof(e=e.detail)&&"data"in e?e.data:null}var Hn=!1;var Bn={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function Wn(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return"input"===t?!!Bn[e.type]:"textarea"===t}function jn(e,t,n,a){xe(a),0<(t=Va(t,"onChange")).length&&(n=new dn("onChange","change",null,n,a),e.push({event:n,listeners:t}))}var Vn=null,$n=null;function Kn(e){Fa(e,0)}function Qn(e){if($(Ai(e)))return e}function Jn(e,t){if("change"===e)return t}var Xn=!1;if(d){var Yn;if(d){var Zn="oninput"in document;if(!Zn){var ea=document.createElement("div");ea.setAttribute("oninput","return;"),Zn="function"===typeof ea.oninput}Yn=Zn}else Yn=!1;Xn=Yn&&(!document.documentMode||9<document.documentMode)}function ta(){Vn&&(Vn.detachEvent("onpropertychange",na),$n=Vn=null)}function na(e){if("value"===e.propertyName&&Qn($n)){var t=[];jn(t,$n,e,Ie(e)),Re(Kn,t)}}function aa(e,t,n){"focusin"===e?(ta(),$n=n,(Vn=t).attachEvent("onpropertychange",na)):"focusout"===e&&ta()}function ia(e){if("selectionchange"===e||"keyup"===e||"keydown"===e)return Qn($n)}function oa(e,t){if("click"===e)return Qn(t)}function sa(e,t){if("input"===e||"change"===e)return Qn(t)}var ra="function"===typeof Object.is?Object.is:function(e,t){return e===t&&(0!==e||1/e===1/t)||e!==e&&t!==t};function la(e,t){if(ra(e,t))return!0;if("object"!==typeof e||null===e||"object"!==typeof t||null===t)return!1;var n=Object.keys(e),a=Object.keys(t);if(n.length!==a.length)return!1;for(a=0;a<n.length;a++){var i=n[a];if(!u.call(t,i)||!ra(e[i],t[i]))return!1}return!0}function ca(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function da(e,t){var n,a=ca(e);for(e=0;a;){if(3===a.nodeType){if(n=e+a.textContent.length,e<=t&&n>=t)return{node:a,offset:t-e};e=n}e:{for(;a;){if(a.nextSibling){a=a.nextSibling;break e}a=a.parentNode}a=void 0}a=ca(a)}}function ua(e,t){return!(!e||!t)&&(e===t||(!e||3!==e.nodeType)&&(t&&3===t.nodeType?ua(e,t.parentNode):"contains"in e?e.contains(t):!!e.compareDocumentPosition&&!!(16&e.compareDocumentPosition(t))))}function pa(){for(var e=window,t=K();t instanceof e.HTMLIFrameElement;){try{var n="string"===typeof t.contentWindow.location.href}catch(a){n=!1}if(!n)break;t=K((e=t.contentWindow).document)}return t}function ma(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&("input"===t&&("text"===e.type||"search"===e.type||"tel"===e.type||"url"===e.type||"password"===e.type)||"textarea"===t||"true"===e.contentEditable)}function ga(e){var t=pa(),n=e.focusedElem,a=e.selectionRange;if(t!==n&&n&&n.ownerDocument&&ua(n.ownerDocument.documentElement,n)){if(null!==a&&ma(n))if(t=a.start,void 0===(e=a.end)&&(e=t),"selectionStart"in n)n.selectionStart=t,n.selectionEnd=Math.min(e,n.value.length);else if((e=(t=n.ownerDocument||document)&&t.defaultView||window).getSelection){e=e.getSelection();var i=n.textContent.length,o=Math.min(a.start,i);a=void 0===a.end?o:Math.min(a.end,i),!e.extend&&o>a&&(i=a,a=o,o=i),i=da(n,o);var s=da(n,a);i&&s&&(1!==e.rangeCount||e.anchorNode!==i.node||e.anchorOffset!==i.offset||e.focusNode!==s.node||e.focusOffset!==s.offset)&&((t=t.createRange()).setStart(i.node,i.offset),e.removeAllRanges(),o>a?(e.addRange(t),e.extend(s.node,s.offset)):(t.setEnd(s.node,s.offset),e.addRange(t)))}for(t=[],e=n;e=e.parentNode;)1===e.nodeType&&t.push({element:e,left:e.scrollLeft,top:e.scrollTop});for("function"===typeof n.focus&&n.focus(),n=0;n<t.length;n++)(e=t[n]).element.scrollLeft=e.left,e.element.scrollTop=e.top}}var ha=d&&"documentMode"in document&&11>=document.documentMode,fa=null,ya=null,va=null,ba=!1;function Aa(e,t,n){var a=n.window===n?n.document:9===n.nodeType?n:n.ownerDocument;ba||null==fa||fa!==K(a)||("selectionStart"in(a=fa)&&ma(a)?a={start:a.selectionStart,end:a.selectionEnd}:a={anchorNode:(a=(a.ownerDocument&&a.ownerDocument.defaultView||window).getSelection()).anchorNode,anchorOffset:a.anchorOffset,focusNode:a.focusNode,focusOffset:a.focusOffset},va&&la(va,a)||(va=a,0<(a=Va(ya,"onSelect")).length&&(t=new dn("onSelect","select",null,t,n),e.push({event:t,listeners:a}),t.target=fa)))}function Ia(e,t){var n={};return n[e.toLowerCase()]=t.toLowerCase(),n["Webkit"+e]="webkit"+t,n["Moz"+e]="moz"+t,n}var wa={animationend:Ia("Animation","AnimationEnd"),animationiteration:Ia("Animation","AnimationIteration"),animationstart:Ia("Animation","AnimationStart"),transitionend:Ia("Transition","TransitionEnd")},ka={},_a={};function Sa(e){if(ka[e])return ka[e];if(!wa[e])return e;var t,n=wa[e];for(t in n)if(n.hasOwnProperty(t)&&t in _a)return ka[e]=n[t];return e}d&&(_a=document.createElement("div").style,"AnimationEvent"in window||(delete wa.animationend.animation,delete wa.animationiteration.animation,delete wa.animationstart.animation),"TransitionEvent"in window||delete wa.transitionend.transition);var xa=Sa("animationend"),Ca=Sa("animationiteration"),Pa=Sa("animationstart"),Ea=Sa("transitionend"),Ta=new Map,Ra="abort auxClick cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");function za(e,t){Ta.set(e,t),l(t,[e])}for(var Da=0;Da<Ra.length;Da++){var qa=Ra[Da];za(qa.toLowerCase(),"on"+(qa[0].toUpperCase()+qa.slice(1)))}za(xa,"onAnimationEnd"),za(Ca,"onAnimationIteration"),za(Pa,"onAnimationStart"),za("dblclick","onDoubleClick"),za("focusin","onFocus"),za("focusout","onBlur"),za(Ea,"onTransitionEnd"),c("onMouseEnter",["mouseout","mouseover"]),c("onMouseLeave",["mouseout","mouseover"]),c("onPointerEnter",["pointerout","pointerover"]),c("onPointerLeave",["pointerout","pointerover"]),l("onChange","change click focusin focusout input keydown keyup selectionchange".split(" ")),l("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" ")),l("onBeforeInput",["compositionend","keypress","textInput","paste"]),l("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" ")),l("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" ")),l("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var Ma="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Ga=new Set("cancel close invalid load scroll toggle".split(" ").concat(Ma));function La(e,t,n){var a=e.type||"unknown-event";e.currentTarget=n,function(e,t,n,a,i,s,r,l,c){if(Oe.apply(this,arguments),Ge){if(!Ge)throw Error(o(198));var d=Le;Ge=!1,Le=null,Fe||(Fe=!0,Ne=d)}}(a,t,void 0,e),e.currentTarget=null}function Fa(e,t){t=0!==(4&t);for(var n=0;n<e.length;n++){var a=e[n],i=a.event;a=a.listeners;e:{var o=void 0;if(t)for(var s=a.length-1;0<=s;s--){var r=a[s],l=r.instance,c=r.currentTarget;if(r=r.listener,l!==o&&i.isPropagationStopped())break e;La(i,r,c),o=l}else for(s=0;s<a.length;s++){if(l=(r=a[s]).instance,c=r.currentTarget,r=r.listener,l!==o&&i.isPropagationStopped())break e;La(i,r,c),o=l}}}if(Fe)throw e=Ne,Fe=!1,Ne=null,e}function Na(e,t){var n=t[hi];void 0===n&&(n=t[hi]=new Set);var a=e+"__bubble";n.has(a)||(Ba(t,e,2,!1),n.add(a))}function Ua(e,t,n){var a=0;t&&(a|=4),Ba(n,e,a,t)}var Oa="_reactListening"+Math.random().toString(36).slice(2);function Ha(e){if(!e[Oa]){e[Oa]=!0,s.forEach((function(t){"selectionchange"!==t&&(Ga.has(t)||Ua(t,!1,e),Ua(t,!0,e))}));var t=9===e.nodeType?e:e.ownerDocument;null===t||t[Oa]||(t[Oa]=!0,Ua("selectionchange",!1,t))}}function Ba(e,t,n,a){switch(Jt(t)){case 1:var i=jt;break;case 4:i=Vt;break;default:i=$t}n=i.bind(null,t,n,e),i=void 0,!De||"touchstart"!==t&&"touchmove"!==t&&"wheel"!==t||(i=!0),a?void 0!==i?e.addEventListener(t,n,{capture:!0,passive:i}):e.addEventListener(t,n,!0):void 0!==i?e.addEventListener(t,n,{passive:i}):e.addEventListener(t,n,!1)}function Wa(e,t,n,a,i){var o=a;if(0===(1&t)&&0===(2&t)&&null!==a)e:for(;;){if(null===a)return;var s=a.tag;if(3===s||4===s){var r=a.stateNode.containerInfo;if(r===i||8===r.nodeType&&r.parentNode===i)break;if(4===s)for(s=a.return;null!==s;){var l=s.tag;if((3===l||4===l)&&((l=s.stateNode.containerInfo)===i||8===l.nodeType&&l.parentNode===i))return;s=s.return}for(;null!==r;){if(null===(s=vi(r)))return;if(5===(l=s.tag)||6===l){a=o=s;continue e}r=r.parentNode}}a=a.return}Re((function(){var a=o,i=Ie(n),s=[];e:{var r=Ta.get(e);if(void 0!==r){var l=dn,c=e;switch(e){case"keypress":if(0===tn(n))break e;case"keydown":case"keyup":l=Cn;break;case"focusin":c="focus",l=fn;break;case"focusout":c="blur",l=fn;break;case"beforeblur":case"afterblur":l=fn;break;case"click":if(2===n.button)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":l=gn;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":l=hn;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":l=En;break;case xa:case Ca:case Pa:l=yn;break;case Ea:l=Tn;break;case"scroll":l=pn;break;case"wheel":l=zn;break;case"copy":case"cut":case"paste":l=bn;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":l=Pn}var d=0!==(4&t),u=!d&&"scroll"===e,p=d?null!==r?r+"Capture":null:r;d=[];for(var m,g=a;null!==g;){var h=(m=g).stateNode;if(5===m.tag&&null!==h&&(m=h,null!==p&&(null!=(h=ze(g,p))&&d.push(ja(g,h,m)))),u)break;g=g.return}0<d.length&&(r=new l(r,c,null,n,i),s.push({event:r,listeners:d}))}}if(0===(7&t)){if(l="mouseout"===e||"pointerout"===e,(!(r="mouseover"===e||"pointerover"===e)||n===Ae||!(c=n.relatedTarget||n.fromElement)||!vi(c)&&!c[gi])&&(l||r)&&(r=i.window===i?i:(r=i.ownerDocument)?r.defaultView||r.parentWindow:window,l?(l=a,null!==(c=(c=n.relatedTarget||n.toElement)?vi(c):null)&&(c!==(u=He(c))||5!==c.tag&&6!==c.tag)&&(c=null)):(l=null,c=a),l!==c)){if(d=gn,h="onMouseLeave",p="onMouseEnter",g="mouse","pointerout"!==e&&"pointerover"!==e||(d=Pn,h="onPointerLeave",p="onPointerEnter",g="pointer"),u=null==l?r:Ai(l),m=null==c?r:Ai(c),(r=new d(h,g+"leave",l,n,i)).target=u,r.relatedTarget=m,h=null,vi(i)===a&&((d=new d(p,g+"enter",c,n,i)).target=m,d.relatedTarget=u,h=d),u=h,l&&c)e:{for(p=c,g=0,m=d=l;m;m=$a(m))g++;for(m=0,h=p;h;h=$a(h))m++;for(;0<g-m;)d=$a(d),g--;for(;0<m-g;)p=$a(p),m--;for(;g--;){if(d===p||null!==p&&d===p.alternate)break e;d=$a(d),p=$a(p)}d=null}else d=null;null!==l&&Ka(s,r,l,d,!1),null!==c&&null!==u&&Ka(s,u,c,d,!0)}if("select"===(l=(r=a?Ai(a):window).nodeName&&r.nodeName.toLowerCase())||"input"===l&&"file"===r.type)var f=Jn;else if(Wn(r))if(Xn)f=sa;else{f=ia;var y=aa}else(l=r.nodeName)&&"input"===l.toLowerCase()&&("checkbox"===r.type||"radio"===r.type)&&(f=oa);switch(f&&(f=f(e,a))?jn(s,f,n,i):(y&&y(e,r,a),"focusout"===e&&(y=r._wrapperState)&&y.controlled&&"number"===r.type&&ee(r,"number",r.value)),y=a?Ai(a):window,e){case"focusin":(Wn(y)||"true"===y.contentEditable)&&(fa=y,ya=a,va=null);break;case"focusout":va=ya=fa=null;break;case"mousedown":ba=!0;break;case"contextmenu":case"mouseup":case"dragend":ba=!1,Aa(s,n,i);break;case"selectionchange":if(ha)break;case"keydown":case"keyup":Aa(s,n,i)}var v;if(qn)e:{switch(e){case"compositionstart":var b="onCompositionStart";break e;case"compositionend":b="onCompositionEnd";break e;case"compositionupdate":b="onCompositionUpdate";break e}b=void 0}else Hn?Un(e,n)&&(b="onCompositionEnd"):"keydown"===e&&229===n.keyCode&&(b="onCompositionStart");b&&(Ln&&"ko"!==n.locale&&(Hn||"onCompositionStart"!==b?"onCompositionEnd"===b&&Hn&&(v=en()):(Yt="value"in(Xt=i)?Xt.value:Xt.textContent,Hn=!0)),0<(y=Va(a,b)).length&&(b=new An(b,e,null,n,i),s.push({event:b,listeners:y}),v?b.data=v:null!==(v=On(n))&&(b.data=v))),(v=Gn?function(e,t){switch(e){case"compositionend":return On(t);case"keypress":return 32!==t.which?null:(Nn=!0,Fn);case"textInput":return(e=t.data)===Fn&&Nn?null:e;default:return null}}(e,n):function(e,t){if(Hn)return"compositionend"===e||!qn&&Un(e,t)?(e=en(),Zt=Yt=Xt=null,Hn=!1,e):null;switch(e){case"paste":default:return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return Ln&&"ko"!==t.locale?null:t.data}}(e,n))&&(0<(a=Va(a,"onBeforeInput")).length&&(i=new An("onBeforeInput","beforeinput",null,n,i),s.push({event:i,listeners:a}),i.data=v))}Fa(s,t)}))}function ja(e,t,n){return{instance:e,listener:t,currentTarget:n}}function Va(e,t){for(var n=t+"Capture",a=[];null!==e;){var i=e,o=i.stateNode;5===i.tag&&null!==o&&(i=o,null!=(o=ze(e,n))&&a.unshift(ja(e,o,i)),null!=(o=ze(e,t))&&a.push(ja(e,o,i))),e=e.return}return a}function $a(e){if(null===e)return null;do{e=e.return}while(e&&5!==e.tag);return e||null}function Ka(e,t,n,a,i){for(var o=t._reactName,s=[];null!==n&&n!==a;){var r=n,l=r.alternate,c=r.stateNode;if(null!==l&&l===a)break;5===r.tag&&null!==c&&(r=c,i?null!=(l=ze(n,o))&&s.unshift(ja(n,l,r)):i||null!=(l=ze(n,o))&&s.push(ja(n,l,r))),n=n.return}0!==s.length&&e.push({event:t,listeners:s})}var Qa=/\r\n?/g,Ja=/\u0000|\uFFFD/g;function Xa(e){return("string"===typeof e?e:""+e).replace(Qa,"\n").replace(Ja,"")}function Ya(e,t,n){if(t=Xa(t),Xa(e)!==t&&n)throw Error(o(425))}function Za(){}var ei=null,ti=null;function ni(e,t){return"textarea"===e||"noscript"===e||"string"===typeof t.children||"number"===typeof t.children||"object"===typeof t.dangerouslySetInnerHTML&&null!==t.dangerouslySetInnerHTML&&null!=t.dangerouslySetInnerHTML.__html}var ai="function"===typeof setTimeout?setTimeout:void 0,ii="function"===typeof clearTimeout?clearTimeout:void 0,oi="function"===typeof Promise?Promise:void 0,si="function"===typeof queueMicrotask?queueMicrotask:"undefined"!==typeof oi?function(e){return oi.resolve(null).then(e).catch(ri)}:ai;function ri(e){setTimeout((function(){throw e}))}function li(e,t){var n=t,a=0;do{var i=n.nextSibling;if(e.removeChild(n),i&&8===i.nodeType)if("/$"===(n=i.data)){if(0===a)return e.removeChild(i),void Ht(t);a--}else"$"!==n&&"$?"!==n&&"$!"!==n||a++;n=i}while(n);Ht(t)}function ci(e){for(;null!=e;e=e.nextSibling){var t=e.nodeType;if(1===t||3===t)break;if(8===t){if("$"===(t=e.data)||"$!"===t||"$?"===t)break;if("/$"===t)return null}}return e}function di(e){e=e.previousSibling;for(var t=0;e;){if(8===e.nodeType){var n=e.data;if("$"===n||"$!"===n||"$?"===n){if(0===t)return e;t--}else"/$"===n&&t++}e=e.previousSibling}return null}var ui=Math.random().toString(36).slice(2),pi="__reactFiber$"+ui,mi="__reactProps$"+ui,gi="__reactContainer$"+ui,hi="__reactEvents$"+ui,fi="__reactListeners$"+ui,yi="__reactHandles$"+ui;function vi(e){var t=e[pi];if(t)return t;for(var n=e.parentNode;n;){if(t=n[gi]||n[pi]){if(n=t.alternate,null!==t.child||null!==n&&null!==n.child)for(e=di(e);null!==e;){if(n=e[pi])return n;e=di(e)}return t}n=(e=n).parentNode}return null}function bi(e){return!(e=e[pi]||e[gi])||5!==e.tag&&6!==e.tag&&13!==e.tag&&3!==e.tag?null:e}function Ai(e){if(5===e.tag||6===e.tag)return e.stateNode;throw Error(o(33))}function Ii(e){return e[mi]||null}var wi=[],ki=-1;function _i(e){return{current:e}}function Si(e){0>ki||(e.current=wi[ki],wi[ki]=null,ki--)}function xi(e,t){ki++,wi[ki]=e.current,e.current=t}var Ci={},Pi=_i(Ci),Ei=_i(!1),Ti=Ci;function Ri(e,t){var n=e.type.contextTypes;if(!n)return Ci;var a=e.stateNode;if(a&&a.__reactInternalMemoizedUnmaskedChildContext===t)return a.__reactInternalMemoizedMaskedChildContext;var i,o={};for(i in n)o[i]=t[i];return a&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=o),o}function zi(e){return null!==(e=e.childContextTypes)&&void 0!==e}function Di(){Si(Ei),Si(Pi)}function qi(e,t,n){if(Pi.current!==Ci)throw Error(o(168));xi(Pi,t),xi(Ei,n)}function Mi(e,t,n){var a=e.stateNode;if(t=t.childContextTypes,"function"!==typeof a.getChildContext)return n;for(var i in a=a.getChildContext())if(!(i in t))throw Error(o(108,B(e)||"Unknown",i));return L({},n,a)}function Gi(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||Ci,Ti=Pi.current,xi(Pi,e),xi(Ei,Ei.current),!0}function Li(e,t,n){var a=e.stateNode;if(!a)throw Error(o(169));n?(e=Mi(e,t,Ti),a.__reactInternalMemoizedMergedChildContext=e,Si(Ei),Si(Pi),xi(Pi,e)):Si(Ei),xi(Ei,n)}var Fi=null,Ni=!1,Ui=!1;function Oi(e){null===Fi?Fi=[e]:Fi.push(e)}function Hi(){if(!Ui&&null!==Fi){Ui=!0;var e=0,t=bt;try{var n=Fi;for(bt=1;e<n.length;e++){var a=n[e];do{a=a(!0)}while(null!==a)}Fi=null,Ni=!1}catch(i){throw null!==Fi&&(Fi=Fi.slice(e+1)),$e(Ze,Hi),i}finally{bt=t,Ui=!1}}return null}var Bi=[],Wi=0,ji=null,Vi=0,$i=[],Ki=0,Qi=null,Ji=1,Xi="";function Yi(e,t){Bi[Wi++]=Vi,Bi[Wi++]=ji,ji=e,Vi=t}function Zi(e,t,n){$i[Ki++]=Ji,$i[Ki++]=Xi,$i[Ki++]=Qi,Qi=e;var a=Ji;e=Xi;var i=32-st(a)-1;a&=~(1<<i),n+=1;var o=32-st(t)+i;if(30<o){var s=i-i%5;o=(a&(1<<s)-1).toString(32),a>>=s,i-=s,Ji=1<<32-st(t)+i|n<<i|a,Xi=o+e}else Ji=1<<o|n<<i|a,Xi=e}function eo(e){null!==e.return&&(Yi(e,1),Zi(e,1,0))}function to(e){for(;e===ji;)ji=Bi[--Wi],Bi[Wi]=null,Vi=Bi[--Wi],Bi[Wi]=null;for(;e===Qi;)Qi=$i[--Ki],$i[Ki]=null,Xi=$i[--Ki],$i[Ki]=null,Ji=$i[--Ki],$i[Ki]=null}var no=null,ao=null,io=!1,oo=null;function so(e,t){var n=Rc(5,null,null,0);n.elementType="DELETED",n.stateNode=t,n.return=e,null===(t=e.deletions)?(e.deletions=[n],e.flags|=16):t.push(n)}function ro(e,t){switch(e.tag){case 5:var n=e.type;return null!==(t=1!==t.nodeType||n.toLowerCase()!==t.nodeName.toLowerCase()?null:t)&&(e.stateNode=t,no=e,ao=ci(t.firstChild),!0);case 6:return null!==(t=""===e.pendingProps||3!==t.nodeType?null:t)&&(e.stateNode=t,no=e,ao=null,!0);case 13:return null!==(t=8!==t.nodeType?null:t)&&(n=null!==Qi?{id:Ji,overflow:Xi}:null,e.memoizedState={dehydrated:t,treeContext:n,retryLane:1073741824},(n=Rc(18,null,null,0)).stateNode=t,n.return=e,e.child=n,no=e,ao=null,!0);default:return!1}}function lo(e){return 0!==(1&e.mode)&&0===(128&e.flags)}function co(e){if(io){var t=ao;if(t){var n=t;if(!ro(e,t)){if(lo(e))throw Error(o(418));t=ci(n.nextSibling);var a=no;t&&ro(e,t)?so(a,n):(e.flags=-4097&e.flags|2,io=!1,no=e)}}else{if(lo(e))throw Error(o(418));e.flags=-4097&e.flags|2,io=!1,no=e}}}function uo(e){for(e=e.return;null!==e&&5!==e.tag&&3!==e.tag&&13!==e.tag;)e=e.return;no=e}function po(e){if(e!==no)return!1;if(!io)return uo(e),io=!0,!1;var t;if((t=3!==e.tag)&&!(t=5!==e.tag)&&(t="head"!==(t=e.type)&&"body"!==t&&!ni(e.type,e.memoizedProps)),t&&(t=ao)){if(lo(e))throw mo(),Error(o(418));for(;t;)so(e,t),t=ci(t.nextSibling)}if(uo(e),13===e.tag){if(!(e=null!==(e=e.memoizedState)?e.dehydrated:null))throw Error(o(317));e:{for(e=e.nextSibling,t=0;e;){if(8===e.nodeType){var n=e.data;if("/$"===n){if(0===t){ao=ci(e.nextSibling);break e}t--}else"$"!==n&&"$!"!==n&&"$?"!==n||t++}e=e.nextSibling}ao=null}}else ao=no?ci(e.stateNode.nextSibling):null;return!0}function mo(){for(var e=ao;e;)e=ci(e.nextSibling)}function go(){ao=no=null,io=!1}function ho(e){null===oo?oo=[e]:oo.push(e)}var fo=A.ReactCurrentBatchConfig;function yo(e,t,n){if(null!==(e=n.ref)&&"function"!==typeof e&&"object"!==typeof e){if(n._owner){if(n=n._owner){if(1!==n.tag)throw Error(o(309));var a=n.stateNode}if(!a)throw Error(o(147,e));var i=a,s=""+e;return null!==t&&null!==t.ref&&"function"===typeof t.ref&&t.ref._stringRef===s?t.ref:(t=function(e){var t=i.refs;null===e?delete t[s]:t[s]=e},t._stringRef=s,t)}if("string"!==typeof e)throw Error(o(284));if(!n._owner)throw Error(o(290,e))}return e}function vo(e,t){throw e=Object.prototype.toString.call(t),Error(o(31,"[object Object]"===e?"object with keys {"+Object.keys(t).join(", ")+"}":e))}function bo(e){return(0,e._init)(e._payload)}function Ao(e){function t(t,n){if(e){var a=t.deletions;null===a?(t.deletions=[n],t.flags|=16):a.push(n)}}function n(n,a){if(!e)return null;for(;null!==a;)t(n,a),a=a.sibling;return null}function a(e,t){for(e=new Map;null!==t;)null!==t.key?e.set(t.key,t):e.set(t.index,t),t=t.sibling;return e}function i(e,t){return(e=Dc(e,t)).index=0,e.sibling=null,e}function s(t,n,a){return t.index=a,e?null!==(a=t.alternate)?(a=a.index)<n?(t.flags|=2,n):a:(t.flags|=2,n):(t.flags|=1048576,n)}function r(t){return e&&null===t.alternate&&(t.flags|=2),t}function l(e,t,n,a){return null===t||6!==t.tag?((t=Lc(n,e.mode,a)).return=e,t):((t=i(t,n)).return=e,t)}function c(e,t,n,a){var o=n.type;return o===k?u(e,t,n.props.children,a,n.key):null!==t&&(t.elementType===o||"object"===typeof o&&null!==o&&o.$$typeof===z&&bo(o)===t.type)?((a=i(t,n.props)).ref=yo(e,t,n),a.return=e,a):((a=qc(n.type,n.key,n.props,null,e.mode,a)).ref=yo(e,t,n),a.return=e,a)}function d(e,t,n,a){return null===t||4!==t.tag||t.stateNode.containerInfo!==n.containerInfo||t.stateNode.implementation!==n.implementation?((t=Fc(n,e.mode,a)).return=e,t):((t=i(t,n.children||[])).return=e,t)}function u(e,t,n,a,o){return null===t||7!==t.tag?((t=Mc(n,e.mode,a,o)).return=e,t):((t=i(t,n)).return=e,t)}function p(e,t,n){if("string"===typeof t&&""!==t||"number"===typeof t)return(t=Lc(""+t,e.mode,n)).return=e,t;if("object"===typeof t&&null!==t){switch(t.$$typeof){case I:return(n=qc(t.type,t.key,t.props,null,e.mode,n)).ref=yo(e,null,t),n.return=e,n;case w:return(t=Fc(t,e.mode,n)).return=e,t;case z:return p(e,(0,t._init)(t._payload),n)}if(te(t)||M(t))return(t=Mc(t,e.mode,n,null)).return=e,t;vo(e,t)}return null}function m(e,t,n,a){var i=null!==t?t.key:null;if("string"===typeof n&&""!==n||"number"===typeof n)return null!==i?null:l(e,t,""+n,a);if("object"===typeof n&&null!==n){switch(n.$$typeof){case I:return n.key===i?c(e,t,n,a):null;case w:return n.key===i?d(e,t,n,a):null;case z:return m(e,t,(i=n._init)(n._payload),a)}if(te(n)||M(n))return null!==i?null:u(e,t,n,a,null);vo(e,n)}return null}function g(e,t,n,a,i){if("string"===typeof a&&""!==a||"number"===typeof a)return l(t,e=e.get(n)||null,""+a,i);if("object"===typeof a&&null!==a){switch(a.$$typeof){case I:return c(t,e=e.get(null===a.key?n:a.key)||null,a,i);case w:return d(t,e=e.get(null===a.key?n:a.key)||null,a,i);case z:return g(e,t,n,(0,a._init)(a._payload),i)}if(te(a)||M(a))return u(t,e=e.get(n)||null,a,i,null);vo(t,a)}return null}function h(i,o,r,l){for(var c=null,d=null,u=o,h=o=0,f=null;null!==u&&h<r.length;h++){u.index>h?(f=u,u=null):f=u.sibling;var y=m(i,u,r[h],l);if(null===y){null===u&&(u=f);break}e&&u&&null===y.alternate&&t(i,u),o=s(y,o,h),null===d?c=y:d.sibling=y,d=y,u=f}if(h===r.length)return n(i,u),io&&Yi(i,h),c;if(null===u){for(;h<r.length;h++)null!==(u=p(i,r[h],l))&&(o=s(u,o,h),null===d?c=u:d.sibling=u,d=u);return io&&Yi(i,h),c}for(u=a(i,u);h<r.length;h++)null!==(f=g(u,i,h,r[h],l))&&(e&&null!==f.alternate&&u.delete(null===f.key?h:f.key),o=s(f,o,h),null===d?c=f:d.sibling=f,d=f);return e&&u.forEach((function(e){return t(i,e)})),io&&Yi(i,h),c}function f(i,r,l,c){var d=M(l);if("function"!==typeof d)throw Error(o(150));if(null==(l=d.call(l)))throw Error(o(151));for(var u=d=null,h=r,f=r=0,y=null,v=l.next();null!==h&&!v.done;f++,v=l.next()){h.index>f?(y=h,h=null):y=h.sibling;var b=m(i,h,v.value,c);if(null===b){null===h&&(h=y);break}e&&h&&null===b.alternate&&t(i,h),r=s(b,r,f),null===u?d=b:u.sibling=b,u=b,h=y}if(v.done)return n(i,h),io&&Yi(i,f),d;if(null===h){for(;!v.done;f++,v=l.next())null!==(v=p(i,v.value,c))&&(r=s(v,r,f),null===u?d=v:u.sibling=v,u=v);return io&&Yi(i,f),d}for(h=a(i,h);!v.done;f++,v=l.next())null!==(v=g(h,i,f,v.value,c))&&(e&&null!==v.alternate&&h.delete(null===v.key?f:v.key),r=s(v,r,f),null===u?d=v:u.sibling=v,u=v);return e&&h.forEach((function(e){return t(i,e)})),io&&Yi(i,f),d}return function e(a,o,s,l){if("object"===typeof s&&null!==s&&s.type===k&&null===s.key&&(s=s.props.children),"object"===typeof s&&null!==s){switch(s.$$typeof){case I:e:{for(var c=s.key,d=o;null!==d;){if(d.key===c){if((c=s.type)===k){if(7===d.tag){n(a,d.sibling),(o=i(d,s.props.children)).return=a,a=o;break e}}else if(d.elementType===c||"object"===typeof c&&null!==c&&c.$$typeof===z&&bo(c)===d.type){n(a,d.sibling),(o=i(d,s.props)).ref=yo(a,d,s),o.return=a,a=o;break e}n(a,d);break}t(a,d),d=d.sibling}s.type===k?((o=Mc(s.props.children,a.mode,l,s.key)).return=a,a=o):((l=qc(s.type,s.key,s.props,null,a.mode,l)).ref=yo(a,o,s),l.return=a,a=l)}return r(a);case w:e:{for(d=s.key;null!==o;){if(o.key===d){if(4===o.tag&&o.stateNode.containerInfo===s.containerInfo&&o.stateNode.implementation===s.implementation){n(a,o.sibling),(o=i(o,s.children||[])).return=a,a=o;break e}n(a,o);break}t(a,o),o=o.sibling}(o=Fc(s,a.mode,l)).return=a,a=o}return r(a);case z:return e(a,o,(d=s._init)(s._payload),l)}if(te(s))return h(a,o,s,l);if(M(s))return f(a,o,s,l);vo(a,s)}return"string"===typeof s&&""!==s||"number"===typeof s?(s=""+s,null!==o&&6===o.tag?(n(a,o.sibling),(o=i(o,s)).return=a,a=o):(n(a,o),(o=Lc(s,a.mode,l)).return=a,a=o),r(a)):n(a,o)}}var Io=Ao(!0),wo=Ao(!1),ko=_i(null),_o=null,So=null,xo=null;function Co(){xo=So=_o=null}function Po(e){var t=ko.current;Si(ko),e._currentValue=t}function Eo(e,t,n){for(;null!==e;){var a=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,null!==a&&(a.childLanes|=t)):null!==a&&(a.childLanes&t)!==t&&(a.childLanes|=t),e===n)break;e=e.return}}function To(e,t){_o=e,xo=So=null,null!==(e=e.dependencies)&&null!==e.firstContext&&(0!==(e.lanes&t)&&(br=!0),e.firstContext=null)}function Ro(e){var t=e._currentValue;if(xo!==e)if(e={context:e,memoizedValue:t,next:null},null===So){if(null===_o)throw Error(o(308));So=e,_o.dependencies={lanes:0,firstContext:e}}else So=So.next=e;return t}var zo=null;function Do(e){null===zo?zo=[e]:zo.push(e)}function qo(e,t,n,a){var i=t.interleaved;return null===i?(n.next=n,Do(t)):(n.next=i.next,i.next=n),t.interleaved=n,Mo(e,a)}function Mo(e,t){e.lanes|=t;var n=e.alternate;for(null!==n&&(n.lanes|=t),n=e,e=e.return;null!==e;)e.childLanes|=t,null!==(n=e.alternate)&&(n.childLanes|=t),n=e,e=e.return;return 3===n.tag?n.stateNode:null}var Go=!1;function Lo(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,interleaved:null,lanes:0},effects:null}}function Fo(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,effects:e.effects})}function No(e,t){return{eventTime:e,lane:t,tag:0,payload:null,callback:null,next:null}}function Uo(e,t,n){var a=e.updateQueue;if(null===a)return null;if(a=a.shared,0!==(2&Pl)){var i=a.pending;return null===i?t.next=t:(t.next=i.next,i.next=t),a.pending=t,Mo(e,n)}return null===(i=a.interleaved)?(t.next=t,Do(a)):(t.next=i.next,i.next=t),a.interleaved=t,Mo(e,n)}function Oo(e,t,n){if(null!==(t=t.updateQueue)&&(t=t.shared,0!==(4194240&n))){var a=t.lanes;n|=a&=e.pendingLanes,t.lanes=n,vt(e,n)}}function Ho(e,t){var n=e.updateQueue,a=e.alternate;if(null!==a&&n===(a=a.updateQueue)){var i=null,o=null;if(null!==(n=n.firstBaseUpdate)){do{var s={eventTime:n.eventTime,lane:n.lane,tag:n.tag,payload:n.payload,callback:n.callback,next:null};null===o?i=o=s:o=o.next=s,n=n.next}while(null!==n);null===o?i=o=t:o=o.next=t}else i=o=t;return n={baseState:a.baseState,firstBaseUpdate:i,lastBaseUpdate:o,shared:a.shared,effects:a.effects},void(e.updateQueue=n)}null===(e=n.lastBaseUpdate)?n.firstBaseUpdate=t:e.next=t,n.lastBaseUpdate=t}function Bo(e,t,n,a){var i=e.updateQueue;Go=!1;var o=i.firstBaseUpdate,s=i.lastBaseUpdate,r=i.shared.pending;if(null!==r){i.shared.pending=null;var l=r,c=l.next;l.next=null,null===s?o=c:s.next=c,s=l;var d=e.alternate;null!==d&&((r=(d=d.updateQueue).lastBaseUpdate)!==s&&(null===r?d.firstBaseUpdate=c:r.next=c,d.lastBaseUpdate=l))}if(null!==o){var u=i.baseState;for(s=0,d=c=l=null,r=o;;){var p=r.lane,m=r.eventTime;if((a&p)===p){null!==d&&(d=d.next={eventTime:m,lane:0,tag:r.tag,payload:r.payload,callback:r.callback,next:null});e:{var g=e,h=r;switch(p=t,m=n,h.tag){case 1:if("function"===typeof(g=h.payload)){u=g.call(m,u,p);break e}u=g;break e;case 3:g.flags=-65537&g.flags|128;case 0:if(null===(p="function"===typeof(g=h.payload)?g.call(m,u,p):g)||void 0===p)break e;u=L({},u,p);break e;case 2:Go=!0}}null!==r.callback&&0!==r.lane&&(e.flags|=64,null===(p=i.effects)?i.effects=[r]:p.push(r))}else m={eventTime:m,lane:p,tag:r.tag,payload:r.payload,callback:r.callback,next:null},null===d?(c=d=m,l=u):d=d.next=m,s|=p;if(null===(r=r.next)){if(null===(r=i.shared.pending))break;r=(p=r).next,p.next=null,i.lastBaseUpdate=p,i.shared.pending=null}}if(null===d&&(l=u),i.baseState=l,i.firstBaseUpdate=c,i.lastBaseUpdate=d,null!==(t=i.shared.interleaved)){i=t;do{s|=i.lane,i=i.next}while(i!==t)}else null===o&&(i.shared.lanes=0);Gl|=s,e.lanes=s,e.memoizedState=u}}function Wo(e,t,n){if(e=t.effects,t.effects=null,null!==e)for(t=0;t<e.length;t++){var a=e[t],i=a.callback;if(null!==i){if(a.callback=null,a=n,"function"!==typeof i)throw Error(o(191,i));i.call(a)}}}var jo={},Vo=_i(jo),$o=_i(jo),Ko=_i(jo);function Qo(e){if(e===jo)throw Error(o(174));return e}function Jo(e,t){switch(xi(Ko,t),xi($o,e),xi(Vo,jo),e=t.nodeType){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:le(null,"");break;default:t=le(t=(e=8===e?t.parentNode:t).namespaceURI||null,e=e.tagName)}Si(Vo),xi(Vo,t)}function Xo(){Si(Vo),Si($o),Si(Ko)}function Yo(e){Qo(Ko.current);var t=Qo(Vo.current),n=le(t,e.type);t!==n&&(xi($o,e),xi(Vo,n))}function Zo(e){$o.current===e&&(Si(Vo),Si($o))}var es=_i(0);function ts(e){for(var t=e;null!==t;){if(13===t.tag){var n=t.memoizedState;if(null!==n&&(null===(n=n.dehydrated)||"$?"===n.data||"$!"===n.data))return t}else if(19===t.tag&&void 0!==t.memoizedProps.revealOrder){if(0!==(128&t.flags))return t}else if(null!==t.child){t.child.return=t,t=t.child;continue}if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var ns=[];function as(){for(var e=0;e<ns.length;e++)ns[e]._workInProgressVersionPrimary=null;ns.length=0}var is=A.ReactCurrentDispatcher,os=A.ReactCurrentBatchConfig,ss=0,rs=null,ls=null,cs=null,ds=!1,us=!1,ps=0,ms=0;function gs(){throw Error(o(321))}function hs(e,t){if(null===t)return!1;for(var n=0;n<t.length&&n<e.length;n++)if(!ra(e[n],t[n]))return!1;return!0}function fs(e,t,n,a,i,s){if(ss=s,rs=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,is.current=null===e||null===e.memoizedState?Zs:er,e=n(a,i),us){s=0;do{if(us=!1,ps=0,25<=s)throw Error(o(301));s+=1,cs=ls=null,t.updateQueue=null,is.current=tr,e=n(a,i)}while(us)}if(is.current=Ys,t=null!==ls&&null!==ls.next,ss=0,cs=ls=rs=null,ds=!1,t)throw Error(o(300));return e}function ys(){var e=0!==ps;return ps=0,e}function vs(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return null===cs?rs.memoizedState=cs=e:cs=cs.next=e,cs}function bs(){if(null===ls){var e=rs.alternate;e=null!==e?e.memoizedState:null}else e=ls.next;var t=null===cs?rs.memoizedState:cs.next;if(null!==t)cs=t,ls=e;else{if(null===e)throw Error(o(310));e={memoizedState:(ls=e).memoizedState,baseState:ls.baseState,baseQueue:ls.baseQueue,queue:ls.queue,next:null},null===cs?rs.memoizedState=cs=e:cs=cs.next=e}return cs}function As(e,t){return"function"===typeof t?t(e):t}function Is(e){var t=bs(),n=t.queue;if(null===n)throw Error(o(311));n.lastRenderedReducer=e;var a=ls,i=a.baseQueue,s=n.pending;if(null!==s){if(null!==i){var r=i.next;i.next=s.next,s.next=r}a.baseQueue=i=s,n.pending=null}if(null!==i){s=i.next,a=a.baseState;var l=r=null,c=null,d=s;do{var u=d.lane;if((ss&u)===u)null!==c&&(c=c.next={lane:0,action:d.action,hasEagerState:d.hasEagerState,eagerState:d.eagerState,next:null}),a=d.hasEagerState?d.eagerState:e(a,d.action);else{var p={lane:u,action:d.action,hasEagerState:d.hasEagerState,eagerState:d.eagerState,next:null};null===c?(l=c=p,r=a):c=c.next=p,rs.lanes|=u,Gl|=u}d=d.next}while(null!==d&&d!==s);null===c?r=a:c.next=l,ra(a,t.memoizedState)||(br=!0),t.memoizedState=a,t.baseState=r,t.baseQueue=c,n.lastRenderedState=a}if(null!==(e=n.interleaved)){i=e;do{s=i.lane,rs.lanes|=s,Gl|=s,i=i.next}while(i!==e)}else null===i&&(n.lanes=0);return[t.memoizedState,n.dispatch]}function ws(e){var t=bs(),n=t.queue;if(null===n)throw Error(o(311));n.lastRenderedReducer=e;var a=n.dispatch,i=n.pending,s=t.memoizedState;if(null!==i){n.pending=null;var r=i=i.next;do{s=e(s,r.action),r=r.next}while(r!==i);ra(s,t.memoizedState)||(br=!0),t.memoizedState=s,null===t.baseQueue&&(t.baseState=s),n.lastRenderedState=s}return[s,a]}function ks(){}function _s(e,t){var n=rs,a=bs(),i=t(),s=!ra(a.memoizedState,i);if(s&&(a.memoizedState=i,br=!0),a=a.queue,Gs(Cs.bind(null,n,a,e),[e]),a.getSnapshot!==t||s||null!==cs&&1&cs.memoizedState.tag){if(n.flags|=2048,Rs(9,xs.bind(null,n,a,i,t),void 0,null),null===El)throw Error(o(349));0!==(30&ss)||Ss(n,t,i)}return i}function Ss(e,t,n){e.flags|=16384,e={getSnapshot:t,value:n},null===(t=rs.updateQueue)?(t={lastEffect:null,stores:null},rs.updateQueue=t,t.stores=[e]):null===(n=t.stores)?t.stores=[e]:n.push(e)}function xs(e,t,n,a){t.value=n,t.getSnapshot=a,Ps(t)&&Es(e)}function Cs(e,t,n){return n((function(){Ps(t)&&Es(e)}))}function Ps(e){var t=e.getSnapshot;e=e.value;try{var n=t();return!ra(e,n)}catch(a){return!0}}function Es(e){var t=Mo(e,1);null!==t&&nc(t,e,1,-1)}function Ts(e){var t=vs();return"function"===typeof e&&(e=e()),t.memoizedState=t.baseState=e,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:As,lastRenderedState:e},t.queue=e,e=e.dispatch=Ks.bind(null,rs,e),[t.memoizedState,e]}function Rs(e,t,n,a){return e={tag:e,create:t,destroy:n,deps:a,next:null},null===(t=rs.updateQueue)?(t={lastEffect:null,stores:null},rs.updateQueue=t,t.lastEffect=e.next=e):null===(n=t.lastEffect)?t.lastEffect=e.next=e:(a=n.next,n.next=e,e.next=a,t.lastEffect=e),e}function zs(){return bs().memoizedState}function Ds(e,t,n,a){var i=vs();rs.flags|=e,i.memoizedState=Rs(1|t,n,void 0,void 0===a?null:a)}function qs(e,t,n,a){var i=bs();a=void 0===a?null:a;var o=void 0;if(null!==ls){var s=ls.memoizedState;if(o=s.destroy,null!==a&&hs(a,s.deps))return void(i.memoizedState=Rs(t,n,o,a))}rs.flags|=e,i.memoizedState=Rs(1|t,n,o,a)}function Ms(e,t){return Ds(8390656,8,e,t)}function Gs(e,t){return qs(2048,8,e,t)}function Ls(e,t){return qs(4,2,e,t)}function Fs(e,t){return qs(4,4,e,t)}function Ns(e,t){return"function"===typeof t?(e=e(),t(e),function(){t(null)}):null!==t&&void 0!==t?(e=e(),t.current=e,function(){t.current=null}):void 0}function Us(e,t,n){return n=null!==n&&void 0!==n?n.concat([e]):null,qs(4,4,Ns.bind(null,t,e),n)}function Os(){}function Hs(e,t){var n=bs();t=void 0===t?null:t;var a=n.memoizedState;return null!==a&&null!==t&&hs(t,a[1])?a[0]:(n.memoizedState=[e,t],e)}function Bs(e,t){var n=bs();t=void 0===t?null:t;var a=n.memoizedState;return null!==a&&null!==t&&hs(t,a[1])?a[0]:(e=e(),n.memoizedState=[e,t],e)}function Ws(e,t,n){return 0===(21&ss)?(e.baseState&&(e.baseState=!1,br=!0),e.memoizedState=n):(ra(n,t)||(n=ht(),rs.lanes|=n,Gl|=n,e.baseState=!0),t)}function js(e,t){var n=bt;bt=0!==n&&4>n?n:4,e(!0);var a=os.transition;os.transition={};try{e(!1),t()}finally{bt=n,os.transition=a}}function Vs(){return bs().memoizedState}function $s(e,t,n){var a=tc(e);if(n={lane:a,action:n,hasEagerState:!1,eagerState:null,next:null},Qs(e))Js(t,n);else if(null!==(n=qo(e,t,n,a))){nc(n,e,a,ec()),Xs(n,t,a)}}function Ks(e,t,n){var a=tc(e),i={lane:a,action:n,hasEagerState:!1,eagerState:null,next:null};if(Qs(e))Js(t,i);else{var o=e.alternate;if(0===e.lanes&&(null===o||0===o.lanes)&&null!==(o=t.lastRenderedReducer))try{var s=t.lastRenderedState,r=o(s,n);if(i.hasEagerState=!0,i.eagerState=r,ra(r,s)){var l=t.interleaved;return null===l?(i.next=i,Do(t)):(i.next=l.next,l.next=i),void(t.interleaved=i)}}catch(c){}null!==(n=qo(e,t,i,a))&&(nc(n,e,a,i=ec()),Xs(n,t,a))}}function Qs(e){var t=e.alternate;return e===rs||null!==t&&t===rs}function Js(e,t){us=ds=!0;var n=e.pending;null===n?t.next=t:(t.next=n.next,n.next=t),e.pending=t}function Xs(e,t,n){if(0!==(4194240&n)){var a=t.lanes;n|=a&=e.pendingLanes,t.lanes=n,vt(e,n)}}var Ys={readContext:Ro,useCallback:gs,useContext:gs,useEffect:gs,useImperativeHandle:gs,useInsertionEffect:gs,useLayoutEffect:gs,useMemo:gs,useReducer:gs,useRef:gs,useState:gs,useDebugValue:gs,useDeferredValue:gs,useTransition:gs,useMutableSource:gs,useSyncExternalStore:gs,useId:gs,unstable_isNewReconciler:!1},Zs={readContext:Ro,useCallback:function(e,t){return vs().memoizedState=[e,void 0===t?null:t],e},useContext:Ro,useEffect:Ms,useImperativeHandle:function(e,t,n){return n=null!==n&&void 0!==n?n.concat([e]):null,Ds(4194308,4,Ns.bind(null,t,e),n)},useLayoutEffect:function(e,t){return Ds(4194308,4,e,t)},useInsertionEffect:function(e,t){return Ds(4,2,e,t)},useMemo:function(e,t){var n=vs();return t=void 0===t?null:t,e=e(),n.memoizedState=[e,t],e},useReducer:function(e,t,n){var a=vs();return t=void 0!==n?n(t):t,a.memoizedState=a.baseState=t,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:t},a.queue=e,e=e.dispatch=$s.bind(null,rs,e),[a.memoizedState,e]},useRef:function(e){return e={current:e},vs().memoizedState=e},useState:Ts,useDebugValue:Os,useDeferredValue:function(e){return vs().memoizedState=e},useTransition:function(){var e=Ts(!1),t=e[0];return e=js.bind(null,e[1]),vs().memoizedState=e,[t,e]},useMutableSource:function(){},useSyncExternalStore:function(e,t,n){var a=rs,i=vs();if(io){if(void 0===n)throw Error(o(407));n=n()}else{if(n=t(),null===El)throw Error(o(349));0!==(30&ss)||Ss(a,t,n)}i.memoizedState=n;var s={value:n,getSnapshot:t};return i.queue=s,Ms(Cs.bind(null,a,s,e),[e]),a.flags|=2048,Rs(9,xs.bind(null,a,s,n,t),void 0,null),n},useId:function(){var e=vs(),t=El.identifierPrefix;if(io){var n=Xi;t=":"+t+"R"+(n=(Ji&~(1<<32-st(Ji)-1)).toString(32)+n),0<(n=ps++)&&(t+="H"+n.toString(32)),t+=":"}else t=":"+t+"r"+(n=ms++).toString(32)+":";return e.memoizedState=t},unstable_isNewReconciler:!1},er={readContext:Ro,useCallback:Hs,useContext:Ro,useEffect:Gs,useImperativeHandle:Us,useInsertionEffect:Ls,useLayoutEffect:Fs,useMemo:Bs,useReducer:Is,useRef:zs,useState:function(){return Is(As)},useDebugValue:Os,useDeferredValue:function(e){return Ws(bs(),ls.memoizedState,e)},useTransition:function(){return[Is(As)[0],bs().memoizedState]},useMutableSource:ks,useSyncExternalStore:_s,useId:Vs,unstable_isNewReconciler:!1},tr={readContext:Ro,useCallback:Hs,useContext:Ro,useEffect:Gs,useImperativeHandle:Us,useInsertionEffect:Ls,useLayoutEffect:Fs,useMemo:Bs,useReducer:ws,useRef:zs,useState:function(){return ws(As)},useDebugValue:Os,useDeferredValue:function(e){var t=bs();return null===ls?t.memoizedState=e:Ws(t,ls.memoizedState,e)},useTransition:function(){return[ws(As)[0],bs().memoizedState]},useMutableSource:ks,useSyncExternalStore:_s,useId:Vs,unstable_isNewReconciler:!1};function nr(e,t){if(e&&e.defaultProps){for(var n in t=L({},t),e=e.defaultProps)void 0===t[n]&&(t[n]=e[n]);return t}return t}function ar(e,t,n,a){n=null===(n=n(a,t=e.memoizedState))||void 0===n?t:L({},t,n),e.memoizedState=n,0===e.lanes&&(e.updateQueue.baseState=n)}var ir={isMounted:function(e){return!!(e=e._reactInternals)&&He(e)===e},enqueueSetState:function(e,t,n){e=e._reactInternals;var a=ec(),i=tc(e),o=No(a,i);o.payload=t,void 0!==n&&null!==n&&(o.callback=n),null!==(t=Uo(e,o,i))&&(nc(t,e,i,a),Oo(t,e,i))},enqueueReplaceState:function(e,t,n){e=e._reactInternals;var a=ec(),i=tc(e),o=No(a,i);o.tag=1,o.payload=t,void 0!==n&&null!==n&&(o.callback=n),null!==(t=Uo(e,o,i))&&(nc(t,e,i,a),Oo(t,e,i))},enqueueForceUpdate:function(e,t){e=e._reactInternals;var n=ec(),a=tc(e),i=No(n,a);i.tag=2,void 0!==t&&null!==t&&(i.callback=t),null!==(t=Uo(e,i,a))&&(nc(t,e,a,n),Oo(t,e,a))}};function or(e,t,n,a,i,o,s){return"function"===typeof(e=e.stateNode).shouldComponentUpdate?e.shouldComponentUpdate(a,o,s):!t.prototype||!t.prototype.isPureReactComponent||(!la(n,a)||!la(i,o))}function sr(e,t,n){var a=!1,i=Ci,o=t.contextType;return"object"===typeof o&&null!==o?o=Ro(o):(i=zi(t)?Ti:Pi.current,o=(a=null!==(a=t.contextTypes)&&void 0!==a)?Ri(e,i):Ci),t=new t(n,o),e.memoizedState=null!==t.state&&void 0!==t.state?t.state:null,t.updater=ir,e.stateNode=t,t._reactInternals=e,a&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=i,e.__reactInternalMemoizedMaskedChildContext=o),t}function rr(e,t,n,a){e=t.state,"function"===typeof t.componentWillReceiveProps&&t.componentWillReceiveProps(n,a),"function"===typeof t.UNSAFE_componentWillReceiveProps&&t.UNSAFE_componentWillReceiveProps(n,a),t.state!==e&&ir.enqueueReplaceState(t,t.state,null)}function lr(e,t,n,a){var i=e.stateNode;i.props=n,i.state=e.memoizedState,i.refs={},Lo(e);var o=t.contextType;"object"===typeof o&&null!==o?i.context=Ro(o):(o=zi(t)?Ti:Pi.current,i.context=Ri(e,o)),i.state=e.memoizedState,"function"===typeof(o=t.getDerivedStateFromProps)&&(ar(e,t,o,n),i.state=e.memoizedState),"function"===typeof t.getDerivedStateFromProps||"function"===typeof i.getSnapshotBeforeUpdate||"function"!==typeof i.UNSAFE_componentWillMount&&"function"!==typeof i.componentWillMount||(t=i.state,"function"===typeof i.componentWillMount&&i.componentWillMount(),"function"===typeof i.UNSAFE_componentWillMount&&i.UNSAFE_componentWillMount(),t!==i.state&&ir.enqueueReplaceState(i,i.state,null),Bo(e,n,i,a),i.state=e.memoizedState),"function"===typeof i.componentDidMount&&(e.flags|=4194308)}function cr(e,t){try{var n="",a=t;do{n+=O(a),a=a.return}while(a);var i=n}catch(o){i="\nError generating stack: "+o.message+"\n"+o.stack}return{value:e,source:t,stack:i,digest:null}}function dr(e,t,n){return{value:e,source:null,stack:null!=n?n:null,digest:null!=t?t:null}}function ur(e,t){try{console.error(t.value)}catch(n){setTimeout((function(){throw n}))}}var pr="function"===typeof WeakMap?WeakMap:Map;function mr(e,t,n){(n=No(-1,n)).tag=3,n.payload={element:null};var a=t.value;return n.callback=function(){Wl||(Wl=!0,jl=a),ur(0,t)},n}function gr(e,t,n){(n=No(-1,n)).tag=3;var a=e.type.getDerivedStateFromError;if("function"===typeof a){var i=t.value;n.payload=function(){return a(i)},n.callback=function(){ur(0,t)}}var o=e.stateNode;return null!==o&&"function"===typeof o.componentDidCatch&&(n.callback=function(){ur(0,t),"function"!==typeof a&&(null===Vl?Vl=new Set([this]):Vl.add(this));var e=t.stack;this.componentDidCatch(t.value,{componentStack:null!==e?e:""})}),n}function hr(e,t,n){var a=e.pingCache;if(null===a){a=e.pingCache=new pr;var i=new Set;a.set(t,i)}else void 0===(i=a.get(t))&&(i=new Set,a.set(t,i));i.has(n)||(i.add(n),e=Sc.bind(null,e,t,n),t.then(e,e))}function fr(e){do{var t;if((t=13===e.tag)&&(t=null===(t=e.memoizedState)||null!==t.dehydrated),t)return e;e=e.return}while(null!==e);return null}function yr(e,t,n,a,i){return 0===(1&e.mode)?(e===t?e.flags|=65536:(e.flags|=128,n.flags|=131072,n.flags&=-52805,1===n.tag&&(null===n.alternate?n.tag=17:((t=No(-1,1)).tag=2,Uo(n,t,1))),n.lanes|=1),e):(e.flags|=65536,e.lanes=i,e)}var vr=A.ReactCurrentOwner,br=!1;function Ar(e,t,n,a){t.child=null===e?wo(t,null,n,a):Io(t,e.child,n,a)}function Ir(e,t,n,a,i){n=n.render;var o=t.ref;return To(t,i),a=fs(e,t,n,a,o,i),n=ys(),null===e||br?(io&&n&&eo(t),t.flags|=1,Ar(e,t,a,i),t.child):(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~i,Wr(e,t,i))}function wr(e,t,n,a,i){if(null===e){var o=n.type;return"function"!==typeof o||zc(o)||void 0!==o.defaultProps||null!==n.compare||void 0!==n.defaultProps?((e=qc(n.type,null,a,t,t.mode,i)).ref=t.ref,e.return=t,t.child=e):(t.tag=15,t.type=o,kr(e,t,o,a,i))}if(o=e.child,0===(e.lanes&i)){var s=o.memoizedProps;if((n=null!==(n=n.compare)?n:la)(s,a)&&e.ref===t.ref)return Wr(e,t,i)}return t.flags|=1,(e=Dc(o,a)).ref=t.ref,e.return=t,t.child=e}function kr(e,t,n,a,i){if(null!==e){var o=e.memoizedProps;if(la(o,a)&&e.ref===t.ref){if(br=!1,t.pendingProps=a=o,0===(e.lanes&i))return t.lanes=e.lanes,Wr(e,t,i);0!==(131072&e.flags)&&(br=!0)}}return xr(e,t,n,a,i)}function _r(e,t,n){var a=t.pendingProps,i=a.children,o=null!==e?e.memoizedState:null;if("hidden"===a.mode)if(0===(1&t.mode))t.memoizedState={baseLanes:0,cachePool:null,transitions:null},xi(Dl,zl),zl|=n;else{if(0===(1073741824&n))return e=null!==o?o.baseLanes|n:n,t.lanes=t.childLanes=1073741824,t.memoizedState={baseLanes:e,cachePool:null,transitions:null},t.updateQueue=null,xi(Dl,zl),zl|=e,null;t.memoizedState={baseLanes:0,cachePool:null,transitions:null},a=null!==o?o.baseLanes:n,xi(Dl,zl),zl|=a}else null!==o?(a=o.baseLanes|n,t.memoizedState=null):a=n,xi(Dl,zl),zl|=a;return Ar(e,t,i,n),t.child}function Sr(e,t){var n=t.ref;(null===e&&null!==n||null!==e&&e.ref!==n)&&(t.flags|=512,t.flags|=2097152)}function xr(e,t,n,a,i){var o=zi(n)?Ti:Pi.current;return o=Ri(t,o),To(t,i),n=fs(e,t,n,a,o,i),a=ys(),null===e||br?(io&&a&&eo(t),t.flags|=1,Ar(e,t,n,i),t.child):(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~i,Wr(e,t,i))}function Cr(e,t,n,a,i){if(zi(n)){var o=!0;Gi(t)}else o=!1;if(To(t,i),null===t.stateNode)Br(e,t),sr(t,n,a),lr(t,n,a,i),a=!0;else if(null===e){var s=t.stateNode,r=t.memoizedProps;s.props=r;var l=s.context,c=n.contextType;"object"===typeof c&&null!==c?c=Ro(c):c=Ri(t,c=zi(n)?Ti:Pi.current);var d=n.getDerivedStateFromProps,u="function"===typeof d||"function"===typeof s.getSnapshotBeforeUpdate;u||"function"!==typeof s.UNSAFE_componentWillReceiveProps&&"function"!==typeof s.componentWillReceiveProps||(r!==a||l!==c)&&rr(t,s,a,c),Go=!1;var p=t.memoizedState;s.state=p,Bo(t,a,s,i),l=t.memoizedState,r!==a||p!==l||Ei.current||Go?("function"===typeof d&&(ar(t,n,d,a),l=t.memoizedState),(r=Go||or(t,n,r,a,p,l,c))?(u||"function"!==typeof s.UNSAFE_componentWillMount&&"function"!==typeof s.componentWillMount||("function"===typeof s.componentWillMount&&s.componentWillMount(),"function"===typeof s.UNSAFE_componentWillMount&&s.UNSAFE_componentWillMount()),"function"===typeof s.componentDidMount&&(t.flags|=4194308)):("function"===typeof s.componentDidMount&&(t.flags|=4194308),t.memoizedProps=a,t.memoizedState=l),s.props=a,s.state=l,s.context=c,a=r):("function"===typeof s.componentDidMount&&(t.flags|=4194308),a=!1)}else{s=t.stateNode,Fo(e,t),r=t.memoizedProps,c=t.type===t.elementType?r:nr(t.type,r),s.props=c,u=t.pendingProps,p=s.context,"object"===typeof(l=n.contextType)&&null!==l?l=Ro(l):l=Ri(t,l=zi(n)?Ti:Pi.current);var m=n.getDerivedStateFromProps;(d="function"===typeof m||"function"===typeof s.getSnapshotBeforeUpdate)||"function"!==typeof s.UNSAFE_componentWillReceiveProps&&"function"!==typeof s.componentWillReceiveProps||(r!==u||p!==l)&&rr(t,s,a,l),Go=!1,p=t.memoizedState,s.state=p,Bo(t,a,s,i);var g=t.memoizedState;r!==u||p!==g||Ei.current||Go?("function"===typeof m&&(ar(t,n,m,a),g=t.memoizedState),(c=Go||or(t,n,c,a,p,g,l)||!1)?(d||"function"!==typeof s.UNSAFE_componentWillUpdate&&"function"!==typeof s.componentWillUpdate||("function"===typeof s.componentWillUpdate&&s.componentWillUpdate(a,g,l),"function"===typeof s.UNSAFE_componentWillUpdate&&s.UNSAFE_componentWillUpdate(a,g,l)),"function"===typeof s.componentDidUpdate&&(t.flags|=4),"function"===typeof s.getSnapshotBeforeUpdate&&(t.flags|=1024)):("function"!==typeof s.componentDidUpdate||r===e.memoizedProps&&p===e.memoizedState||(t.flags|=4),"function"!==typeof s.getSnapshotBeforeUpdate||r===e.memoizedProps&&p===e.memoizedState||(t.flags|=1024),t.memoizedProps=a,t.memoizedState=g),s.props=a,s.state=g,s.context=l,a=c):("function"!==typeof s.componentDidUpdate||r===e.memoizedProps&&p===e.memoizedState||(t.flags|=4),"function"!==typeof s.getSnapshotBeforeUpdate||r===e.memoizedProps&&p===e.memoizedState||(t.flags|=1024),a=!1)}return Pr(e,t,n,a,o,i)}function Pr(e,t,n,a,i,o){Sr(e,t);var s=0!==(128&t.flags);if(!a&&!s)return i&&Li(t,n,!1),Wr(e,t,o);a=t.stateNode,vr.current=t;var r=s&&"function"!==typeof n.getDerivedStateFromError?null:a.render();return t.flags|=1,null!==e&&s?(t.child=Io(t,e.child,null,o),t.child=Io(t,null,r,o)):Ar(e,t,r,o),t.memoizedState=a.state,i&&Li(t,n,!0),t.child}function Er(e){var t=e.stateNode;t.pendingContext?qi(0,t.pendingContext,t.pendingContext!==t.context):t.context&&qi(0,t.context,!1),Jo(e,t.containerInfo)}function Tr(e,t,n,a,i){return go(),ho(i),t.flags|=256,Ar(e,t,n,a),t.child}var Rr,zr,Dr,qr,Mr={dehydrated:null,treeContext:null,retryLane:0};function Gr(e){return{baseLanes:e,cachePool:null,transitions:null}}function Lr(e,t,n){var a,i=t.pendingProps,s=es.current,r=!1,l=0!==(128&t.flags);if((a=l)||(a=(null===e||null!==e.memoizedState)&&0!==(2&s)),a?(r=!0,t.flags&=-129):null!==e&&null===e.memoizedState||(s|=1),xi(es,1&s),null===e)return co(t),null!==(e=t.memoizedState)&&null!==(e=e.dehydrated)?(0===(1&t.mode)?t.lanes=1:"$!"===e.data?t.lanes=8:t.lanes=1073741824,null):(l=i.children,e=i.fallback,r?(i=t.mode,r=t.child,l={mode:"hidden",children:l},0===(1&i)&&null!==r?(r.childLanes=0,r.pendingProps=l):r=Gc(l,i,0,null),e=Mc(e,i,n,null),r.return=t,e.return=t,r.sibling=e,t.child=r,t.child.memoizedState=Gr(n),t.memoizedState=Mr,e):Fr(t,l));if(null!==(s=e.memoizedState)&&null!==(a=s.dehydrated))return function(e,t,n,a,i,s,r){if(n)return 256&t.flags?(t.flags&=-257,Nr(e,t,r,a=dr(Error(o(422))))):null!==t.memoizedState?(t.child=e.child,t.flags|=128,null):(s=a.fallback,i=t.mode,a=Gc({mode:"visible",children:a.children},i,0,null),(s=Mc(s,i,r,null)).flags|=2,a.return=t,s.return=t,a.sibling=s,t.child=a,0!==(1&t.mode)&&Io(t,e.child,null,r),t.child.memoizedState=Gr(r),t.memoizedState=Mr,s);if(0===(1&t.mode))return Nr(e,t,r,null);if("$!"===i.data){if(a=i.nextSibling&&i.nextSibling.dataset)var l=a.dgst;return a=l,Nr(e,t,r,a=dr(s=Error(o(419)),a,void 0))}if(l=0!==(r&e.childLanes),br||l){if(null!==(a=El)){switch(r&-r){case 4:i=2;break;case 16:i=8;break;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:i=32;break;case 536870912:i=268435456;break;default:i=0}0!==(i=0!==(i&(a.suspendedLanes|r))?0:i)&&i!==s.retryLane&&(s.retryLane=i,Mo(e,i),nc(a,e,i,-1))}return hc(),Nr(e,t,r,a=dr(Error(o(421))))}return"$?"===i.data?(t.flags|=128,t.child=e.child,t=Cc.bind(null,e),i._reactRetry=t,null):(e=s.treeContext,ao=ci(i.nextSibling),no=t,io=!0,oo=null,null!==e&&($i[Ki++]=Ji,$i[Ki++]=Xi,$i[Ki++]=Qi,Ji=e.id,Xi=e.overflow,Qi=t),t=Fr(t,a.children),t.flags|=4096,t)}(e,t,l,i,a,s,n);if(r){r=i.fallback,l=t.mode,a=(s=e.child).sibling;var c={mode:"hidden",children:i.children};return 0===(1&l)&&t.child!==s?((i=t.child).childLanes=0,i.pendingProps=c,t.deletions=null):(i=Dc(s,c)).subtreeFlags=14680064&s.subtreeFlags,null!==a?r=Dc(a,r):(r=Mc(r,l,n,null)).flags|=2,r.return=t,i.return=t,i.sibling=r,t.child=i,i=r,r=t.child,l=null===(l=e.child.memoizedState)?Gr(n):{baseLanes:l.baseLanes|n,cachePool:null,transitions:l.transitions},r.memoizedState=l,r.childLanes=e.childLanes&~n,t.memoizedState=Mr,i}return e=(r=e.child).sibling,i=Dc(r,{mode:"visible",children:i.children}),0===(1&t.mode)&&(i.lanes=n),i.return=t,i.sibling=null,null!==e&&(null===(n=t.deletions)?(t.deletions=[e],t.flags|=16):n.push(e)),t.child=i,t.memoizedState=null,i}function Fr(e,t){return(t=Gc({mode:"visible",children:t},e.mode,0,null)).return=e,e.child=t}function Nr(e,t,n,a){return null!==a&&ho(a),Io(t,e.child,null,n),(e=Fr(t,t.pendingProps.children)).flags|=2,t.memoizedState=null,e}function Ur(e,t,n){e.lanes|=t;var a=e.alternate;null!==a&&(a.lanes|=t),Eo(e.return,t,n)}function Or(e,t,n,a,i){var o=e.memoizedState;null===o?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:a,tail:n,tailMode:i}:(o.isBackwards=t,o.rendering=null,o.renderingStartTime=0,o.last=a,o.tail=n,o.tailMode=i)}function Hr(e,t,n){var a=t.pendingProps,i=a.revealOrder,o=a.tail;if(Ar(e,t,a.children,n),0!==(2&(a=es.current)))a=1&a|2,t.flags|=128;else{if(null!==e&&0!==(128&e.flags))e:for(e=t.child;null!==e;){if(13===e.tag)null!==e.memoizedState&&Ur(e,n,t);else if(19===e.tag)Ur(e,n,t);else if(null!==e.child){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;null===e.sibling;){if(null===e.return||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}a&=1}if(xi(es,a),0===(1&t.mode))t.memoizedState=null;else switch(i){case"forwards":for(n=t.child,i=null;null!==n;)null!==(e=n.alternate)&&null===ts(e)&&(i=n),n=n.sibling;null===(n=i)?(i=t.child,t.child=null):(i=n.sibling,n.sibling=null),Or(t,!1,i,n,o);break;case"backwards":for(n=null,i=t.child,t.child=null;null!==i;){if(null!==(e=i.alternate)&&null===ts(e)){t.child=i;break}e=i.sibling,i.sibling=n,n=i,i=e}Or(t,!0,n,null,o);break;case"together":Or(t,!1,null,null,void 0);break;default:t.memoizedState=null}return t.child}function Br(e,t){0===(1&t.mode)&&null!==e&&(e.alternate=null,t.alternate=null,t.flags|=2)}function Wr(e,t,n){if(null!==e&&(t.dependencies=e.dependencies),Gl|=t.lanes,0===(n&t.childLanes))return null;if(null!==e&&t.child!==e.child)throw Error(o(153));if(null!==t.child){for(n=Dc(e=t.child,e.pendingProps),t.child=n,n.return=t;null!==e.sibling;)e=e.sibling,(n=n.sibling=Dc(e,e.pendingProps)).return=t;n.sibling=null}return t.child}function jr(e,t){if(!io)switch(e.tailMode){case"hidden":t=e.tail;for(var n=null;null!==t;)null!==t.alternate&&(n=t),t=t.sibling;null===n?e.tail=null:n.sibling=null;break;case"collapsed":n=e.tail;for(var a=null;null!==n;)null!==n.alternate&&(a=n),n=n.sibling;null===a?t||null===e.tail?e.tail=null:e.tail.sibling=null:a.sibling=null}}function Vr(e){var t=null!==e.alternate&&e.alternate.child===e.child,n=0,a=0;if(t)for(var i=e.child;null!==i;)n|=i.lanes|i.childLanes,a|=14680064&i.subtreeFlags,a|=14680064&i.flags,i.return=e,i=i.sibling;else for(i=e.child;null!==i;)n|=i.lanes|i.childLanes,a|=i.subtreeFlags,a|=i.flags,i.return=e,i=i.sibling;return e.subtreeFlags|=a,e.childLanes=n,t}function $r(e,t,n){var a=t.pendingProps;switch(to(t),t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return Vr(t),null;case 1:case 17:return zi(t.type)&&Di(),Vr(t),null;case 3:return a=t.stateNode,Xo(),Si(Ei),Si(Pi),as(),a.pendingContext&&(a.context=a.pendingContext,a.pendingContext=null),null!==e&&null!==e.child||(po(t)?t.flags|=4:null===e||e.memoizedState.isDehydrated&&0===(256&t.flags)||(t.flags|=1024,null!==oo&&(sc(oo),oo=null))),zr(e,t),Vr(t),null;case 5:Zo(t);var i=Qo(Ko.current);if(n=t.type,null!==e&&null!=t.stateNode)Dr(e,t,n,a,i),e.ref!==t.ref&&(t.flags|=512,t.flags|=2097152);else{if(!a){if(null===t.stateNode)throw Error(o(166));return Vr(t),null}if(e=Qo(Vo.current),po(t)){a=t.stateNode,n=t.type;var s=t.memoizedProps;switch(a[pi]=t,a[mi]=s,e=0!==(1&t.mode),n){case"dialog":Na("cancel",a),Na("close",a);break;case"iframe":case"object":case"embed":Na("load",a);break;case"video":case"audio":for(i=0;i<Ma.length;i++)Na(Ma[i],a);break;case"source":Na("error",a);break;case"img":case"image":case"link":Na("error",a),Na("load",a);break;case"details":Na("toggle",a);break;case"input":J(a,s),Na("invalid",a);break;case"select":a._wrapperState={wasMultiple:!!s.multiple},Na("invalid",a);break;case"textarea":ie(a,s),Na("invalid",a)}for(var l in ve(n,s),i=null,s)if(s.hasOwnProperty(l)){var c=s[l];"children"===l?"string"===typeof c?a.textContent!==c&&(!0!==s.suppressHydrationWarning&&Ya(a.textContent,c,e),i=["children",c]):"number"===typeof c&&a.textContent!==""+c&&(!0!==s.suppressHydrationWarning&&Ya(a.textContent,c,e),i=["children",""+c]):r.hasOwnProperty(l)&&null!=c&&"onScroll"===l&&Na("scroll",a)}switch(n){case"input":V(a),Z(a,s,!0);break;case"textarea":V(a),se(a);break;case"select":case"option":break;default:"function"===typeof s.onClick&&(a.onclick=Za)}a=i,t.updateQueue=a,null!==a&&(t.flags|=4)}else{l=9===i.nodeType?i:i.ownerDocument,"http://www.w3.org/1999/xhtml"===e&&(e=re(n)),"http://www.w3.org/1999/xhtml"===e?"script"===n?((e=l.createElement("div")).innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):"string"===typeof a.is?e=l.createElement(n,{is:a.is}):(e=l.createElement(n),"select"===n&&(l=e,a.multiple?l.multiple=!0:a.size&&(l.size=a.size))):e=l.createElementNS(e,n),e[pi]=t,e[mi]=a,Rr(e,t,!1,!1),t.stateNode=e;e:{switch(l=be(n,a),n){case"dialog":Na("cancel",e),Na("close",e),i=a;break;case"iframe":case"object":case"embed":Na("load",e),i=a;break;case"video":case"audio":for(i=0;i<Ma.length;i++)Na(Ma[i],e);i=a;break;case"source":Na("error",e),i=a;break;case"img":case"image":case"link":Na("error",e),Na("load",e),i=a;break;case"details":Na("toggle",e),i=a;break;case"input":J(e,a),i=Q(e,a),Na("invalid",e);break;case"option":default:i=a;break;case"select":e._wrapperState={wasMultiple:!!a.multiple},i=L({},a,{value:void 0}),Na("invalid",e);break;case"textarea":ie(e,a),i=ae(e,a),Na("invalid",e)}for(s in ve(n,i),c=i)if(c.hasOwnProperty(s)){var d=c[s];"style"===s?fe(e,d):"dangerouslySetInnerHTML"===s?null!=(d=d?d.__html:void 0)&&ue(e,d):"children"===s?"string"===typeof d?("textarea"!==n||""!==d)&&pe(e,d):"number"===typeof d&&pe(e,""+d):"suppressContentEditableWarning"!==s&&"suppressHydrationWarning"!==s&&"autoFocus"!==s&&(r.hasOwnProperty(s)?null!=d&&"onScroll"===s&&Na("scroll",e):null!=d&&b(e,s,d,l))}switch(n){case"input":V(e),Z(e,a,!1);break;case"textarea":V(e),se(e);break;case"option":null!=a.value&&e.setAttribute("value",""+W(a.value));break;case"select":e.multiple=!!a.multiple,null!=(s=a.value)?ne(e,!!a.multiple,s,!1):null!=a.defaultValue&&ne(e,!!a.multiple,a.defaultValue,!0);break;default:"function"===typeof i.onClick&&(e.onclick=Za)}switch(n){case"button":case"input":case"select":case"textarea":a=!!a.autoFocus;break e;case"img":a=!0;break e;default:a=!1}}a&&(t.flags|=4)}null!==t.ref&&(t.flags|=512,t.flags|=2097152)}return Vr(t),null;case 6:if(e&&null!=t.stateNode)qr(e,t,e.memoizedProps,a);else{if("string"!==typeof a&&null===t.stateNode)throw Error(o(166));if(n=Qo(Ko.current),Qo(Vo.current),po(t)){if(a=t.stateNode,n=t.memoizedProps,a[pi]=t,(s=a.nodeValue!==n)&&null!==(e=no))switch(e.tag){case 3:Ya(a.nodeValue,n,0!==(1&e.mode));break;case 5:!0!==e.memoizedProps.suppressHydrationWarning&&Ya(a.nodeValue,n,0!==(1&e.mode))}s&&(t.flags|=4)}else(a=(9===n.nodeType?n:n.ownerDocument).createTextNode(a))[pi]=t,t.stateNode=a}return Vr(t),null;case 13:if(Si(es),a=t.memoizedState,null===e||null!==e.memoizedState&&null!==e.memoizedState.dehydrated){if(io&&null!==ao&&0!==(1&t.mode)&&0===(128&t.flags))mo(),go(),t.flags|=98560,s=!1;else if(s=po(t),null!==a&&null!==a.dehydrated){if(null===e){if(!s)throw Error(o(318));if(!(s=null!==(s=t.memoizedState)?s.dehydrated:null))throw Error(o(317));s[pi]=t}else go(),0===(128&t.flags)&&(t.memoizedState=null),t.flags|=4;Vr(t),s=!1}else null!==oo&&(sc(oo),oo=null),s=!0;if(!s)return 65536&t.flags?t:null}return 0!==(128&t.flags)?(t.lanes=n,t):((a=null!==a)!==(null!==e&&null!==e.memoizedState)&&a&&(t.child.flags|=8192,0!==(1&t.mode)&&(null===e||0!==(1&es.current)?0===ql&&(ql=3):hc())),null!==t.updateQueue&&(t.flags|=4),Vr(t),null);case 4:return Xo(),zr(e,t),null===e&&Ha(t.stateNode.containerInfo),Vr(t),null;case 10:return Po(t.type._context),Vr(t),null;case 19:if(Si(es),null===(s=t.memoizedState))return Vr(t),null;if(a=0!==(128&t.flags),null===(l=s.rendering))if(a)jr(s,!1);else{if(0!==ql||null!==e&&0!==(128&e.flags))for(e=t.child;null!==e;){if(null!==(l=ts(e))){for(t.flags|=128,jr(s,!1),null!==(a=l.updateQueue)&&(t.updateQueue=a,t.flags|=4),t.subtreeFlags=0,a=n,n=t.child;null!==n;)e=a,(s=n).flags&=14680066,null===(l=s.alternate)?(s.childLanes=0,s.lanes=e,s.child=null,s.subtreeFlags=0,s.memoizedProps=null,s.memoizedState=null,s.updateQueue=null,s.dependencies=null,s.stateNode=null):(s.childLanes=l.childLanes,s.lanes=l.lanes,s.child=l.child,s.subtreeFlags=0,s.deletions=null,s.memoizedProps=l.memoizedProps,s.memoizedState=l.memoizedState,s.updateQueue=l.updateQueue,s.type=l.type,e=l.dependencies,s.dependencies=null===e?null:{lanes:e.lanes,firstContext:e.firstContext}),n=n.sibling;return xi(es,1&es.current|2),t.child}e=e.sibling}null!==s.tail&&Xe()>Hl&&(t.flags|=128,a=!0,jr(s,!1),t.lanes=4194304)}else{if(!a)if(null!==(e=ts(l))){if(t.flags|=128,a=!0,null!==(n=e.updateQueue)&&(t.updateQueue=n,t.flags|=4),jr(s,!0),null===s.tail&&"hidden"===s.tailMode&&!l.alternate&&!io)return Vr(t),null}else 2*Xe()-s.renderingStartTime>Hl&&1073741824!==n&&(t.flags|=128,a=!0,jr(s,!1),t.lanes=4194304);s.isBackwards?(l.sibling=t.child,t.child=l):(null!==(n=s.last)?n.sibling=l:t.child=l,s.last=l)}return null!==s.tail?(t=s.tail,s.rendering=t,s.tail=t.sibling,s.renderingStartTime=Xe(),t.sibling=null,n=es.current,xi(es,a?1&n|2:1&n),t):(Vr(t),null);case 22:case 23:return uc(),a=null!==t.memoizedState,null!==e&&null!==e.memoizedState!==a&&(t.flags|=8192),a&&0!==(1&t.mode)?0!==(1073741824&zl)&&(Vr(t),6&t.subtreeFlags&&(t.flags|=8192)):Vr(t),null;case 24:case 25:return null}throw Error(o(156,t.tag))}function Kr(e,t){switch(to(t),t.tag){case 1:return zi(t.type)&&Di(),65536&(e=t.flags)?(t.flags=-65537&e|128,t):null;case 3:return Xo(),Si(Ei),Si(Pi),as(),0!==(65536&(e=t.flags))&&0===(128&e)?(t.flags=-65537&e|128,t):null;case 5:return Zo(t),null;case 13:if(Si(es),null!==(e=t.memoizedState)&&null!==e.dehydrated){if(null===t.alternate)throw Error(o(340));go()}return 65536&(e=t.flags)?(t.flags=-65537&e|128,t):null;case 19:return Si(es),null;case 4:return Xo(),null;case 10:return Po(t.type._context),null;case 22:case 23:return uc(),null;default:return null}}Rr=function(e,t){for(var n=t.child;null!==n;){if(5===n.tag||6===n.tag)e.appendChild(n.stateNode);else if(4!==n.tag&&null!==n.child){n.child.return=n,n=n.child;continue}if(n===t)break;for(;null===n.sibling;){if(null===n.return||n.return===t)return;n=n.return}n.sibling.return=n.return,n=n.sibling}},zr=function(){},Dr=function(e,t,n,a){var i=e.memoizedProps;if(i!==a){e=t.stateNode,Qo(Vo.current);var o,s=null;switch(n){case"input":i=Q(e,i),a=Q(e,a),s=[];break;case"select":i=L({},i,{value:void 0}),a=L({},a,{value:void 0}),s=[];break;case"textarea":i=ae(e,i),a=ae(e,a),s=[];break;default:"function"!==typeof i.onClick&&"function"===typeof a.onClick&&(e.onclick=Za)}for(d in ve(n,a),n=null,i)if(!a.hasOwnProperty(d)&&i.hasOwnProperty(d)&&null!=i[d])if("style"===d){var l=i[d];for(o in l)l.hasOwnProperty(o)&&(n||(n={}),n[o]="")}else"dangerouslySetInnerHTML"!==d&&"children"!==d&&"suppressContentEditableWarning"!==d&&"suppressHydrationWarning"!==d&&"autoFocus"!==d&&(r.hasOwnProperty(d)?s||(s=[]):(s=s||[]).push(d,null));for(d in a){var c=a[d];if(l=null!=i?i[d]:void 0,a.hasOwnProperty(d)&&c!==l&&(null!=c||null!=l))if("style"===d)if(l){for(o in l)!l.hasOwnProperty(o)||c&&c.hasOwnProperty(o)||(n||(n={}),n[o]="");for(o in c)c.hasOwnProperty(o)&&l[o]!==c[o]&&(n||(n={}),n[o]=c[o])}else n||(s||(s=[]),s.push(d,n)),n=c;else"dangerouslySetInnerHTML"===d?(c=c?c.__html:void 0,l=l?l.__html:void 0,null!=c&&l!==c&&(s=s||[]).push(d,c)):"children"===d?"string"!==typeof c&&"number"!==typeof c||(s=s||[]).push(d,""+c):"suppressContentEditableWarning"!==d&&"suppressHydrationWarning"!==d&&(r.hasOwnProperty(d)?(null!=c&&"onScroll"===d&&Na("scroll",e),s||l===c||(s=[])):(s=s||[]).push(d,c))}n&&(s=s||[]).push("style",n);var d=s;(t.updateQueue=d)&&(t.flags|=4)}},qr=function(e,t,n,a){n!==a&&(t.flags|=4)};var Qr=!1,Jr=!1,Xr="function"===typeof WeakSet?WeakSet:Set,Yr=null;function Zr(e,t){var n=e.ref;if(null!==n)if("function"===typeof n)try{n(null)}catch(a){_c(e,t,a)}else n.current=null}function el(e,t,n){try{n()}catch(a){_c(e,t,a)}}var tl=!1;function nl(e,t,n){var a=t.updateQueue;if(null!==(a=null!==a?a.lastEffect:null)){var i=a=a.next;do{if((i.tag&e)===e){var o=i.destroy;i.destroy=void 0,void 0!==o&&el(t,n,o)}i=i.next}while(i!==a)}}function al(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var n=t=t.next;do{if((n.tag&e)===e){var a=n.create;n.destroy=a()}n=n.next}while(n!==t)}}function il(e){var t=e.ref;if(null!==t){var n=e.stateNode;e.tag,e=n,"function"===typeof t?t(e):t.current=e}}function ol(e){var t=e.alternate;null!==t&&(e.alternate=null,ol(t)),e.child=null,e.deletions=null,e.sibling=null,5===e.tag&&(null!==(t=e.stateNode)&&(delete t[pi],delete t[mi],delete t[hi],delete t[fi],delete t[yi])),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}function sl(e){return 5===e.tag||3===e.tag||4===e.tag}function rl(e){e:for(;;){for(;null===e.sibling;){if(null===e.return||sl(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;5!==e.tag&&6!==e.tag&&18!==e.tag;){if(2&e.flags)continue e;if(null===e.child||4===e.tag)continue e;e.child.return=e,e=e.child}if(!(2&e.flags))return e.stateNode}}function ll(e,t,n){var a=e.tag;if(5===a||6===a)e=e.stateNode,t?8===n.nodeType?n.parentNode.insertBefore(e,t):n.insertBefore(e,t):(8===n.nodeType?(t=n.parentNode).insertBefore(e,n):(t=n).appendChild(e),null!==(n=n._reactRootContainer)&&void 0!==n||null!==t.onclick||(t.onclick=Za));else if(4!==a&&null!==(e=e.child))for(ll(e,t,n),e=e.sibling;null!==e;)ll(e,t,n),e=e.sibling}function cl(e,t,n){var a=e.tag;if(5===a||6===a)e=e.stateNode,t?n.insertBefore(e,t):n.appendChild(e);else if(4!==a&&null!==(e=e.child))for(cl(e,t,n),e=e.sibling;null!==e;)cl(e,t,n),e=e.sibling}var dl=null,ul=!1;function pl(e,t,n){for(n=n.child;null!==n;)ml(e,t,n),n=n.sibling}function ml(e,t,n){if(ot&&"function"===typeof ot.onCommitFiberUnmount)try{ot.onCommitFiberUnmount(it,n)}catch(r){}switch(n.tag){case 5:Jr||Zr(n,t);case 6:var a=dl,i=ul;dl=null,pl(e,t,n),ul=i,null!==(dl=a)&&(ul?(e=dl,n=n.stateNode,8===e.nodeType?e.parentNode.removeChild(n):e.removeChild(n)):dl.removeChild(n.stateNode));break;case 18:null!==dl&&(ul?(e=dl,n=n.stateNode,8===e.nodeType?li(e.parentNode,n):1===e.nodeType&&li(e,n),Ht(e)):li(dl,n.stateNode));break;case 4:a=dl,i=ul,dl=n.stateNode.containerInfo,ul=!0,pl(e,t,n),dl=a,ul=i;break;case 0:case 11:case 14:case 15:if(!Jr&&(null!==(a=n.updateQueue)&&null!==(a=a.lastEffect))){i=a=a.next;do{var o=i,s=o.destroy;o=o.tag,void 0!==s&&(0!==(2&o)||0!==(4&o))&&el(n,t,s),i=i.next}while(i!==a)}pl(e,t,n);break;case 1:if(!Jr&&(Zr(n,t),"function"===typeof(a=n.stateNode).componentWillUnmount))try{a.props=n.memoizedProps,a.state=n.memoizedState,a.componentWillUnmount()}catch(r){_c(n,t,r)}pl(e,t,n);break;case 21:pl(e,t,n);break;case 22:1&n.mode?(Jr=(a=Jr)||null!==n.memoizedState,pl(e,t,n),Jr=a):pl(e,t,n);break;default:pl(e,t,n)}}function gl(e){var t=e.updateQueue;if(null!==t){e.updateQueue=null;var n=e.stateNode;null===n&&(n=e.stateNode=new Xr),t.forEach((function(t){var a=Pc.bind(null,e,t);n.has(t)||(n.add(t),t.then(a,a))}))}}function hl(e,t){var n=t.deletions;if(null!==n)for(var a=0;a<n.length;a++){var i=n[a];try{var s=e,r=t,l=r;e:for(;null!==l;){switch(l.tag){case 5:dl=l.stateNode,ul=!1;break e;case 3:case 4:dl=l.stateNode.containerInfo,ul=!0;break e}l=l.return}if(null===dl)throw Error(o(160));ml(s,r,i),dl=null,ul=!1;var c=i.alternate;null!==c&&(c.return=null),i.return=null}catch(d){_c(i,t,d)}}if(12854&t.subtreeFlags)for(t=t.child;null!==t;)fl(t,e),t=t.sibling}function fl(e,t){var n=e.alternate,a=e.flags;switch(e.tag){case 0:case 11:case 14:case 15:if(hl(t,e),yl(e),4&a){try{nl(3,e,e.return),al(3,e)}catch(f){_c(e,e.return,f)}try{nl(5,e,e.return)}catch(f){_c(e,e.return,f)}}break;case 1:hl(t,e),yl(e),512&a&&null!==n&&Zr(n,n.return);break;case 5:if(hl(t,e),yl(e),512&a&&null!==n&&Zr(n,n.return),32&e.flags){var i=e.stateNode;try{pe(i,"")}catch(f){_c(e,e.return,f)}}if(4&a&&null!=(i=e.stateNode)){var s=e.memoizedProps,r=null!==n?n.memoizedProps:s,l=e.type,c=e.updateQueue;if(e.updateQueue=null,null!==c)try{"input"===l&&"radio"===s.type&&null!=s.name&&X(i,s),be(l,r);var d=be(l,s);for(r=0;r<c.length;r+=2){var u=c[r],p=c[r+1];"style"===u?fe(i,p):"dangerouslySetInnerHTML"===u?ue(i,p):"children"===u?pe(i,p):b(i,u,p,d)}switch(l){case"input":Y(i,s);break;case"textarea":oe(i,s);break;case"select":var m=i._wrapperState.wasMultiple;i._wrapperState.wasMultiple=!!s.multiple;var g=s.value;null!=g?ne(i,!!s.multiple,g,!1):m!==!!s.multiple&&(null!=s.defaultValue?ne(i,!!s.multiple,s.defaultValue,!0):ne(i,!!s.multiple,s.multiple?[]:"",!1))}i[mi]=s}catch(f){_c(e,e.return,f)}}break;case 6:if(hl(t,e),yl(e),4&a){if(null===e.stateNode)throw Error(o(162));i=e.stateNode,s=e.memoizedProps;try{i.nodeValue=s}catch(f){_c(e,e.return,f)}}break;case 3:if(hl(t,e),yl(e),4&a&&null!==n&&n.memoizedState.isDehydrated)try{Ht(t.containerInfo)}catch(f){_c(e,e.return,f)}break;case 4:default:hl(t,e),yl(e);break;case 13:hl(t,e),yl(e),8192&(i=e.child).flags&&(s=null!==i.memoizedState,i.stateNode.isHidden=s,!s||null!==i.alternate&&null!==i.alternate.memoizedState||(Ol=Xe())),4&a&&gl(e);break;case 22:if(u=null!==n&&null!==n.memoizedState,1&e.mode?(Jr=(d=Jr)||u,hl(t,e),Jr=d):hl(t,e),yl(e),8192&a){if(d=null!==e.memoizedState,(e.stateNode.isHidden=d)&&!u&&0!==(1&e.mode))for(Yr=e,u=e.child;null!==u;){for(p=Yr=u;null!==Yr;){switch(g=(m=Yr).child,m.tag){case 0:case 11:case 14:case 15:nl(4,m,m.return);break;case 1:Zr(m,m.return);var h=m.stateNode;if("function"===typeof h.componentWillUnmount){a=m,n=m.return;try{t=a,h.props=t.memoizedProps,h.state=t.memoizedState,h.componentWillUnmount()}catch(f){_c(a,n,f)}}break;case 5:Zr(m,m.return);break;case 22:if(null!==m.memoizedState){Il(p);continue}}null!==g?(g.return=m,Yr=g):Il(p)}u=u.sibling}e:for(u=null,p=e;;){if(5===p.tag){if(null===u){u=p;try{i=p.stateNode,d?"function"===typeof(s=i.style).setProperty?s.setProperty("display","none","important"):s.display="none":(l=p.stateNode,r=void 0!==(c=p.memoizedProps.style)&&null!==c&&c.hasOwnProperty("display")?c.display:null,l.style.display=he("display",r))}catch(f){_c(e,e.return,f)}}}else if(6===p.tag){if(null===u)try{p.stateNode.nodeValue=d?"":p.memoizedProps}catch(f){_c(e,e.return,f)}}else if((22!==p.tag&&23!==p.tag||null===p.memoizedState||p===e)&&null!==p.child){p.child.return=p,p=p.child;continue}if(p===e)break e;for(;null===p.sibling;){if(null===p.return||p.return===e)break e;u===p&&(u=null),p=p.return}u===p&&(u=null),p.sibling.return=p.return,p=p.sibling}}break;case 19:hl(t,e),yl(e),4&a&&gl(e);case 21:}}function yl(e){var t=e.flags;if(2&t){try{e:{for(var n=e.return;null!==n;){if(sl(n)){var a=n;break e}n=n.return}throw Error(o(160))}switch(a.tag){case 5:var i=a.stateNode;32&a.flags&&(pe(i,""),a.flags&=-33),cl(e,rl(e),i);break;case 3:case 4:var s=a.stateNode.containerInfo;ll(e,rl(e),s);break;default:throw Error(o(161))}}catch(r){_c(e,e.return,r)}e.flags&=-3}4096&t&&(e.flags&=-4097)}function vl(e,t,n){Yr=e,bl(e,t,n)}function bl(e,t,n){for(var a=0!==(1&e.mode);null!==Yr;){var i=Yr,o=i.child;if(22===i.tag&&a){var s=null!==i.memoizedState||Qr;if(!s){var r=i.alternate,l=null!==r&&null!==r.memoizedState||Jr;r=Qr;var c=Jr;if(Qr=s,(Jr=l)&&!c)for(Yr=i;null!==Yr;)l=(s=Yr).child,22===s.tag&&null!==s.memoizedState?wl(i):null!==l?(l.return=s,Yr=l):wl(i);for(;null!==o;)Yr=o,bl(o,t,n),o=o.sibling;Yr=i,Qr=r,Jr=c}Al(e)}else 0!==(8772&i.subtreeFlags)&&null!==o?(o.return=i,Yr=o):Al(e)}}function Al(e){for(;null!==Yr;){var t=Yr;if(0!==(8772&t.flags)){var n=t.alternate;try{if(0!==(8772&t.flags))switch(t.tag){case 0:case 11:case 15:Jr||al(5,t);break;case 1:var a=t.stateNode;if(4&t.flags&&!Jr)if(null===n)a.componentDidMount();else{var i=t.elementType===t.type?n.memoizedProps:nr(t.type,n.memoizedProps);a.componentDidUpdate(i,n.memoizedState,a.__reactInternalSnapshotBeforeUpdate)}var s=t.updateQueue;null!==s&&Wo(t,s,a);break;case 3:var r=t.updateQueue;if(null!==r){if(n=null,null!==t.child)switch(t.child.tag){case 5:case 1:n=t.child.stateNode}Wo(t,r,n)}break;case 5:var l=t.stateNode;if(null===n&&4&t.flags){n=l;var c=t.memoizedProps;switch(t.type){case"button":case"input":case"select":case"textarea":c.autoFocus&&n.focus();break;case"img":c.src&&(n.src=c.src)}}break;case 6:case 4:case 12:case 19:case 17:case 21:case 22:case 23:case 25:break;case 13:if(null===t.memoizedState){var d=t.alternate;if(null!==d){var u=d.memoizedState;if(null!==u){var p=u.dehydrated;null!==p&&Ht(p)}}}break;default:throw Error(o(163))}Jr||512&t.flags&&il(t)}catch(m){_c(t,t.return,m)}}if(t===e){Yr=null;break}if(null!==(n=t.sibling)){n.return=t.return,Yr=n;break}Yr=t.return}}function Il(e){for(;null!==Yr;){var t=Yr;if(t===e){Yr=null;break}var n=t.sibling;if(null!==n){n.return=t.return,Yr=n;break}Yr=t.return}}function wl(e){for(;null!==Yr;){var t=Yr;try{switch(t.tag){case 0:case 11:case 15:var n=t.return;try{al(4,t)}catch(l){_c(t,n,l)}break;case 1:var a=t.stateNode;if("function"===typeof a.componentDidMount){var i=t.return;try{a.componentDidMount()}catch(l){_c(t,i,l)}}var o=t.return;try{il(t)}catch(l){_c(t,o,l)}break;case 5:var s=t.return;try{il(t)}catch(l){_c(t,s,l)}}}catch(l){_c(t,t.return,l)}if(t===e){Yr=null;break}var r=t.sibling;if(null!==r){r.return=t.return,Yr=r;break}Yr=t.return}}var kl,_l=Math.ceil,Sl=A.ReactCurrentDispatcher,xl=A.ReactCurrentOwner,Cl=A.ReactCurrentBatchConfig,Pl=0,El=null,Tl=null,Rl=0,zl=0,Dl=_i(0),ql=0,Ml=null,Gl=0,Ll=0,Fl=0,Nl=null,Ul=null,Ol=0,Hl=1/0,Bl=null,Wl=!1,jl=null,Vl=null,$l=!1,Kl=null,Ql=0,Jl=0,Xl=null,Yl=-1,Zl=0;function ec(){return 0!==(6&Pl)?Xe():-1!==Yl?Yl:Yl=Xe()}function tc(e){return 0===(1&e.mode)?1:0!==(2&Pl)&&0!==Rl?Rl&-Rl:null!==fo.transition?(0===Zl&&(Zl=ht()),Zl):0!==(e=bt)?e:e=void 0===(e=window.event)?16:Jt(e.type)}function nc(e,t,n,a){if(50<Jl)throw Jl=0,Xl=null,Error(o(185));yt(e,n,a),0!==(2&Pl)&&e===El||(e===El&&(0===(2&Pl)&&(Ll|=n),4===ql&&rc(e,Rl)),ac(e,a),1===n&&0===Pl&&0===(1&t.mode)&&(Hl=Xe()+500,Ni&&Hi()))}function ac(e,t){var n=e.callbackNode;!function(e,t){for(var n=e.suspendedLanes,a=e.pingedLanes,i=e.expirationTimes,o=e.pendingLanes;0<o;){var s=31-st(o),r=1<<s,l=i[s];-1===l?0!==(r&n)&&0===(r&a)||(i[s]=mt(r,t)):l<=t&&(e.expiredLanes|=r),o&=~r}}(e,t);var a=pt(e,e===El?Rl:0);if(0===a)null!==n&&Ke(n),e.callbackNode=null,e.callbackPriority=0;else if(t=a&-a,e.callbackPriority!==t){if(null!=n&&Ke(n),1===t)0===e.tag?function(e){Ni=!0,Oi(e)}(lc.bind(null,e)):Oi(lc.bind(null,e)),si((function(){0===(6&Pl)&&Hi()})),n=null;else{switch(At(a)){case 1:n=Ze;break;case 4:n=et;break;case 16:default:n=tt;break;case 536870912:n=at}n=Ec(n,ic.bind(null,e))}e.callbackPriority=t,e.callbackNode=n}}function ic(e,t){if(Yl=-1,Zl=0,0!==(6&Pl))throw Error(o(327));var n=e.callbackNode;if(wc()&&e.callbackNode!==n)return null;var a=pt(e,e===El?Rl:0);if(0===a)return null;if(0!==(30&a)||0!==(a&e.expiredLanes)||t)t=fc(e,a);else{t=a;var i=Pl;Pl|=2;var s=gc();for(El===e&&Rl===t||(Bl=null,Hl=Xe()+500,pc(e,t));;)try{vc();break}catch(l){mc(e,l)}Co(),Sl.current=s,Pl=i,null!==Tl?t=0:(El=null,Rl=0,t=ql)}if(0!==t){if(2===t&&(0!==(i=gt(e))&&(a=i,t=oc(e,i))),1===t)throw n=Ml,pc(e,0),rc(e,a),ac(e,Xe()),n;if(6===t)rc(e,a);else{if(i=e.current.alternate,0===(30&a)&&!function(e){for(var t=e;;){if(16384&t.flags){var n=t.updateQueue;if(null!==n&&null!==(n=n.stores))for(var a=0;a<n.length;a++){var i=n[a],o=i.getSnapshot;i=i.value;try{if(!ra(o(),i))return!1}catch(r){return!1}}}if(n=t.child,16384&t.subtreeFlags&&null!==n)n.return=t,t=n;else{if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}(i)&&(2===(t=fc(e,a))&&(0!==(s=gt(e))&&(a=s,t=oc(e,s))),1===t))throw n=Ml,pc(e,0),rc(e,a),ac(e,Xe()),n;switch(e.finishedWork=i,e.finishedLanes=a,t){case 0:case 1:throw Error(o(345));case 2:case 5:Ic(e,Ul,Bl);break;case 3:if(rc(e,a),(130023424&a)===a&&10<(t=Ol+500-Xe())){if(0!==pt(e,0))break;if(((i=e.suspendedLanes)&a)!==a){ec(),e.pingedLanes|=e.suspendedLanes&i;break}e.timeoutHandle=ai(Ic.bind(null,e,Ul,Bl),t);break}Ic(e,Ul,Bl);break;case 4:if(rc(e,a),(4194240&a)===a)break;for(t=e.eventTimes,i=-1;0<a;){var r=31-st(a);s=1<<r,(r=t[r])>i&&(i=r),a&=~s}if(a=i,10<(a=(120>(a=Xe()-a)?120:480>a?480:1080>a?1080:1920>a?1920:3e3>a?3e3:4320>a?4320:1960*_l(a/1960))-a)){e.timeoutHandle=ai(Ic.bind(null,e,Ul,Bl),a);break}Ic(e,Ul,Bl);break;default:throw Error(o(329))}}}return ac(e,Xe()),e.callbackNode===n?ic.bind(null,e):null}function oc(e,t){var n=Nl;return e.current.memoizedState.isDehydrated&&(pc(e,t).flags|=256),2!==(e=fc(e,t))&&(t=Ul,Ul=n,null!==t&&sc(t)),e}function sc(e){null===Ul?Ul=e:Ul.push.apply(Ul,e)}function rc(e,t){for(t&=~Fl,t&=~Ll,e.suspendedLanes|=t,e.pingedLanes&=~t,e=e.expirationTimes;0<t;){var n=31-st(t),a=1<<n;e[n]=-1,t&=~a}}function lc(e){if(0!==(6&Pl))throw Error(o(327));wc();var t=pt(e,0);if(0===(1&t))return ac(e,Xe()),null;var n=fc(e,t);if(0!==e.tag&&2===n){var a=gt(e);0!==a&&(t=a,n=oc(e,a))}if(1===n)throw n=Ml,pc(e,0),rc(e,t),ac(e,Xe()),n;if(6===n)throw Error(o(345));return e.finishedWork=e.current.alternate,e.finishedLanes=t,Ic(e,Ul,Bl),ac(e,Xe()),null}function cc(e,t){var n=Pl;Pl|=1;try{return e(t)}finally{0===(Pl=n)&&(Hl=Xe()+500,Ni&&Hi())}}function dc(e){null!==Kl&&0===Kl.tag&&0===(6&Pl)&&wc();var t=Pl;Pl|=1;var n=Cl.transition,a=bt;try{if(Cl.transition=null,bt=1,e)return e()}finally{bt=a,Cl.transition=n,0===(6&(Pl=t))&&Hi()}}function uc(){zl=Dl.current,Si(Dl)}function pc(e,t){e.finishedWork=null,e.finishedLanes=0;var n=e.timeoutHandle;if(-1!==n&&(e.timeoutHandle=-1,ii(n)),null!==Tl)for(n=Tl.return;null!==n;){var a=n;switch(to(a),a.tag){case 1:null!==(a=a.type.childContextTypes)&&void 0!==a&&Di();break;case 3:Xo(),Si(Ei),Si(Pi),as();break;case 5:Zo(a);break;case 4:Xo();break;case 13:case 19:Si(es);break;case 10:Po(a.type._context);break;case 22:case 23:uc()}n=n.return}if(El=e,Tl=e=Dc(e.current,null),Rl=zl=t,ql=0,Ml=null,Fl=Ll=Gl=0,Ul=Nl=null,null!==zo){for(t=0;t<zo.length;t++)if(null!==(a=(n=zo[t]).interleaved)){n.interleaved=null;var i=a.next,o=n.pending;if(null!==o){var s=o.next;o.next=i,a.next=s}n.pending=a}zo=null}return e}function mc(e,t){for(;;){var n=Tl;try{if(Co(),is.current=Ys,ds){for(var a=rs.memoizedState;null!==a;){var i=a.queue;null!==i&&(i.pending=null),a=a.next}ds=!1}if(ss=0,cs=ls=rs=null,us=!1,ps=0,xl.current=null,null===n||null===n.return){ql=1,Ml=t,Tl=null;break}e:{var s=e,r=n.return,l=n,c=t;if(t=Rl,l.flags|=32768,null!==c&&"object"===typeof c&&"function"===typeof c.then){var d=c,u=l,p=u.tag;if(0===(1&u.mode)&&(0===p||11===p||15===p)){var m=u.alternate;m?(u.updateQueue=m.updateQueue,u.memoizedState=m.memoizedState,u.lanes=m.lanes):(u.updateQueue=null,u.memoizedState=null)}var g=fr(r);if(null!==g){g.flags&=-257,yr(g,r,l,0,t),1&g.mode&&hr(s,d,t),c=d;var h=(t=g).updateQueue;if(null===h){var f=new Set;f.add(c),t.updateQueue=f}else h.add(c);break e}if(0===(1&t)){hr(s,d,t),hc();break e}c=Error(o(426))}else if(io&&1&l.mode){var y=fr(r);if(null!==y){0===(65536&y.flags)&&(y.flags|=256),yr(y,r,l,0,t),ho(cr(c,l));break e}}s=c=cr(c,l),4!==ql&&(ql=2),null===Nl?Nl=[s]:Nl.push(s),s=r;do{switch(s.tag){case 3:s.flags|=65536,t&=-t,s.lanes|=t,Ho(s,mr(0,c,t));break e;case 1:l=c;var v=s.type,b=s.stateNode;if(0===(128&s.flags)&&("function"===typeof v.getDerivedStateFromError||null!==b&&"function"===typeof b.componentDidCatch&&(null===Vl||!Vl.has(b)))){s.flags|=65536,t&=-t,s.lanes|=t,Ho(s,gr(s,l,t));break e}}s=s.return}while(null!==s)}Ac(n)}catch(A){t=A,Tl===n&&null!==n&&(Tl=n=n.return);continue}break}}function gc(){var e=Sl.current;return Sl.current=Ys,null===e?Ys:e}function hc(){0!==ql&&3!==ql&&2!==ql||(ql=4),null===El||0===(268435455&Gl)&&0===(268435455&Ll)||rc(El,Rl)}function fc(e,t){var n=Pl;Pl|=2;var a=gc();for(El===e&&Rl===t||(Bl=null,pc(e,t));;)try{yc();break}catch(i){mc(e,i)}if(Co(),Pl=n,Sl.current=a,null!==Tl)throw Error(o(261));return El=null,Rl=0,ql}function yc(){for(;null!==Tl;)bc(Tl)}function vc(){for(;null!==Tl&&!Qe();)bc(Tl)}function bc(e){var t=kl(e.alternate,e,zl);e.memoizedProps=e.pendingProps,null===t?Ac(e):Tl=t,xl.current=null}function Ac(e){var t=e;do{var n=t.alternate;if(e=t.return,0===(32768&t.flags)){if(null!==(n=$r(n,t,zl)))return void(Tl=n)}else{if(null!==(n=Kr(n,t)))return n.flags&=32767,void(Tl=n);if(null===e)return ql=6,void(Tl=null);e.flags|=32768,e.subtreeFlags=0,e.deletions=null}if(null!==(t=t.sibling))return void(Tl=t);Tl=t=e}while(null!==t);0===ql&&(ql=5)}function Ic(e,t,n){var a=bt,i=Cl.transition;try{Cl.transition=null,bt=1,function(e,t,n,a){do{wc()}while(null!==Kl);if(0!==(6&Pl))throw Error(o(327));n=e.finishedWork;var i=e.finishedLanes;if(null===n)return null;if(e.finishedWork=null,e.finishedLanes=0,n===e.current)throw Error(o(177));e.callbackNode=null,e.callbackPriority=0;var s=n.lanes|n.childLanes;if(function(e,t){var n=e.pendingLanes&~t;e.pendingLanes=t,e.suspendedLanes=0,e.pingedLanes=0,e.expiredLanes&=t,e.mutableReadLanes&=t,e.entangledLanes&=t,t=e.entanglements;var a=e.eventTimes;for(e=e.expirationTimes;0<n;){var i=31-st(n),o=1<<i;t[i]=0,a[i]=-1,e[i]=-1,n&=~o}}(e,s),e===El&&(Tl=El=null,Rl=0),0===(2064&n.subtreeFlags)&&0===(2064&n.flags)||$l||($l=!0,Ec(tt,(function(){return wc(),null}))),s=0!==(15990&n.flags),0!==(15990&n.subtreeFlags)||s){s=Cl.transition,Cl.transition=null;var r=bt;bt=1;var l=Pl;Pl|=4,xl.current=null,function(e,t){if(ei=Wt,ma(e=pa())){if("selectionStart"in e)var n={start:e.selectionStart,end:e.selectionEnd};else e:{var a=(n=(n=e.ownerDocument)&&n.defaultView||window).getSelection&&n.getSelection();if(a&&0!==a.rangeCount){n=a.anchorNode;var i=a.anchorOffset,s=a.focusNode;a=a.focusOffset;try{n.nodeType,s.nodeType}catch(I){n=null;break e}var r=0,l=-1,c=-1,d=0,u=0,p=e,m=null;t:for(;;){for(var g;p!==n||0!==i&&3!==p.nodeType||(l=r+i),p!==s||0!==a&&3!==p.nodeType||(c=r+a),3===p.nodeType&&(r+=p.nodeValue.length),null!==(g=p.firstChild);)m=p,p=g;for(;;){if(p===e)break t;if(m===n&&++d===i&&(l=r),m===s&&++u===a&&(c=r),null!==(g=p.nextSibling))break;m=(p=m).parentNode}p=g}n=-1===l||-1===c?null:{start:l,end:c}}else n=null}n=n||{start:0,end:0}}else n=null;for(ti={focusedElem:e,selectionRange:n},Wt=!1,Yr=t;null!==Yr;)if(e=(t=Yr).child,0!==(1028&t.subtreeFlags)&&null!==e)e.return=t,Yr=e;else for(;null!==Yr;){t=Yr;try{var h=t.alternate;if(0!==(1024&t.flags))switch(t.tag){case 0:case 11:case 15:case 5:case 6:case 4:case 17:break;case 1:if(null!==h){var f=h.memoizedProps,y=h.memoizedState,v=t.stateNode,b=v.getSnapshotBeforeUpdate(t.elementType===t.type?f:nr(t.type,f),y);v.__reactInternalSnapshotBeforeUpdate=b}break;case 3:var A=t.stateNode.containerInfo;1===A.nodeType?A.textContent="":9===A.nodeType&&A.documentElement&&A.removeChild(A.documentElement);break;default:throw Error(o(163))}}catch(I){_c(t,t.return,I)}if(null!==(e=t.sibling)){e.return=t.return,Yr=e;break}Yr=t.return}h=tl,tl=!1}(e,n),fl(n,e),ga(ti),Wt=!!ei,ti=ei=null,e.current=n,vl(n,e,i),Je(),Pl=l,bt=r,Cl.transition=s}else e.current=n;if($l&&($l=!1,Kl=e,Ql=i),s=e.pendingLanes,0===s&&(Vl=null),function(e){if(ot&&"function"===typeof ot.onCommitFiberRoot)try{ot.onCommitFiberRoot(it,e,void 0,128===(128&e.current.flags))}catch(t){}}(n.stateNode),ac(e,Xe()),null!==t)for(a=e.onRecoverableError,n=0;n<t.length;n++)i=t[n],a(i.value,{componentStack:i.stack,digest:i.digest});if(Wl)throw Wl=!1,e=jl,jl=null,e;0!==(1&Ql)&&0!==e.tag&&wc(),s=e.pendingLanes,0!==(1&s)?e===Xl?Jl++:(Jl=0,Xl=e):Jl=0,Hi()}(e,t,n,a)}finally{Cl.transition=i,bt=a}return null}function wc(){if(null!==Kl){var e=At(Ql),t=Cl.transition,n=bt;try{if(Cl.transition=null,bt=16>e?16:e,null===Kl)var a=!1;else{if(e=Kl,Kl=null,Ql=0,0!==(6&Pl))throw Error(o(331));var i=Pl;for(Pl|=4,Yr=e.current;null!==Yr;){var s=Yr,r=s.child;if(0!==(16&Yr.flags)){var l=s.deletions;if(null!==l){for(var c=0;c<l.length;c++){var d=l[c];for(Yr=d;null!==Yr;){var u=Yr;switch(u.tag){case 0:case 11:case 15:nl(8,u,s)}var p=u.child;if(null!==p)p.return=u,Yr=p;else for(;null!==Yr;){var m=(u=Yr).sibling,g=u.return;if(ol(u),u===d){Yr=null;break}if(null!==m){m.return=g,Yr=m;break}Yr=g}}}var h=s.alternate;if(null!==h){var f=h.child;if(null!==f){h.child=null;do{var y=f.sibling;f.sibling=null,f=y}while(null!==f)}}Yr=s}}if(0!==(2064&s.subtreeFlags)&&null!==r)r.return=s,Yr=r;else e:for(;null!==Yr;){if(0!==(2048&(s=Yr).flags))switch(s.tag){case 0:case 11:case 15:nl(9,s,s.return)}var v=s.sibling;if(null!==v){v.return=s.return,Yr=v;break e}Yr=s.return}}var b=e.current;for(Yr=b;null!==Yr;){var A=(r=Yr).child;if(0!==(2064&r.subtreeFlags)&&null!==A)A.return=r,Yr=A;else e:for(r=b;null!==Yr;){if(0!==(2048&(l=Yr).flags))try{switch(l.tag){case 0:case 11:case 15:al(9,l)}}catch(w){_c(l,l.return,w)}if(l===r){Yr=null;break e}var I=l.sibling;if(null!==I){I.return=l.return,Yr=I;break e}Yr=l.return}}if(Pl=i,Hi(),ot&&"function"===typeof ot.onPostCommitFiberRoot)try{ot.onPostCommitFiberRoot(it,e)}catch(w){}a=!0}return a}finally{bt=n,Cl.transition=t}}return!1}function kc(e,t,n){e=Uo(e,t=mr(0,t=cr(n,t),1),1),t=ec(),null!==e&&(yt(e,1,t),ac(e,t))}function _c(e,t,n){if(3===e.tag)kc(e,e,n);else for(;null!==t;){if(3===t.tag){kc(t,e,n);break}if(1===t.tag){var a=t.stateNode;if("function"===typeof t.type.getDerivedStateFromError||"function"===typeof a.componentDidCatch&&(null===Vl||!Vl.has(a))){t=Uo(t,e=gr(t,e=cr(n,e),1),1),e=ec(),null!==t&&(yt(t,1,e),ac(t,e));break}}t=t.return}}function Sc(e,t,n){var a=e.pingCache;null!==a&&a.delete(t),t=ec(),e.pingedLanes|=e.suspendedLanes&n,El===e&&(Rl&n)===n&&(4===ql||3===ql&&(130023424&Rl)===Rl&&500>Xe()-Ol?pc(e,0):Fl|=n),ac(e,t)}function xc(e,t){0===t&&(0===(1&e.mode)?t=1:(t=dt,0===(130023424&(dt<<=1))&&(dt=4194304)));var n=ec();null!==(e=Mo(e,t))&&(yt(e,t,n),ac(e,n))}function Cc(e){var t=e.memoizedState,n=0;null!==t&&(n=t.retryLane),xc(e,n)}function Pc(e,t){var n=0;switch(e.tag){case 13:var a=e.stateNode,i=e.memoizedState;null!==i&&(n=i.retryLane);break;case 19:a=e.stateNode;break;default:throw Error(o(314))}null!==a&&a.delete(t),xc(e,n)}function Ec(e,t){return $e(e,t)}function Tc(e,t,n,a){this.tag=e,this.key=n,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=a,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function Rc(e,t,n,a){return new Tc(e,t,n,a)}function zc(e){return!(!(e=e.prototype)||!e.isReactComponent)}function Dc(e,t){var n=e.alternate;return null===n?((n=Rc(e.tag,t,e.key,e.mode)).elementType=e.elementType,n.type=e.type,n.stateNode=e.stateNode,n.alternate=e,e.alternate=n):(n.pendingProps=t,n.type=e.type,n.flags=0,n.subtreeFlags=0,n.deletions=null),n.flags=14680064&e.flags,n.childLanes=e.childLanes,n.lanes=e.lanes,n.child=e.child,n.memoizedProps=e.memoizedProps,n.memoizedState=e.memoizedState,n.updateQueue=e.updateQueue,t=e.dependencies,n.dependencies=null===t?null:{lanes:t.lanes,firstContext:t.firstContext},n.sibling=e.sibling,n.index=e.index,n.ref=e.ref,n}function qc(e,t,n,a,i,s){var r=2;if(a=e,"function"===typeof e)zc(e)&&(r=1);else if("string"===typeof e)r=5;else e:switch(e){case k:return Mc(n.children,i,s,t);case _:r=8,i|=8;break;case S:return(e=Rc(12,n,t,2|i)).elementType=S,e.lanes=s,e;case E:return(e=Rc(13,n,t,i)).elementType=E,e.lanes=s,e;case T:return(e=Rc(19,n,t,i)).elementType=T,e.lanes=s,e;case D:return Gc(n,i,s,t);default:if("object"===typeof e&&null!==e)switch(e.$$typeof){case x:r=10;break e;case C:r=9;break e;case P:r=11;break e;case R:r=14;break e;case z:r=16,a=null;break e}throw Error(o(130,null==e?e:typeof e,""))}return(t=Rc(r,n,t,i)).elementType=e,t.type=a,t.lanes=s,t}function Mc(e,t,n,a){return(e=Rc(7,e,a,t)).lanes=n,e}function Gc(e,t,n,a){return(e=Rc(22,e,a,t)).elementType=D,e.lanes=n,e.stateNode={isHidden:!1},e}function Lc(e,t,n){return(e=Rc(6,e,null,t)).lanes=n,e}function Fc(e,t,n){return(t=Rc(4,null!==e.children?e.children:[],e.key,t)).lanes=n,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function Nc(e,t,n,a,i){this.tag=t,this.containerInfo=e,this.finishedWork=this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.pendingContext=this.context=null,this.callbackPriority=0,this.eventTimes=ft(0),this.expirationTimes=ft(-1),this.entangledLanes=this.finishedLanes=this.mutableReadLanes=this.expiredLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=ft(0),this.identifierPrefix=a,this.onRecoverableError=i,this.mutableSourceEagerHydrationData=null}function Uc(e,t,n,a,i,o,s,r,l){return e=new Nc(e,t,n,r,l),1===t?(t=1,!0===o&&(t|=8)):t=0,o=Rc(3,null,null,t),e.current=o,o.stateNode=e,o.memoizedState={element:a,isDehydrated:n,cache:null,transitions:null,pendingSuspenseBoundaries:null},Lo(o),e}function Oc(e){if(!e)return Ci;e:{if(He(e=e._reactInternals)!==e||1!==e.tag)throw Error(o(170));var t=e;do{switch(t.tag){case 3:t=t.stateNode.context;break e;case 1:if(zi(t.type)){t=t.stateNode.__reactInternalMemoizedMergedChildContext;break e}}t=t.return}while(null!==t);throw Error(o(171))}if(1===e.tag){var n=e.type;if(zi(n))return Mi(e,n,t)}return t}function Hc(e,t,n,a,i,o,s,r,l){return(e=Uc(n,a,!0,e,0,o,0,r,l)).context=Oc(null),n=e.current,(o=No(a=ec(),i=tc(n))).callback=void 0!==t&&null!==t?t:null,Uo(n,o,i),e.current.lanes=i,yt(e,i,a),ac(e,a),e}function Bc(e,t,n,a){var i=t.current,o=ec(),s=tc(i);return n=Oc(n),null===t.context?t.context=n:t.pendingContext=n,(t=No(o,s)).payload={element:e},null!==(a=void 0===a?null:a)&&(t.callback=a),null!==(e=Uo(i,t,s))&&(nc(e,i,s,o),Oo(e,i,s)),s}function Wc(e){return(e=e.current).child?(e.child.tag,e.child.stateNode):null}function jc(e,t){if(null!==(e=e.memoizedState)&&null!==e.dehydrated){var n=e.retryLane;e.retryLane=0!==n&&n<t?n:t}}function Vc(e,t){jc(e,t),(e=e.alternate)&&jc(e,t)}kl=function(e,t,n){if(null!==e)if(e.memoizedProps!==t.pendingProps||Ei.current)br=!0;else{if(0===(e.lanes&n)&&0===(128&t.flags))return br=!1,function(e,t,n){switch(t.tag){case 3:Er(t),go();break;case 5:Yo(t);break;case 1:zi(t.type)&&Gi(t);break;case 4:Jo(t,t.stateNode.containerInfo);break;case 10:var a=t.type._context,i=t.memoizedProps.value;xi(ko,a._currentValue),a._currentValue=i;break;case 13:if(null!==(a=t.memoizedState))return null!==a.dehydrated?(xi(es,1&es.current),t.flags|=128,null):0!==(n&t.child.childLanes)?Lr(e,t,n):(xi(es,1&es.current),null!==(e=Wr(e,t,n))?e.sibling:null);xi(es,1&es.current);break;case 19:if(a=0!==(n&t.childLanes),0!==(128&e.flags)){if(a)return Hr(e,t,n);t.flags|=128}if(null!==(i=t.memoizedState)&&(i.rendering=null,i.tail=null,i.lastEffect=null),xi(es,es.current),a)break;return null;case 22:case 23:return t.lanes=0,_r(e,t,n)}return Wr(e,t,n)}(e,t,n);br=0!==(131072&e.flags)}else br=!1,io&&0!==(1048576&t.flags)&&Zi(t,Vi,t.index);switch(t.lanes=0,t.tag){case 2:var a=t.type;Br(e,t),e=t.pendingProps;var i=Ri(t,Pi.current);To(t,n),i=fs(null,t,a,e,i,n);var s=ys();return t.flags|=1,"object"===typeof i&&null!==i&&"function"===typeof i.render&&void 0===i.$$typeof?(t.tag=1,t.memoizedState=null,t.updateQueue=null,zi(a)?(s=!0,Gi(t)):s=!1,t.memoizedState=null!==i.state&&void 0!==i.state?i.state:null,Lo(t),i.updater=ir,t.stateNode=i,i._reactInternals=t,lr(t,a,e,n),t=Pr(null,t,a,!0,s,n)):(t.tag=0,io&&s&&eo(t),Ar(null,t,i,n),t=t.child),t;case 16:a=t.elementType;e:{switch(Br(e,t),e=t.pendingProps,a=(i=a._init)(a._payload),t.type=a,i=t.tag=function(e){if("function"===typeof e)return zc(e)?1:0;if(void 0!==e&&null!==e){if((e=e.$$typeof)===P)return 11;if(e===R)return 14}return 2}(a),e=nr(a,e),i){case 0:t=xr(null,t,a,e,n);break e;case 1:t=Cr(null,t,a,e,n);break e;case 11:t=Ir(null,t,a,e,n);break e;case 14:t=wr(null,t,a,nr(a.type,e),n);break e}throw Error(o(306,a,""))}return t;case 0:return a=t.type,i=t.pendingProps,xr(e,t,a,i=t.elementType===a?i:nr(a,i),n);case 1:return a=t.type,i=t.pendingProps,Cr(e,t,a,i=t.elementType===a?i:nr(a,i),n);case 3:e:{if(Er(t),null===e)throw Error(o(387));a=t.pendingProps,i=(s=t.memoizedState).element,Fo(e,t),Bo(t,a,null,n);var r=t.memoizedState;if(a=r.element,s.isDehydrated){if(s={element:a,isDehydrated:!1,cache:r.cache,pendingSuspenseBoundaries:r.pendingSuspenseBoundaries,transitions:r.transitions},t.updateQueue.baseState=s,t.memoizedState=s,256&t.flags){t=Tr(e,t,a,n,i=cr(Error(o(423)),t));break e}if(a!==i){t=Tr(e,t,a,n,i=cr(Error(o(424)),t));break e}for(ao=ci(t.stateNode.containerInfo.firstChild),no=t,io=!0,oo=null,n=wo(t,null,a,n),t.child=n;n;)n.flags=-3&n.flags|4096,n=n.sibling}else{if(go(),a===i){t=Wr(e,t,n);break e}Ar(e,t,a,n)}t=t.child}return t;case 5:return Yo(t),null===e&&co(t),a=t.type,i=t.pendingProps,s=null!==e?e.memoizedProps:null,r=i.children,ni(a,i)?r=null:null!==s&&ni(a,s)&&(t.flags|=32),Sr(e,t),Ar(e,t,r,n),t.child;case 6:return null===e&&co(t),null;case 13:return Lr(e,t,n);case 4:return Jo(t,t.stateNode.containerInfo),a=t.pendingProps,null===e?t.child=Io(t,null,a,n):Ar(e,t,a,n),t.child;case 11:return a=t.type,i=t.pendingProps,Ir(e,t,a,i=t.elementType===a?i:nr(a,i),n);case 7:return Ar(e,t,t.pendingProps,n),t.child;case 8:case 12:return Ar(e,t,t.pendingProps.children,n),t.child;case 10:e:{if(a=t.type._context,i=t.pendingProps,s=t.memoizedProps,r=i.value,xi(ko,a._currentValue),a._currentValue=r,null!==s)if(ra(s.value,r)){if(s.children===i.children&&!Ei.current){t=Wr(e,t,n);break e}}else for(null!==(s=t.child)&&(s.return=t);null!==s;){var l=s.dependencies;if(null!==l){r=s.child;for(var c=l.firstContext;null!==c;){if(c.context===a){if(1===s.tag){(c=No(-1,n&-n)).tag=2;var d=s.updateQueue;if(null!==d){var u=(d=d.shared).pending;null===u?c.next=c:(c.next=u.next,u.next=c),d.pending=c}}s.lanes|=n,null!==(c=s.alternate)&&(c.lanes|=n),Eo(s.return,n,t),l.lanes|=n;break}c=c.next}}else if(10===s.tag)r=s.type===t.type?null:s.child;else if(18===s.tag){if(null===(r=s.return))throw Error(o(341));r.lanes|=n,null!==(l=r.alternate)&&(l.lanes|=n),Eo(r,n,t),r=s.sibling}else r=s.child;if(null!==r)r.return=s;else for(r=s;null!==r;){if(r===t){r=null;break}if(null!==(s=r.sibling)){s.return=r.return,r=s;break}r=r.return}s=r}Ar(e,t,i.children,n),t=t.child}return t;case 9:return i=t.type,a=t.pendingProps.children,To(t,n),a=a(i=Ro(i)),t.flags|=1,Ar(e,t,a,n),t.child;case 14:return i=nr(a=t.type,t.pendingProps),wr(e,t,a,i=nr(a.type,i),n);case 15:return kr(e,t,t.type,t.pendingProps,n);case 17:return a=t.type,i=t.pendingProps,i=t.elementType===a?i:nr(a,i),Br(e,t),t.tag=1,zi(a)?(e=!0,Gi(t)):e=!1,To(t,n),sr(t,a,i),lr(t,a,i,n),Pr(null,t,a,!0,e,n);case 19:return Hr(e,t,n);case 22:return _r(e,t,n)}throw Error(o(156,t.tag))};var $c="function"===typeof reportError?reportError:function(e){console.error(e)};function Kc(e){this._internalRoot=e}function Qc(e){this._internalRoot=e}function Jc(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType)}function Xc(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType&&(8!==e.nodeType||" react-mount-point-unstable "!==e.nodeValue))}function Yc(){}function Zc(e,t,n,a,i){var o=n._reactRootContainer;if(o){var s=o;if("function"===typeof i){var r=i;i=function(){var e=Wc(s);r.call(e)}}Bc(t,s,e,i)}else s=function(e,t,n,a,i){if(i){if("function"===typeof a){var o=a;a=function(){var e=Wc(s);o.call(e)}}var s=Hc(t,a,e,0,null,!1,0,"",Yc);return e._reactRootContainer=s,e[gi]=s.current,Ha(8===e.nodeType?e.parentNode:e),dc(),s}for(;i=e.lastChild;)e.removeChild(i);if("function"===typeof a){var r=a;a=function(){var e=Wc(l);r.call(e)}}var l=Uc(e,0,!1,null,0,!1,0,"",Yc);return e._reactRootContainer=l,e[gi]=l.current,Ha(8===e.nodeType?e.parentNode:e),dc((function(){Bc(t,l,n,a)})),l}(n,t,e,i,a);return Wc(s)}Qc.prototype.render=Kc.prototype.render=function(e){var t=this._internalRoot;if(null===t)throw Error(o(409));Bc(e,t,null,null)},Qc.prototype.unmount=Kc.prototype.unmount=function(){var e=this._internalRoot;if(null!==e){this._internalRoot=null;var t=e.containerInfo;dc((function(){Bc(null,e,null,null)})),t[gi]=null}},Qc.prototype.unstable_scheduleHydration=function(e){if(e){var t=_t();e={blockedOn:null,target:e,priority:t};for(var n=0;n<Dt.length&&0!==t&&t<Dt[n].priority;n++);Dt.splice(n,0,e),0===n&&Lt(e)}},It=function(e){switch(e.tag){case 3:var t=e.stateNode;if(t.current.memoizedState.isDehydrated){var n=ut(t.pendingLanes);0!==n&&(vt(t,1|n),ac(t,Xe()),0===(6&Pl)&&(Hl=Xe()+500,Hi()))}break;case 13:dc((function(){var t=Mo(e,1);if(null!==t){var n=ec();nc(t,e,1,n)}})),Vc(e,1)}},wt=function(e){if(13===e.tag){var t=Mo(e,134217728);if(null!==t)nc(t,e,134217728,ec());Vc(e,134217728)}},kt=function(e){if(13===e.tag){var t=tc(e),n=Mo(e,t);if(null!==n)nc(n,e,t,ec());Vc(e,t)}},_t=function(){return bt},St=function(e,t){var n=bt;try{return bt=e,t()}finally{bt=n}},we=function(e,t,n){switch(t){case"input":if(Y(e,n),t=n.name,"radio"===n.type&&null!=t){for(n=e;n.parentNode;)n=n.parentNode;for(n=n.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<n.length;t++){var a=n[t];if(a!==e&&a.form===e.form){var i=Ii(a);if(!i)throw Error(o(90));$(a),Y(a,i)}}}break;case"textarea":oe(e,n);break;case"select":null!=(t=n.value)&&ne(e,!!n.multiple,t,!1)}},Pe=cc,Ee=dc;var ed={usingClientEntryPoint:!1,Events:[bi,Ai,Ii,xe,Ce,cc]},td={findFiberByHostInstance:vi,bundleType:0,version:"18.3.1",rendererPackageName:"react-dom"},nd={bundleType:td.bundleType,version:td.version,rendererPackageName:td.rendererPackageName,rendererConfig:td.rendererConfig,overrideHookState:null,overrideHookStateDeletePath:null,overrideHookStateRenamePath:null,overrideProps:null,overridePropsDeletePath:null,overridePropsRenamePath:null,setErrorHandler:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:A.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return null===(e=je(e))?null:e.stateNode},findFiberByHostInstance:td.findFiberByHostInstance||function(){return null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null,reconcilerVersion:"18.3.1-next-f1338f8080-20240426"};if("undefined"!==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__){var ad=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!ad.isDisabled&&ad.supportsFiber)try{it=ad.inject(nd),ot=ad}catch(de){}}t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=ed,t.createPortal=function(e,t){var n=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;if(!Jc(t))throw Error(o(200));return function(e,t,n){var a=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:w,key:null==a?null:""+a,children:e,containerInfo:t,implementation:n}}(e,t,null,n)},t.createRoot=function(e,t){if(!Jc(e))throw Error(o(299));var n=!1,a="",i=$c;return null!==t&&void 0!==t&&(!0===t.unstable_strictMode&&(n=!0),void 0!==t.identifierPrefix&&(a=t.identifierPrefix),void 0!==t.onRecoverableError&&(i=t.onRecoverableError)),t=Uc(e,1,!1,null,0,n,0,a,i),e[gi]=t.current,Ha(8===e.nodeType?e.parentNode:e),new Kc(t)},t.findDOMNode=function(e){if(null==e)return null;if(1===e.nodeType)return e;var t=e._reactInternals;if(void 0===t){if("function"===typeof e.render)throw Error(o(188));throw e=Object.keys(e).join(","),Error(o(268,e))}return e=null===(e=je(t))?null:e.stateNode},t.flushSync=function(e){return dc(e)},t.hydrate=function(e,t,n){if(!Xc(t))throw Error(o(200));return Zc(null,e,t,!0,n)},t.hydrateRoot=function(e,t,n){if(!Jc(e))throw Error(o(405));var a=null!=n&&n.hydratedSources||null,i=!1,s="",r=$c;if(null!==n&&void 0!==n&&(!0===n.unstable_strictMode&&(i=!0),void 0!==n.identifierPrefix&&(s=n.identifierPrefix),void 0!==n.onRecoverableError&&(r=n.onRecoverableError)),t=Hc(t,null,e,1,null!=n?n:null,i,0,s,r),e[gi]=t.current,Ha(e),a)for(e=0;e<a.length;e++)i=(i=(n=a[e])._getVersion)(n._source),null==t.mutableSourceEagerHydrationData?t.mutableSourceEagerHydrationData=[n,i]:t.mutableSourceEagerHydrationData.push(n,i);return new Qc(t)},t.render=function(e,t,n){if(!Xc(t))throw Error(o(200));return Zc(null,e,t,!1,n)},t.unmountComponentAtNode=function(e){if(!Xc(e))throw Error(o(40));return!!e._reactRootContainer&&(dc((function(){Zc(null,null,e,!1,(function(){e._reactRootContainer=null,e[gi]=null}))})),!0)},t.unstable_batchedUpdates=cc,t.unstable_renderSubtreeIntoContainer=function(e,t,n,a){if(!Xc(n))throw Error(o(200));if(null==e||void 0===e._reactInternals)throw Error(o(38));return Zc(e,t,n,!1,a)},t.version="18.3.1-next-f1338f8080-20240426"},391:(e,t,n)=>{var a=n(950);t.createRoot=a.createRoot,t.hydrateRoot=a.hydrateRoot},950:(e,t,n)=>{!function e(){if("undefined"!==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__&&"function"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE)try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(e)}catch(t){console.error(t)}}(),e.exports=n(730)},153:(e,t,n)=>{var a=n(43),i=Symbol.for("react.element"),o=Symbol.for("react.fragment"),s=Object.prototype.hasOwnProperty,r=a.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.ReactCurrentOwner,l={key:!0,ref:!0,__self:!0,__source:!0};function c(e,t,n){var a,o={},c=null,d=null;for(a in void 0!==n&&(c=""+n),void 0!==t.key&&(c=""+t.key),void 0!==t.ref&&(d=t.ref),t)s.call(t,a)&&!l.hasOwnProperty(a)&&(o[a]=t[a]);if(e&&e.defaultProps)for(a in t=e.defaultProps)void 0===o[a]&&(o[a]=t[a]);return{$$typeof:i,type:e,key:c,ref:d,props:o,_owner:r.current}}t.jsx=c,t.jsxs=c},202:(e,t)=>{var n=Symbol.for("react.element"),a=Symbol.for("react.portal"),i=Symbol.for("react.fragment"),o=Symbol.for("react.strict_mode"),s=Symbol.for("react.profiler"),r=Symbol.for("react.provider"),l=Symbol.for("react.context"),c=Symbol.for("react.forward_ref"),d=Symbol.for("react.suspense"),u=Symbol.for("react.memo"),p=Symbol.for("react.lazy"),m=Symbol.iterator;var g={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},h=Object.assign,f={};function y(e,t,n){this.props=e,this.context=t,this.refs=f,this.updater=n||g}function v(){}function b(e,t,n){this.props=e,this.context=t,this.refs=f,this.updater=n||g}y.prototype.isReactComponent={},y.prototype.setState=function(e,t){if("object"!==typeof e&&"function"!==typeof e&&null!=e)throw Error("setState(...): takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,e,t,"setState")},y.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")},v.prototype=y.prototype;var A=b.prototype=new v;A.constructor=b,h(A,y.prototype),A.isPureReactComponent=!0;var I=Array.isArray,w=Object.prototype.hasOwnProperty,k={current:null},_={key:!0,ref:!0,__self:!0,__source:!0};function S(e,t,a){var i,o={},s=null,r=null;if(null!=t)for(i in void 0!==t.ref&&(r=t.ref),void 0!==t.key&&(s=""+t.key),t)w.call(t,i)&&!_.hasOwnProperty(i)&&(o[i]=t[i]);var l=arguments.length-2;if(1===l)o.children=a;else if(1<l){for(var c=Array(l),d=0;d<l;d++)c[d]=arguments[d+2];o.children=c}if(e&&e.defaultProps)for(i in l=e.defaultProps)void 0===o[i]&&(o[i]=l[i]);return{$$typeof:n,type:e,key:s,ref:r,props:o,_owner:k.current}}function x(e){return"object"===typeof e&&null!==e&&e.$$typeof===n}var C=/\/+/g;function P(e,t){return"object"===typeof e&&null!==e&&null!=e.key?function(e){var t={"=":"=0",":":"=2"};return"$"+e.replace(/[=:]/g,(function(e){return t[e]}))}(""+e.key):t.toString(36)}function E(e,t,i,o,s){var r=typeof e;"undefined"!==r&&"boolean"!==r||(e=null);var l=!1;if(null===e)l=!0;else switch(r){case"string":case"number":l=!0;break;case"object":switch(e.$$typeof){case n:case a:l=!0}}if(l)return s=s(l=e),e=""===o?"."+P(l,0):o,I(s)?(i="",null!=e&&(i=e.replace(C,"$&/")+"/"),E(s,t,i,"",(function(e){return e}))):null!=s&&(x(s)&&(s=function(e,t){return{$$typeof:n,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}(s,i+(!s.key||l&&l.key===s.key?"":(""+s.key).replace(C,"$&/")+"/")+e)),t.push(s)),1;if(l=0,o=""===o?".":o+":",I(e))for(var c=0;c<e.length;c++){var d=o+P(r=e[c],c);l+=E(r,t,i,d,s)}else if(d=function(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=m&&e[m]||e["@@iterator"])?e:null}(e),"function"===typeof d)for(e=d.call(e),c=0;!(r=e.next()).done;)l+=E(r=r.value,t,i,d=o+P(r,c++),s);else if("object"===r)throw t=String(e),Error("Objects are not valid as a React child (found: "+("[object Object]"===t?"object with keys {"+Object.keys(e).join(", ")+"}":t)+"). If you meant to render a collection of children, use an array instead.");return l}function T(e,t,n){if(null==e)return e;var a=[],i=0;return E(e,a,"","",(function(e){return t.call(n,e,i++)})),a}function R(e){if(-1===e._status){var t=e._result;(t=t()).then((function(t){0!==e._status&&-1!==e._status||(e._status=1,e._result=t)}),(function(t){0!==e._status&&-1!==e._status||(e._status=2,e._result=t)})),-1===e._status&&(e._status=0,e._result=t)}if(1===e._status)return e._result.default;throw e._result}var z={current:null},D={transition:null},q={ReactCurrentDispatcher:z,ReactCurrentBatchConfig:D,ReactCurrentOwner:k};function M(){throw Error("act(...) is not supported in production builds of React.")}t.Children={map:T,forEach:function(e,t,n){T(e,(function(){t.apply(this,arguments)}),n)},count:function(e){var t=0;return T(e,(function(){t++})),t},toArray:function(e){return T(e,(function(e){return e}))||[]},only:function(e){if(!x(e))throw Error("React.Children.only expected to receive a single React element child.");return e}},t.Component=y,t.Fragment=i,t.Profiler=s,t.PureComponent=b,t.StrictMode=o,t.Suspense=d,t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=q,t.act=M,t.cloneElement=function(e,t,a){if(null===e||void 0===e)throw Error("React.cloneElement(...): The argument must be a React element, but you passed "+e+".");var i=h({},e.props),o=e.key,s=e.ref,r=e._owner;if(null!=t){if(void 0!==t.ref&&(s=t.ref,r=k.current),void 0!==t.key&&(o=""+t.key),e.type&&e.type.defaultProps)var l=e.type.defaultProps;for(c in t)w.call(t,c)&&!_.hasOwnProperty(c)&&(i[c]=void 0===t[c]&&void 0!==l?l[c]:t[c])}var c=arguments.length-2;if(1===c)i.children=a;else if(1<c){l=Array(c);for(var d=0;d<c;d++)l[d]=arguments[d+2];i.children=l}return{$$typeof:n,type:e.type,key:o,ref:s,props:i,_owner:r}},t.createContext=function(e){return(e={$$typeof:l,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null,_defaultValue:null,_globalName:null}).Provider={$$typeof:r,_context:e},e.Consumer=e},t.createElement=S,t.createFactory=function(e){var t=S.bind(null,e);return t.type=e,t},t.createRef=function(){return{current:null}},t.forwardRef=function(e){return{$$typeof:c,render:e}},t.isValidElement=x,t.lazy=function(e){return{$$typeof:p,_payload:{_status:-1,_result:e},_init:R}},t.memo=function(e,t){return{$$typeof:u,type:e,compare:void 0===t?null:t}},t.startTransition=function(e){var t=D.transition;D.transition={};try{e()}finally{D.transition=t}},t.unstable_act=M,t.useCallback=function(e,t){return z.current.useCallback(e,t)},t.useContext=function(e){return z.current.useContext(e)},t.useDebugValue=function(){},t.useDeferredValue=function(e){return z.current.useDeferredValue(e)},t.useEffect=function(e,t){return z.current.useEffect(e,t)},t.useId=function(){return z.current.useId()},t.useImperativeHandle=function(e,t,n){return z.current.useImperativeHandle(e,t,n)},t.useInsertionEffect=function(e,t){return z.current.useInsertionEffect(e,t)},t.useLayoutEffect=function(e,t){return z.current.useLayoutEffect(e,t)},t.useMemo=function(e,t){return z.current.useMemo(e,t)},t.useReducer=function(e,t,n){return z.current.useReducer(e,t,n)},t.useRef=function(e){return z.current.useRef(e)},t.useState=function(e){return z.current.useState(e)},t.useSyncExternalStore=function(e,t,n){return z.current.useSyncExternalStore(e,t,n)},t.useTransition=function(){return z.current.useTransition()},t.version="18.3.1"},43:(e,t,n)=>{e.exports=n(202)},579:(e,t,n)=>{e.exports=n(153)},234:(e,t)=>{function n(e,t){var n=e.length;e.push(t);e:for(;0<n;){var a=n-1>>>1,i=e[a];if(!(0<o(i,t)))break e;e[a]=t,e[n]=i,n=a}}function a(e){return 0===e.length?null:e[0]}function i(e){if(0===e.length)return null;var t=e[0],n=e.pop();if(n!==t){e[0]=n;e:for(var a=0,i=e.length,s=i>>>1;a<s;){var r=2*(a+1)-1,l=e[r],c=r+1,d=e[c];if(0>o(l,n))c<i&&0>o(d,l)?(e[a]=d,e[c]=n,a=c):(e[a]=l,e[r]=n,a=r);else{if(!(c<i&&0>o(d,n)))break e;e[a]=d,e[c]=n,a=c}}}return t}function o(e,t){var n=e.sortIndex-t.sortIndex;return 0!==n?n:e.id-t.id}if("object"===typeof performance&&"function"===typeof performance.now){var s=performance;t.unstable_now=function(){return s.now()}}else{var r=Date,l=r.now();t.unstable_now=function(){return r.now()-l}}var c=[],d=[],u=1,p=null,m=3,g=!1,h=!1,f=!1,y="function"===typeof setTimeout?setTimeout:null,v="function"===typeof clearTimeout?clearTimeout:null,b="undefined"!==typeof setImmediate?setImmediate:null;function A(e){for(var t=a(d);null!==t;){if(null===t.callback)i(d);else{if(!(t.startTime<=e))break;i(d),t.sortIndex=t.expirationTime,n(c,t)}t=a(d)}}function I(e){if(f=!1,A(e),!h)if(null!==a(c))h=!0,D(w);else{var t=a(d);null!==t&&q(I,t.startTime-e)}}function w(e,n){h=!1,f&&(f=!1,v(x),x=-1),g=!0;var o=m;try{for(A(n),p=a(c);null!==p&&(!(p.expirationTime>n)||e&&!E());){var s=p.callback;if("function"===typeof s){p.callback=null,m=p.priorityLevel;var r=s(p.expirationTime<=n);n=t.unstable_now(),"function"===typeof r?p.callback=r:p===a(c)&&i(c),A(n)}else i(c);p=a(c)}if(null!==p)var l=!0;else{var u=a(d);null!==u&&q(I,u.startTime-n),l=!1}return l}finally{p=null,m=o,g=!1}}"undefined"!==typeof navigator&&void 0!==navigator.scheduling&&void 0!==navigator.scheduling.isInputPending&&navigator.scheduling.isInputPending.bind(navigator.scheduling);var k,_=!1,S=null,x=-1,C=5,P=-1;function E(){return!(t.unstable_now()-P<C)}function T(){if(null!==S){var e=t.unstable_now();P=e;var n=!0;try{n=S(!0,e)}finally{n?k():(_=!1,S=null)}}else _=!1}if("function"===typeof b)k=function(){b(T)};else if("undefined"!==typeof MessageChannel){var R=new MessageChannel,z=R.port2;R.port1.onmessage=T,k=function(){z.postMessage(null)}}else k=function(){y(T,0)};function D(e){S=e,_||(_=!0,k())}function q(e,n){x=y((function(){e(t.unstable_now())}),n)}t.unstable_IdlePriority=5,t.unstable_ImmediatePriority=1,t.unstable_LowPriority=4,t.unstable_NormalPriority=3,t.unstable_Profiling=null,t.unstable_UserBlockingPriority=2,t.unstable_cancelCallback=function(e){e.callback=null},t.unstable_continueExecution=function(){h||g||(h=!0,D(w))},t.unstable_forceFrameRate=function(e){0>e||125<e?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):C=0<e?Math.floor(1e3/e):5},t.unstable_getCurrentPriorityLevel=function(){return m},t.unstable_getFirstCallbackNode=function(){return a(c)},t.unstable_next=function(e){switch(m){case 1:case 2:case 3:var t=3;break;default:t=m}var n=m;m=t;try{return e()}finally{m=n}},t.unstable_pauseExecution=function(){},t.unstable_requestPaint=function(){},t.unstable_runWithPriority=function(e,t){switch(e){case 1:case 2:case 3:case 4:case 5:break;default:e=3}var n=m;m=e;try{return t()}finally{m=n}},t.unstable_scheduleCallback=function(e,i,o){var s=t.unstable_now();switch("object"===typeof o&&null!==o?o="number"===typeof(o=o.delay)&&0<o?s+o:s:o=s,e){case 1:var r=-1;break;case 2:r=250;break;case 5:r=1073741823;break;case 4:r=1e4;break;default:r=5e3}return e={id:u++,callback:i,priorityLevel:e,startTime:o,expirationTime:r=o+r,sortIndex:-1},o>s?(e.sortIndex=o,n(d,e),null===a(c)&&e===a(d)&&(f?(v(x),x=-1):f=!0,q(I,o-s))):(e.sortIndex=r,n(c,e),h||g||(h=!0,D(w))),e},t.unstable_shouldYield=E,t.unstable_wrapCallback=function(e){var t=m;return function(){var n=m;m=t;try{return e.apply(this,arguments)}finally{m=n}}}},853:(e,t,n)=>{e.exports=n(234)}},t={};function n(a){var i=t[a];if(void 0!==i)return i.exports;var o=t[a]={exports:{}};return e[a](o,o.exports,n),o.exports}n.m=e,n.d=(e,t)=>{for(var a in t)n.o(t,a)&&!n.o(e,a)&&Object.defineProperty(e,a,{enumerable:!0,get:t[a]})},n.f={},n.e=e=>Promise.all(Object.keys(n.f).reduce(((t,a)=>(n.f[a](e,t),t)),[])),n.u=e=>"static/js/"+e+".269dfbb9.chunk.js",n.miniCssF=e=>{},n.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),(()=>{var e={},t="ai-submissions-viewer:";n.l=(a,i,o,s)=>{if(e[a])e[a].push(i);else{var r,l;if(void 0!==o)for(var c=document.getElementsByTagName("script"),d=0;d<c.length;d++){var u=c[d];if(u.getAttribute("src")==a||u.getAttribute("data-webpack")==t+o){r=u;break}}r||(l=!0,(r=document.createElement("script")).charset="utf-8",r.timeout=120,n.nc&&r.setAttribute("nonce",n.nc),r.setAttribute("data-webpack",t+o),r.src=a),e[a]=[i];var p=(t,n)=>{r.onerror=r.onload=null,clearTimeout(m);var i=e[a];if(delete e[a],r.parentNode&&r.parentNode.removeChild(r),i&&i.forEach((e=>e(n))),t)return t(n)},m=setTimeout(p.bind(null,void 0,{type:"timeout",target:r}),12e4);r.onerror=p.bind(null,r.onerror),r.onload=p.bind(null,r.onload),l&&document.head.appendChild(r)}}})(),n.r=e=>{"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},n.p="/visualize_responsible_AI/",(()=>{var e={792:0};n.f.j=(t,a)=>{var i=n.o(e,t)?e[t]:void 0;if(0!==i)if(i)a.push(i[2]);else{var o=new Promise(((n,a)=>i=e[t]=[n,a]));a.push(i[2]=o);var s=n.p+n.u(t),r=new Error;n.l(s,(a=>{if(n.o(e,t)&&(0!==(i=e[t])&&(e[t]=void 0),i)){var o=a&&("load"===a.type?"missing":a.type),s=a&&a.target&&a.target.src;r.message="Loading chunk "+t+" failed.\n("+o+": "+s+")",r.name="ChunkLoadError",r.type=o,r.request=s,i[1](r)}}),"chunk-"+t,t)}};var t=(t,a)=>{var i,o,s=a[0],r=a[1],l=a[2],c=0;if(s.some((t=>0!==e[t]))){for(i in r)n.o(r,i)&&(n.m[i]=r[i]);if(l)l(n)}for(t&&t(a);c<s.length;c++)o=s[c],n.o(e,o)&&e[o]&&e[o][0](),e[o]=0},a=self.webpackChunkai_submissions_viewer=self.webpackChunkai_submissions_viewer||[];a.forEach(t.bind(null,0)),a.push=t.bind(null,a.push.bind(a))})();var a=n(43),i=n(391);const o=function(){for(var e=arguments.length,t=new Array(e),n=0;n<e;n++)t[n]=arguments[n];return t.filter(((e,t,n)=>Boolean(e)&&n.indexOf(e)===t)).join(" ")};var s={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};const r=(0,a.forwardRef)(((e,t)=>{let{color:n="currentColor",size:i=24,strokeWidth:r=2,absoluteStrokeWidth:l,className:c="",children:d,iconNode:u,...p}=e;return(0,a.createElement)("svg",{ref:t,...s,width:i,height:i,stroke:n,strokeWidth:l?24*Number(r)/Number(i):r,className:o("lucide",c),...p},[...u.map((e=>{let[t,n]=e;return(0,a.createElement)(t,n)})),...Array.isArray(d)?d:[d]])})),l=(e,t)=>{const n=(0,a.forwardRef)(((n,i)=>{let{className:s,...l}=n;return(0,a.createElement)(r,{ref:i,iconNode:t,className:o(`lucide-${c=e,c.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,s),...l});var c}));return n.displayName=`${e}`,n},c=l("ChevronDown",[["path",{d:"m6 9 6 6 6-6",key:"qrunsl"}]]),d=l("ChevronRight",[["path",{d:"m9 18 6-6-6-6",key:"mthhwq"}]]),u=l("ExternalLink",[["path",{d:"M15 3h6v6",key:"1q9fwt"}],["path",{d:"M10 14 21 3",key:"gplh6r"}],["path",{d:"M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6",key:"a6xqqp"}]]),p=JSON.parse('{"7_2023 - Epistemic virtues of harnessing rigorous.pdf":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Supports the use of rigorous machine learning systems in ethically sensitive domains","arguments":["ML systems offer an opportunity for correction of imbalances, not amplification","Formalization through ML systems can promote transparency, consistency, and replicability in decision-making","Rigorous ML systems can help ensure models abide by well-defined constraints and standards"],"counterarguments":["ML systems can create further imbalances in decision-making","Use of ML systems in medical practices raises concerns about perpetuating inequalities","ML systems are often described as \'black box\' systems"],"key_recommendations":["Develop and use rigorous ML systems that can be interrogated as accurately and fully as a human performing the same task","Pursue further development of interpretable and explainable ML systems","Implement a comprehensive approach considering sociotechnical aspects, power relations, legal considerations, and real-world consequences"],"risks_and_challenges":["Potential for perpetuating existing inequalities","Risk of overestimating ML systems\' capabilities","Possibility of displacing human authority"],"safeguards_and_mitigations":["Develop rigorous ML systems that are interpretable and explainable","Ensure ML systems remain strictly consistent or neutral to chosen variables","Facilitate sharing and replication of models to enable building on each other\'s work"],"examples":{"Use of ML-based prediction drug monitoring programmes (PDMPs) in opioid prescription":"Some physicians, in their care of patients at risk of misusing opioids, use machine learning (ML)-based prediction drug monitoring programmes (PDMPs) to guide their decision making in the prescription of opioids.","Potential conflict between PDMP Score and patient testimony":"This can cause a conflict: a PDMP Score can indicate a patient is at a high risk of opioid abuse while a patient expressly reports oppositely."},"international_alignment":"null","values":["Transparency","Consistency","Replicability"],"tone":"Optimistic","stakeholders":[{"entity":"Technologists","role":"Collaborate in interdisciplinary teams to evaluate and refine ML models"},{"entity":"Ethicists","role":"Collaborate in interdisciplinary teams to evaluate and refine ML models"},{"entity":"Physicians","role":"Collaborate in interdisciplinary teams to evaluate and refine ML models"},{"entity":"Patients","role":"Participate as stakeholders in evaluating and refining ML models"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential improvement in decision-making for opioid prescriptions and reduction of biases"},{"sector":"Medicine","impact":"Improved transparency, consistency, and replicability in medical decision-making processes"}],"quotes":["Rigorous ML systems should be seen as a necessary but not sufficient condition for ensuring ethical and responsible use of ML systems in medicine.","Instead of relying solely on the ability of an individual prescriber, we can transparently formalise salient variables\u2014not to supplant the prescriber or the patient in their knowledge and expertise, but to supplement and debias.","ML systems are an opportunity for correction, not amplification, of these imbalances."]},"420_Government Submission Safe and Responsible AI in Australia 4 August 2023.fdf44dcba4297.pdf":{"organization_name":"KomplyAi","organization_type":"Technology start-up","classification":"Proponent","overall_position":"Supports new AI-specific legislation with a unique approach to licensing and regulation","arguments":["Government intervention is required to address tangible and intangible harm of AI","Non-regulatory ethical AI principles have been globally ineffective","Australia has an opportunity to take a different approach to AI regulation that promotes innovation while ensuring safety"],"counterarguments":["Uncertainty in laws facilitates single or potential multiple judge-based decisions","A regime that is ill-equipped to deal with changes at pace is not good for anyone","Traditional forms of technology and supporting Government and agency administrative infrastructure are diametrically different to AI"],"key_recommendations":["Introduce a \'Technology Passport\' licensing regime for organizations developing, procuring, deploying, or exporting AI","Implement mandatory and public reporting of AI harm","Create clear licensing exclusions for certain categories, such as start-ups and SMEs that meet specific safety criterion","Closely consider exemptions for research and development activities and open source AI components","Introduce strong and clear prohibitions on certain types of AI activities"],"risks_and_challenges":["Disproportionate impact on start-ups and SMEs due to compliance costs","National skills shortages in technical areas of AI in Australia","Uncertainty in intellectual property laws as they relate to AI","Potential for AI activities that do not have a place in Australian society"],"safeguards_and_mitigations":["Licensing regime to ensure organizations meet baseline requirements for good corporate governance","Central repository of searchable details of licence holders and their activities","Mandatory and public reporting of AI harm data","Mechanism for Government to better support the advancement of foundation models in public interests"],"examples":{"Cambridge Analytica case":"Cambridge Analytica, a political consulting firm, used Facebook data to build profiles of millions of users, inferring personality traits from their online activities. The firm allegedly used this data to deliver customised political ads during the 2016 U.S. Presidential Election and Brexit referendum, with claims of influencing voters.","China\'s Social Credit System":"China\'s Social Credit System assigns a score to citizens based on their financial behaviour, legal compliance, and social interactions, impacting various aspects of their lives, such as loan eligibility or travel rights.","Clearview AI":"Clearview AI, a company uses facial recognition technology to scrape images on the web to create a searchable biometric database. A user can upload a snapshot of any person, and in response the system generates additional matches from the internet using biometric comparison."},"international_alignment":"Supports coherence with global standards but advocates for a unique Australian approach","values":["Innovation","Safety","Transparency"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Introduce and enforce AI legislation, support innovation"},{"entity":"Start-ups and SMEs","role":"Comply with regulations while being protected from disproportionate impact"},{"entity":"Large technology companies","role":"Comply with regulations and support smaller players in the ecosystem"}],"sector_impacts":[{"sector":"Technology","impact":"Increased innovation and trust in AI systems"},{"sector":"Research and Development","impact":"Potential exemptions from certain regulations to foster innovation"}],"quotes":["Australia has the opportunity to do things differently here and come out on top. At the same time, ensure that it is not a testing ground for AI that has serious public consequences that will be difficult to wind back.","We believe that this could better enable technology fluidity, neutrality, and create fewer barriers that unnaturally confines AI, and its multiple layers and intersections with other emerging forms of technology, to sectors, activity types, and a level of proscription, that should rather be connected to fundamental baseline corporate governance requirements, that better address issues of public safety.","There are obvious and well researched significant economic benefits that AI can generate, that we need to take advantage of. However, there are certain AI activities where the outcomes of those activities do not have a place in Australian society."]},"165_FINALAustralia_Safe and Responsible AI Paper_Submission[11] copy.b1faec303be1b.pdf":{"organization_name":"Adobe","organization_type":"Technology company","classification":"Proponent","overall_position":"Favors collaborative, risk-based approach with industry-led initiatives for AI governance","arguments":["Collaborative approach leveraging industry leadership can be effective","Risk-based approach empowers organizations to establish oversight in an adaptable manner","Industry-led initiatives like Content Authenticity Initiative promote transparency without third-party intervention"],"counterarguments":["null"],"key_recommendations":["Leverage Content Credentials for transparency in AI-generated content","Adopt a risk-based approach aligned with global frameworks","Government should curate standardized datasets to mitigate harmful bias in AI"],"risks_and_challenges":["Potential for AI to generate deepfakes and misinformation","Fragmentation of AI regulations globally"],"safeguards_and_mitigations":["Internal AI Ethics principles and review processes","Content Credentials to indicate AI-generated content","Do Not Train tag to allow creators to opt out of AI training datasets"],"examples":{"Neural Filters development":"The Neural Filters development team used our AI Ethics Assessment to assess the potential ethical impact of the filters to avoid perpetuating negative biases.","Firefly beta launch":"We launched our Firefly tool as a beta to seek input from our employees, customers, and our broader creative community.","Content Authenticity Initiative":"Adobe leads the Content Authenticity Initiative (CAI) \u2013 a global coalition working to increase transparency in digital content through open industry standards."},"international_alignment":"Supports aligning AI standards globally and harmonising leading AI governance frameworks","values":["Accountability","Responsibility","Transparency"],"tone":"Positive","stakeholders":[{"entity":"Industry","role":"Implement responsible AI mechanisms and collaborate on initiatives"},{"entity":"Government","role":"Facilitate industry AI governance initiatives and provide standardized datasets"}],"sector_impacts":[{"sector":"Creative industry","impact":"AI amplifies human creativity and capabilities"},{"sector":"Digital content","impact":"Improved transparency and trust through Content Credentials"}],"quotes":["Adobe agrees with the Discussion Paper\'s general approach to governing harmful outcomes of AI systems, not AI technology itself.","We believe that AI done right will amplify human creativity and capabilities to new levels with deeper insights, accelerated task performance, and improved decision-making ability.","Adobe favours aligning AI standards globally and harmonising leading AI governance frameworks."]},"154_Law Squared \u2013 AI Consultation Response.da94b4bcb6d5c.pdf":{"organization_name":"Law Squared","organization_type":"Law firm","classification":"Proponent","overall_position":"Supports new AI-specific laws while updating existing laws","arguments":["Regulatory action should be taken on AI to ensure benefits are realized without compromising rights, safety or security","Current legislative framework for technology has a broad scope and does not specifically contemplate AI, creating gaps","A risk-based approach allows for a nuanced and tailored response to the unique challenges posed by AI technologies"],"counterarguments":["Non-regulatory initiatives can fill important gaps left by regulatory initiatives","Banning high-risk activities could potentially stifle the development and competitiveness of tech companies operating in Australia","A risk-based approach should not neglect ethical concerns associated with AI technologies"],"key_recommendations":["Implement a risk-based approach to AI regulation","Incorporate AI regulation into existing frameworks where sensible, supplemented by specific AI legislation","Introduce a right for individuals not to be subject to automated decision making","Require organizations to notify individuals about AI use in service delivery","Remove the small business exemption from the Privacy Act"],"risks_and_challenges":["Lack of public trust in AI","Generation of deepfakes to cause deceit","Creating misinformation and disinformation","Encouraging people to self-harm","Inaccuracies from AI models"],"safeguards_and_mitigations":["Conduct Privacy Impact Assessments for high privacy risk projects","Implement enhanced risk assessment requirements for biometric information use","Require human verification and monitoring for legal or significant AI decisions","Mandate thorough testing of AI for safety and accuracy"],"examples":{"Privacy Impact Assessments":"PIAs under the Privacy Act. A PIA is a systematic assessment of a project to identify the positive and negative impacts that project might have on the privacy of individuals.","Data Protection Impact Assessments":"DPIAs under Article 35 of the EU GDPR. A DPIA requires a data controller (the entity that controls the \'when\' and \'how\' of data processing) to assess the impact new ways of handling personal information may have on individuals.","Facial recognition technology":"Facial recognition can make it easier and more cost-effective for business to ensure that it: provides a safer workplace for employees by knowing exactly who is in the workplace and when (and hence comply with their duty of care under State/Territory based legislation and common law);"},"international_alignment":"Supports global standards with national implementation","values":["Transparency","Accountability","Fairness","Privacy","Security"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations"},{"entity":"Private sector organizations","role":"Implement responsible AI practices"},{"entity":"Public sector organizations","role":"Implement responsible AI practices"},{"entity":"Individuals","role":"Be informed about AI use and have rights protected"}],"sector_impacts":[{"sector":"Tech sector","impact":"Potential limitation on development and competitiveness if high-risk activities are banned"},{"sector":"Financial services","impact":"Potential impact on AFS Licensees\' obligation to provide services efficiently, honestly and fairly"}],"quotes":["Law Squared considers that regulatory action should be taken on AI to ensure the promised benefits of the technology are realised by all and without compromising the rights, safety or security of individuals.","We consider that the Privacy Act will be an important regulatory mechanism through which the Australian Government can address some of the risks of the use of AI in Australia.","We recommend that there should be a new right in the Privacy Act for individuals not to be subject to a decision based solely on automated processing which produces legal effects concerning that individual, or which similarly significantly affects that individual, unless it is necessary for entering into or performing a contract between the individual and the organisation, is authorised by law, or is based on the individual\'s explicit consent."]},"175_Supporting repsonsible AI Discussion Paper Submission July 2023 Public Purpose.0c02e5ad8af02.pdf":{"organization_name":"Public Purpose Pty Ltd","organization_type":"Advisory practice","classification":"Proponent","overall_position":"Advocates for responsible AI governance with a focus on public sector implementation","arguments":["AI governance needs to balance innovation with safety","Indigenous perspectives should be incorporated into AI governance","Human responsibility and judgement should remain central in AI decision-making"],"counterarguments":["null"],"key_recommendations":["Embed an Indigenous framework at the heart of AI policy and regulation","Align philosophy, principles and practice for effective use of AI in government","Get the right balance between humans and machines in AI implementation","Make AI systems and practice trustworthy"],"risks_and_challenges":["Rapid evolution of AI technologies outpacing policy and regulatory responses","Potential for AI to reinforce ideologies and world views","Risk of errors and unexpected consequences in AI implementation"],"safeguards_and_mitigations":["Develop standards for safe AI diffusion","Implement a process of certification across the algorithmic lifecycle","Ensure human-in-the-loop processes in AI decision-making"],"examples":{"Indigenous approach to land management as a model for AI governance":"The Indigenous approach to land is that it is not separate from who we are and if cared for is an infinitely bountiful gift. The ethics of an algorithmic system cannot be divorced from the system itself.","Use of engineering and actuarial professions as models for AI governance":"Historically, society has approached similar challenges by relying on suitable qualified people who are subject to strict professional standards to take responsibility. For example, we rely on engineers to certify that bridges and building are built to the required standard."},"international_alignment":"null","values":["Trustworthiness","Human responsibility","Ethical alignment","Wellness of the whole system"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Public sector leaders","role":"Implement and oversee responsible AI use in government"},{"entity":"Indigenous people","role":"Provide insights for AI governance frameworks"}],"sector_impacts":[{"sector":"Public sector","impact":"Improved policy-making and service delivery through responsible AI integration"}],"quotes":["It\'s about our humanity, not the technology. That assumption shouldn\'t be an item on a checklist. It should be an assumption about how we approach and build AI from the start.","Being trustworthy is not either an outcome or a measure. It\'s a way of thinking, knowing and acting that has to go all the way down and across every aspect of the way AI tools and capabilities are conceived, deployed and evaluated.","Complex systems like AI are non-linear, have feedback loops, are fractal in nature and are defined by emergent properties, where the system needs to be considered as a whole, rather than as a sum of its parts. Wellness of the whole system is the principal aim, not a desired outcome or aspirational goal."]},"218_Submission_Vijeyarasa_Gender and AI.94e242aa3011e.pdf":{"organization_name":"University of Technology Sydney","organization_type":"Academic institution","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on gender-responsive approaches","arguments":["AI systems can perpetuate gender-based biases and discrimination","Current regulatory approaches lack adequate consideration of gender impacts","Greater diversity in AI governance is needed to address gender-based harms"],"counterarguments":["null"],"key_recommendations":["Adopt a gender lens in future Australian Artificial Intelligence Act","Strengthen accountability through gender assessments and independent mechanisms to flag harmful content","Ensure active participation of diverse women in AI governance"],"risks_and_challenges":["Allocative harms resulting in loss of opportunities for women","Representational harms reinforcing gender stereotypes","Knowledge-based harms due to unequal representation in AI leadership"],"safeguards_and_mitigations":["Gender-specific impact assessments","Trusted flaggers to identify harmful online content","Gender-sensitive due diligence in AI governance"],"examples":{"AI recruitment bias":"An AI system used in a recruitment process may disproportionately classify applications for male candidates as more suitable than female.","Virtual assistants reinforcing stereotypes":"The use of female voices in AI-powered virtual assistants \u2013 Amazon\'s Alexa, Apple\'s Siri, Microsoft\'s Cortana and Google\'s Voice Assistant.","Deepfakes in pornography":"The use of \'deepfakes\' in pornography-related attacks that are a non-consensual form of gender-based online abuse."},"international_alignment":"Supports learning from international approaches, particularly Canada, Brazil, and the EU","values":["Gender equality","Non-discrimination","Accountability","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop gender-responsive AI regulations"},{"entity":"Private and public entities","role":"Conduct gender-sensitive due diligence in AI deployment"},{"entity":"Women\'s rights organizations","role":"Act as trusted whistleblowers and participate in AI governance"}],"sector_impacts":[{"sector":"Employment","impact":"Potential discrimination in recruitment processes"},{"sector":"Technology","impact":"Perpetuation of gender stereotypes in AI applications"}],"quotes":["A gender-responsive approach to addressing the harms of AI involves recognizing what can broadly be categorised as three types of harms:","Gender-sensitive due diligence places a particular emphasis on the experiences of women and girls, and the multiple intersecting forms of discrimination that influence the realisation of equal rights.","The over-representation of men in the design of AI-driven technologies is widely acknowledged. Meanwhile women dominate among those scholars identifying the gender-based biases that are and can result from AI\'s deployment."]},"331_AI Safety Melbourne Submission FINAL.52974677d2a43.pdf":{"organization_name":"AI Safety Melbourne","organization_type":"Community group","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI, particularly advanced and general-purpose AI systems","arguments":["Current AI landscape poses significant risks that require proactive regulation","Structural forces of AI are analogous to powerful economic and strategic assets like nuclear weapons and oil","Advanced AI systems will be difficult to control and may act in unpredictable and deceptive ways"],"counterarguments":["Some experts argue that catastrophic AI scenarios are implausible","Concerns about over-regulation stifling innovation","Arguments that AI is just advanced statistics and not inherently dangerous"],"key_recommendations":["Create distinct strategies for narrow AI vs general purpose AI","Adopt four key principles for managing general purpose AI: Monitor AI deployment, Control supply & distribution channels, Low-risk high-value deployment, Invest in AI safety","Implement a General-Purpose AI Risk Management Strategy"],"risks_and_challenges":["Misaligned AI pursuing unintended objectives","AI terrorism and malicious use of advanced AI systems","Passive or active AI takeover scenarios","Geopolitical risks and arms-race dynamics"],"safeguards_and_mitigations":["Implement a four-stage approval process for deployment of advanced AI systems","Establish a permit scheme for users of advanced AI systems","Conduct regular audits and compliance checks on AI usage"],"examples":{"Norway\'s oil management":"Norway has managed their crude oil resources with such success that even they deem it to be a lucky turn of events.","AI system in Qbert game":"an AI agent trained to play the arcade game Qbert learned that it could maximise points by repeatedly committing suicide in order to defeat its enemies","ChaosGPT incident":"Screenshot from the infamous ChaosGPT video, where an anonymous user releases a terrorist version of AutoGPT on the internet."},"international_alignment":"Supports international cooperation and the establishment of an international AI regulator","values":["Safety","Accountability","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement and enforce AI regulations, invest in AI safety research"},{"entity":"AI Labs","role":"Develop AI systems responsibly, adhere to safety protocols"},{"entity":"Cloud compute providers","role":"Assist in monitoring compliance with regulations"}],"sector_impacts":[{"sector":"National security","impact":"Increased need for capabilities to combat malicious uses of AI"},{"sector":"AI safety industry","impact":"Potential for significant growth and export opportunities"}],"quotes":["AI is crude oil for the information age; it powers machines to do things that were previously not possible.","The ideal landscape for advanced AI is one where development is carried out by a highly targeted cohort of trustworthy, well-governed organisations.","Being effective in handling key policy issues will increasingly require the Government to have a nuanced understanding of what is happening in the world of AI, how AI can interface with the world via technological tools, who is developing these capabilities, and what this means for Australia."]},"280_2023-26-07_SOCY_Submission_Dept Industry_AI Discussion Paper.f9c448bf151ae.pdf":{"organization_name":"ANU School of Cybernetics","organization_type":"Academic institution","classification":"Proponent","overall_position":"Advocates for comprehensive capability building and systems thinking approach to AI governance","arguments":["AI is not a singular technology, but a constellation of technologies, people and practices","Cybernetics approach is important for AI governance to steer towards better outcomes","Diverse teams and perspectives are crucial for responsible AI development and governance"],"counterarguments":["null"],"key_recommendations":["Invest in capability and capacity building for AI systems","Adopt a whole-of-government, whole-of-society approach to AI readiness","Encourage diversity in AI education and development","Focus on systems thinking and cybernetics in AI governance"],"risks_and_challenges":["Energy intensity of AI technologies","Potential for bias and lack of diversity in AI development","Diminished trust in social institutions and the role of new technologies"],"safeguards_and_mitigations":["Cohort-based learning to build strong ties and community","Partnerships with industry, government, and non-profit sectors for practical application of cybernetic tools","Investment in storytelling to shape the socio-technical imagination of AI systems"],"examples":{"Diverse educational programs":"Currently, we offer three quite different kinds of learning experiences (see Table 1) to build a range of AI system literacy and competence.","Industry partnerships":"Our partners have fostered training and development in their organisations by supporting their people to participate in the Master of Applied Cybernetics.","Residency program":"Our first two cohorts of residents (see Table 3) are extraordinary and already helping re/shape the way we think and work."},"international_alignment":"null","values":["Diversity","Systems thinking","Sustainability","Responsibility"],"tone":"Positive","stakeholders":[{"entity":"Academic institutions","role":"Provide education and build capability for AI systems"},{"entity":"Industry","role":"Partner in capability building and practical application of AI tools"},{"entity":"Government","role":"Support and engage in whole-of-government approach to AI readiness"}],"sector_impacts":[{"sector":"Education","impact":"Development of new educational models and programs for AI literacy"},{"sector":"Creative industries","impact":"New opportunities for storytelling and engaging with AI technologies"}],"quotes":["Put another way, AI (including AI governance) is a complex, dynamic system and we need to frame our approach to capacity and capability building for the future with that in mind.","We believe that cybernetics \u2013 an approach to complex dynamic systems that concentrates on the relationships between the human, the technical and ecological, as well as the individual component pieces \u2013 is an important way to approach AI, and therefore is a way in for those working in AI governance and regulation to steer towards better outcomes.","We see significant value in an approach focused on how we build new capability and capacity in systems thinking, framed by cybernetics."]},"306_UNSW.ai submission to Responsible AI.98937e768dff2.pdf":{"organization_name":"UNSW AI Institute","organization_type":"Academic research institute","classification":"Proponent","overall_position":"Supports a risk-based approach with AI-specific regulation and updating existing laws","arguments":["AI presents unique challenges that require specific regulatory frameworks","A risk-based approach similar to the EU AI Act has much to offer","Government investment is crucial for responsible AI deployment"],"counterarguments":["Some challenges can be addressed by enforcing existing generic rules","Poorly targeted regulation can have unintended consequences","Over-regulation could hinder research and innovation"],"key_recommendations":["Significant government investment in AI research, translation, and regulatory bodies","Develop AI-specific regulatory frameworks for cross-sector threats","Use government procurement power to set standards for responsible AI deployment","Invest in sovereign AI capability","Increase transparency in AI deployment","Implement widespread AI education and training programs"],"risks_and_challenges":["AI deployment at unprecedented speed and scale","Potential for biases, discrimination, and unfairness to vulnerable populations","Threats to privacy, consent, individual dignity, and human rights","Misinformation and deep fakes","Lack of transparency in AI systems"],"safeguards_and_mitigations":["Develop and pursue design guidelines and codes of ethics for AI development","Implement AI-specific regulatory frameworks","Ensure affordable access to data for oversight purposes","Keep humans in the loop for critical decision-making processes","Enhance AI literacy at all levels of society"],"examples":{"Biometric identification":"There are, however, some new threats that cross sectors, such as the use of AI in biometric identification or in employment.","Facial recognition technology":"Some AI technology is specially concerning, such as facial recognition that identifies sexual and political preferences.","Social media use by children":"For example, one concerning area is social media used by children and teenagers."},"international_alignment":"Supports alignment with global standards while adapting to local context","values":["Transparency","Fairness","Accountability","Human rights","Privacy","Sovereignty"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Invest in AI research, develop regulations, and set standards through procurement"},{"entity":"Academia","role":"Conduct research, provide expertise, and contribute to policy development"},{"entity":"Industry","role":"Implement responsible AI practices and collaborate with other stakeholders"},{"entity":"Public","role":"Engage in AI education and provide input on ethical considerations"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential for improved diagnostics and patient care, but requires careful regulation"},{"sector":"Employment","impact":"Risk of biases in hiring processes, need for specific regulations"},{"sector":"Education","impact":"Need for widespread AI education and training programs"},{"sector":"Finance","impact":"Potential for improved services but requires careful regulation"}],"quotes":["Although some of the challenges of ensuring that AI is deployed responsibly may be addressed by enforcing existing generic rules (such as those around privacy) and domain specific legislation (such as that around financial services or medical devices), the majority of the challenges and risks are not yet addressed by the existing ethical and regulatory frameworks.","Artificial intelligence does throw up several unique challenges. One of the most important is the speed and scale with which AI technologies can be deployed. A consequence of this is that there is some urgency for government to respond to these challenges.","Australia needs to cooperate and keep pace with the global leaders in the AI regulatory eco-system."]},"354_Roche Submission to the Safe and Responsible AI in Australia Discussion Paper 2023.acdb9bb222137.pdf":{"organization_name":"Roche","organization_type":"Pharmaceutical and diagnostics company","classification":"Proponent","overall_position":"Supports responsible AI use with balanced approach to risks and benefits","arguments":["AI can dramatically accelerate and transform research into new therapeutics, diagnostics, and treatments","AI-based solutions have the potential to support healthcare to become more efficient through personalising healthcare","AI can help identify changes in patient health and enable tracking of similar patient groups across institutions and geographies"],"counterarguments":["null"],"key_recommendations":["Federal coordination of governance for data sharing and access","Government-led campaign to update Australian\'s understanding of data literacy","Coordinated effort to upskill and educate all stakeholders along the healthcare value chain"],"risks_and_challenges":["Potential risks of patient harm from AI misuse or negligence","Fragmented approaches to AI technologies","Lack of public understanding leading to trust issues"],"safeguards_and_mitigations":["Human-in-the-loop to make final decisions on AI recommendations","Regular checkpoints to ensure AI avoids and minimises bias while maximising fairness","Robust testing over wide-ranging test datasets","Consideration of security during the design and development of AI systems"],"examples":{"Digital solution for pathology":"Our digital solution for pathology now delivers high-resolution digital images of tissue samples even faster as the evaluation is not carried out using a microscope, but electronically. AI makes it easier to diagnose cancer and enables more targeted and effective treatment for the patient concerned.","Use of ML in research and development":"ML is essential to research and development because the life sciences field is generating more data now than ever before, and ML algorithms are needed to parse large amounts of data to help find patterns that are challenging for human experts to identify."},"international_alignment":"Supports global coordination and learning from international experiences","values":["Responsibility","Patient benefit","Efficiency","Trust"],"tone":"Positive","stakeholders":[{"entity":"Government","role":"Lead in increasing public trust and coordinating AI governance"},{"entity":"Private sector","role":"Demonstrate benefits and involve in ethical AI partnerships"},{"entity":"Healthcare stakeholders","role":"Support government efforts to provide credible information"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved patient outcomes and increased system efficiencies"},{"sector":"Research and Development","impact":"Accelerated drug discovery and development of new therapies"}],"quotes":["While the Discussion Paper is looking to identify potential gaps in the existing domestic governance landscape, it must not lose sight of the enormous patient benefit that this new technology can bring.","Roche believes that any new governance mechanisms must take a balanced approach to the potential risks and benefits of the use of this new technology.","To avoid barriers for adoption and fragmented approaches to these technologies, it is essential to include a goal around federal coordination of governance for data sharing and access, supported by federal funding, that will enable greater use and uptake of AI across Australia."]},"360.pdf":{"organization_name":"null","organization_type":"Individual submission","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of Automatic Activity (AI) through a legislative containment model","arguments":["Current laws are inadequate to address the risks and challenges posed by AI","AI has the potential to create irreversible critical dependencies in various spheres of life","Regulation is necessary to ensure AI aligns with national interests and human rights"],"counterarguments":["Some AI proponents argue for light-touch regulation to avoid hindering growth","Global AI companies may resist regulation that limits their access to data or market power"],"key_recommendations":["Implement a legislative containment model for Automatic Activity","Establish a licensing system for certain types of AI research, development, and deployment","Promote sovereign AI capabilities through incentive programs","Reform Five Eyes intelligence practices to prioritize human intelligence and privacy"],"risks_and_challenges":["Mass unemployment due to AI replacing human workforces","Threats to democracy and unfair trading practices","Privacy concerns and potential for criminal behavior","Dominance of big tech companies and abuse of market power","National security threats and the rise of China\'s AI capabilities"],"safeguards_and_mitigations":["Prohibit certain types of Automatic Activity (e.g., judicial or legislative powers, human implants)","Implement licensing requirements for high-risk AI applications","Ensure human review and assistance for AI-assisted activities","Promote data handling on a need-to-know basis","Establish industry standards for security and privacy"],"examples":{"Sully AI Scenario":"Illustrates how closed-source AI may lead to monopolies of irreversible human dependence","AI Spy vs Spy Scenario":"Explores AI security fallacies and hazards of trusting public-private partnerships with government business","The Investment Scenario":"Demonstrates the potential risks of unregulated AI in biotech and personal assistance"},"international_alignment":"Supports a national approach while acknowledging global AI competition","values":["National security","Privacy","Human rights and fundamental freedoms","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, promote sovereign AI capabilities"},{"entity":"AusIndustry","role":"License AI research, development, and deployment"},{"entity":"ACCC","role":"License AI operations and monitor market power abuses"},{"entity":"Security Services Minister","role":"License AI for security services"}],"sector_impacts":[{"sector":"Aviation","impact":"Potential monopolization and critical dependency on AI systems"},{"sector":"Intelligence and national security","impact":"Shift from over-reliance on electronic intelligence to human intelligence"},{"sector":"Finance","impact":"AI trading algorithms potentially pushing out human investors"}],"quotes":["How do we control technologies which though not sentient, are apparently quicker and smarter than humans, which can make decisions affecting us?","The proposed containment model also bans all Automatic Activities seeking to exercise judicial or legislative powers, functions of review or arbitration; unlawful, unconstitutional or dishonest operation; subliminal practices or those lacking full free and informed consent; or controlling human implants.","Nothing like this exists in Australia in relation to Automatic Activity."]},"63_Legal Considerations for a Safe and Responsible AI Future in Australia.7892a93d3243e.pdf":{"organization_name":"Terrene Global","organization_type":"Company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and legal frameworks for AI","arguments":["AI deployment must be accompanied by legal structures and safeguards","Current legal frameworks are insufficient to address AI-related challenges","Privacy laws in Australia are imbalanced compared to international standards"],"counterarguments":["null"],"key_recommendations":["Establish comprehensive privacy regulations aligned with AI technology","Develop clear guidelines for AI transparency and accountability","Address gaps in criminal and civil procedures concerning AI"],"risks_and_challenges":["Flawed algorithmic decision-making","Biased outcomes in AI systems","Exploitation of personal data","Disinformation and \'fake news\' propagation"],"safeguards_and_mitigations":["Conduct independent evaluations and algorithmic impact assessments","Implement effective mitigation strategies for identified disparities","Establish strong safeguards for biometric data collection and usage"],"examples":{"Robodebt scheme":"The Robodebt scheme between 2016 and 2019 resulted in over half a million inaccurate Centrelink debts being raised, causing significant harm.","Amazon AI recruiting system":"An Amazon AI-powered recruiting system developed in 2014 showed bias against female candidates due to skewed training data.","Clearview AI controversy":"The ACLU v. Clearview AI case exposed the misuse of biometric data for surveillance purposes."},"international_alignment":"Supports aligning with international privacy standards","values":["Transparency","Accountability","Privacy","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Establish legal frameworks and regulations for AI"},{"entity":"Judicial system","role":"Adapt legal procedures to address AI-related cases"}],"sector_impacts":[{"sector":"Finance","impact":"Potential for significant financial losses due to algorithmic trading errors"},{"sector":"Healthcare","impact":"Improved medical data analysis capabilities"}],"quotes":["The integration of Artificial Intelligence (AI) into our society marks a significant milestone in humanity\'s progress.","To address these concerns, it is crucial to adhere to the principles outlined in the AI Bill of Rights.","By addressing these various considerations, we can pave the way for a legal AI landscape that maximizes its potential while safeguarding our Australian society."]},"305_Responsible AI Submission - Australasian Cyber Law Institute.1519fa4a41ee5.pdf":{"organization_name":"Australasian Cyber Law Institute","organization_type":"Research institute","classification":"Proponent","overall_position":"Supports comprehensive regulation with a focus on safety and human rights","arguments":["Existing regulatory approaches may not adequately address AI risks","A safety-based approach is warranted due to potential severe harms","AI governance should be based on universal human rights principles"],"counterarguments":["Banning high-risk AI activities may impact Australia\'s tech sector and trade","Generic solutions may not be suitable for all AI applications"],"key_recommendations":["Establish or nominate a primary regulator for AI","Implement mandatory risk assessments for certain AI applications","Operationalize Australia\'s AI Principles within AI services","Introduce an equivalent of the Consumer Data Right for AI products"],"risks_and_challenges":["Privacy breaches through data use in AI training","Invasive surveillance and bias in facial recognition technology","Community division and manipulation through social media algorithms","Machine bias in automated decision-making"],"safeguards_and_mitigations":["Adoption of a safety-based approach","Implementation of industry codes setting out risks and minimum outcomes","Establishment of certification schemes for trustworthy AI","Encouragement of responsible innovation practices"],"examples":{"Food Standard Code labelling standards":"ACLI notes that the Food Standard Code labelling standards provides a useful regulatory approach to transparency requirements and may provide a sensible parallel for AI transparency.","Consumer Data Right":"Introducing an equivalent of the Consumer Data Right for any products utilising data profiles would improve the ability of people to vote with their feet when they consider that a company\'s approach isn\'t acceptable."},"international_alignment":"Supports consideration of European approach while tailoring to Australian context","values":["Transparency","Human rights","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Lead by example in applying risk assessment frameworks"},{"entity":"Private sector","role":"Align corporate values with AI Principles"}],"sector_impacts":[{"sector":"Tech sector","impact":"Potential impact on exports if high-risk activities are banned"},{"sector":"Public sector","impact":"Higher risk rating for AI systems due to mandatory nature of interactions"}],"quotes":["ACLI takes the view that the potential harms from poorly implemented AI are severe enough to warrant the adoption of a safety-based approach that imposes responsibility on those deploying AI to ensure it does not do harm.","We consider that both public and private sector use of AI should be subject to the same laws and standards.","ACLI strongly believes that basing AI adoption on universal human rights principles will help facilitate and strengthen trust in the ongoing development of AI technology."]},"478_Submission 478 - Meta - 9-Aug.ca0e4157f039.pdf":{"organization_name":"Meta","organization_type":"Technology company","classification":"Proponent","overall_position":"Supports principle-based, risk-based approach to AI regulation while encouraging open innovation and building upon existing regulatory frameworks","arguments":["AI is already widely deployed within industry and supporting many regulatory obligations","Existing regulatory frameworks may already address many AI policy concerns","Open innovation approach increases market contestability and has safety benefits"],"counterarguments":["AI poses new and unique governance questions","Some existing laws may need to be adjusted to address new AI concerns","Rushing to impose onerous requirements risks creating substantial risks for companies and users"],"key_recommendations":["Use definitions that strike the right balance between precision and flexibility","Review existing regulatory frameworks to assess fit-for-purpose","Adopt a framework to review and identify policy issues related to AI","Take a risk-based approach focused on most sensitive AI applications","Encourage open innovation and competition","Ensure AI regulation is a product of collaboration amongst multiple stakeholders"],"risks_and_challenges":["Potential for AI to generate harmful or biased content","Privacy concerns related to AI training data","Misinformation and toxicity in AI-generated content"],"safeguards_and_mitigations":["Use of AI for proactive detection and action on harmful content","Development of tools like Fairness Flow to analyze AI for risks or unintended consequences","Open-sourcing of AI models to allow for transparent identification and mitigation of risks"],"examples":{"AI improving content moderation":"In the last five or so years, we have had a strong focus on using AI to help enforce our Community Standards, which are the rules that set out what people can or cannot do on Facebook and Instagram.","AI supporting small businesses":"Many Australian businesses, especially small businesses benefit from using personalised advertising because it is more efficient and allows them to better reach the right consumer for their business and compete with larger established businesses.","Open-sourcing large language models":"We released over 1,000 models and AI databases on non-commercial licences for researchers, so that they can benefit from the computing power we are able to deploy and pursue their own research openly and safely."},"international_alignment":"Supports globally-harmonised frameworks","values":["Transparency","Fairness","Innovation","Privacy","Safety"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Develop flexible and future-proof AI regulations"},{"entity":"Industry","role":"Implement responsible AI practices and contribute to open innovation"},{"entity":"Researchers","role":"Contribute to development and evaluation of AI models"}],"sector_impacts":[{"sector":"Small businesses","impact":"Improved access to efficient and effective targeted advertising"},{"sector":"Online content moderation","impact":"Enhanced ability to detect and action harmful content at scale"}],"quotes":["Meta has been at the global forefront of calling for updated regulation for the internet.","We are still at the very early stages of AI technology, and there is an exciting opportunity for the global community working on AI to strive towards the innovations that will help to solve our greatest challenges.","The fundamental challenge is to develop regulations that are broad and flexible enough to adapt to future technologies while not overly restrictive to the point of suppressing valuable and beneficial innovations in, and uses of, AI technology."]},"212_The Need for Human Rights-centred Artificial Intelligence.ab4c91d3fef12.pdf":{"organization_name":"Australian Human Rights Commission","organization_type":"Government agency","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and human rights-centered approach to AI","arguments":["AI poses significant risks to human rights and privacy","Existing regulatory frameworks are insufficient to address AI-specific challenges","Comprehensive regulation is needed to ensure responsible AI development and use"],"counterarguments":["Regulating specific technologies may result in delays on uptake or realization of economic benefits","Placing the onus on individuals to protect their own data is insufficient"],"key_recommendations":["Establish an AI Safety Commissioner as an independent statutory office","Introduce legislation to regulate facial recognition and other biometric technologies","Require human rights impact assessments before using AI-informed decision-making systems"],"risks_and_challenges":["Privacy intrusions and data breaches","Algorithmic bias and discrimination","Misinformation and disinformation amplified by AI","Automated decision-making without proper human oversight"],"safeguards_and_mitigations":["Implement explicability functions in AI systems","Conduct regular audits of AI use in government and private sectors","Increase digital literacy and public education on AI risks"],"examples":{"Robodebt scheme":"The \'robodebt\' scheme demonstrates the dangers of the of utilising ADM systems which lack human scrutiny and where clear, understandable reasons cannot be provided for decisions that inherently impact a person\'s human rights.","Amazon\'s AI hiring tool":"Amazon used an AI software that was designed to review resumes and determine which applicants Amazon should hire. The algorithm systemically discriminated against women applying for technical jobs, such as software engineer positions.","Clearview AI":"Clearview AI, who scraped approximately 3 billion images of faces from publicly accessible sources (such as Facebook and Google) to create a database. The company then licensed this database to over 600 law enforcement agencies (in addition to banks, private companies and schools)."},"international_alignment":"Supports consideration of international approaches while developing national regulation","values":["Human rights","Privacy","Transparency","Accountability","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, conduct audits, and increase public awareness"},{"entity":"Private sector","role":"Implement ethical AI practices and comply with regulations"},{"entity":"AI Safety Commissioner","role":"Provide expert guidance and oversight on AI development and use"}],"sector_impacts":[{"sector":"Employment","impact":"Potential discrimination in hiring and firing processes"},{"sector":"Law enforcement","impact":"Risks associated with facial recognition technologies"}],"quotes":["The Australian Government should not make administrative decisions using automation or artificial intelligence if the decision maker cannot generate reasons or a technical explanation for an affected person.","Australia should introduce specific legislation to address the risks of artificial intelligence, that are not already sufficiently addressed within the existing regulatory framework.","The Commission considers that in light of recent technological developments, a right to request reasons is more pressing than ever. However significant work is required to include explicability functions to protect human rights."]},"257_ForHumanity responsible AI in Australia submission draft response.551b3eb16dba3.pdf":{"organization_name":"ForHumanity","organization_type":"Non-profit organization","classification":"Proponent","overall_position":"Advocates for comprehensive regulation through independent audits of AI systems","arguments":["Self-assessment and self-regulation are insufficient for high-risk AI systems","Independent audits create an infrastructure of trust for AI governance","Risk-based approach is necessary for effective AI regulation"],"counterarguments":["Organizations claim internal frameworks are sufficient","Concerns about financial burden of regulation","Self-regulation as a typical first-step response to new technologies"],"key_recommendations":["Establish oversight body for certifying bodies and approving audit criteria","Adopt risk-based approach to AI governance","Implement mandatory annual risk-based audits conducted by independent third parties","Create a list of prohibited AI systems","Develop sector-specific and use case specific criteria for AI certification"],"risks_and_challenges":["Unfairness due to statistical, cognitive, and non-response bias in data and systems","Lack of transparency and explainability in AI systems","Insufficient governance and oversight","Model drift in automated decision making"],"safeguards_and_mitigations":["Independent Audit of AI Systems (IAAIS)","Certification schemes aligned with local legal requirements and international best practices","Training and accreditation of certified auditors","Establishment of certification bodies"],"examples":{"Robodebt scandal":"Robodebt, which has been found to have been venal, incompetent and to require enabling legislation, demonstrates most organisations will default to self-interest or profit over human rights, public interest and compliance, even in the Government sector without independent oversight.","Financial audits":"ForHumanity enables an infrastructure of trust, modelled on existing financial audit approaches, in support of the inevitable maturity in governance, oversight and accountability for an AAA System"},"international_alignment":"Supports global standards with local implementation","values":["Accountability","Transparency","Human rights protection"],"tone":"Cautionary yet proactive","stakeholders":[{"entity":"Government","role":"Establish oversight body and approve audit criteria"},{"entity":"Certification Bodies","role":"Conduct independent audits of AI systems"},{"entity":"Corporations","role":"Implement governance, oversight, and accountability for AI systems"}],"sector_impacts":[{"sector":"All sectors using AI","impact":"Increased accountability and trust in AI systems"}],"quotes":["ForHumanity\'s mission is to: \'....examine and analyse downside risk associated with the ubiquitous advance of AI, algorithmic and autonomous systems and where possible to engage in risk mitigation to maximise the benefits of these systems\u2026\'","Self-assessment is an important first-step forward, but can only be the endgame for systems and processes that are low-risk on the human-rights scale.","ForHumanity enables an infrastructure of trust, modelled on existing financial audit approaches, in support of the inevitable maturity in governance, oversight and accountability for an AAA System"]},"372_230802 - CHOICE submission to the Safe and Responsible AI in Australia consultation.dee476b48c40e.pdf":{"organization_name":"CHOICE","organization_type":"Consumer advocacy group","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI through new AI-specific laws and strengthening existing laws","arguments":["Strong laws and well-resourced regulators are needed to promote responsible AI use and protect consumers","Existing sector-specific legislation and voluntary industry codes are inadequate to protect consumers from AI harms","A risk-based framework is needed to classify AI uses and apply appropriate restrictions"],"counterarguments":["null"],"key_recommendations":["Introduce economy-wide legislation to regulate AI in Australia","Develop a risk-based framework for AI regulation","Establish an AI Commissioner with regulatory powers","Strengthen existing laws like the Australian Consumer Law and Privacy Act","Establish a digital ombudsman for consumer complaints related to AI"],"risks_and_challenges":["Algorithmic discrimination and perpetuation of bias","AI-assisted scams","Spread of misinformation","Chatbots promoting harmful behavior","Misuse of consumer data in automated decision making"],"safeguards_and_mitigations":["Legal restrictions and prohibitions on high-risk AI uses","Requirement for AI systems to be safe, fair, reliable, transparent, and accountable","Impact assessments for high-risk AI systems","Product intervention powers for the AI Commissioner"],"examples":{"Consumer concerns about data use":"CHOICE\'s research has found that 71% of consumers are concerned about their data being used in automated decision making. Over half of consumers are concerned about how their data is being used to personalise products and services marketed to them.","Harms from social media platforms":"Many of the Big Tech companies that are deploying AI systems have already contributed to widespread consumer harm, including from the use of social media platforms."},"international_alignment":"Supports alignment with international approaches, referencing EU and Canadian AI regulation efforts","values":["Safety","Fairness","Reliability","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Federal Government","role":"Introduce and enforce AI legislation"},{"entity":"AI Commissioner","role":"Enforce AI laws and coordinate with other regulators"},{"entity":"Businesses","role":"Comply with AI regulations and ensure safe AI systems"}],"sector_impacts":[{"sector":"HealthTech","impact":"Potential for emerging risks that need to be addressed"},{"sector":"FinTech","impact":"Potential for emerging risks that need to be addressed"}],"quotes":["Consumers in Australia need strong protections from the use of arti\ufb01cial intelligence (\'AI\').","Businesses that use AI should not be trusted to regulate themselves. Strong laws, enforced by well-resourced regulators are needed to promote the responsible use of AI and protect consumers from further harm.","This consultation comes at a critical time, as Australia risks lagging behind international developments in the regulation of AI. The Federal Government has an opportunity to create an enduring regulatory framework to ensure the safe and fair use of AI in Australia."]},"394_human_compatible-submission-hmc7s3ekwko2jpav.0b1cd68928f93.pdf":{"organization_name":"Human Compatible","organization_type":"Private organization","classification":"Proponent","overall_position":"Supports a balanced regulatory approach that builds on existing frameworks while addressing emerging issues","arguments":["A risk-based approach appropriately balances legislative obligations with the need for flexibility","Non-regulatory initiatives can complement regulation in building trust and capabilities","Existing laws provide a baseline level of protection but gaps exist in regulating systemic impacts and emerging capabilities"],"counterarguments":["Overly restrictive regulation of AI could disadvantage Australian companies","Blanket human oversight requirements may not always be feasible or desirable"],"key_recommendations":["Establish an independent advisory council on AI ethics and governance","Develop voluntary certification programs and testing facilities with industry collaboration","Require impact assessments, transparency and contestability mechanisms for high-risk AI","Take a precautionary approach by monitoring and preparing mitigation strategies for extreme long-term scenarios"],"risks_and_challenges":["Failure to regulate emerging high-risk applications before widespread harm occurs","Lack of coordination across government creates a complex regulatory environment","Potential for AI to fundamentally change the nature of employment"],"safeguards_and_mitigations":["Develop specialised conformity infrastructure to support standards and auditing","Leverage international collaboration on technical standards","Support SMEs to adopt AI responsibly through initiatives like the AI Adopt program"],"examples":{"Royal Commission into Centrelink":"This potential impact has been highlighted by the recent Royal Commision into failures of automated decision making deployed by Centrelink.","JobKeeper during Covid Pandemic":"We see a powerful analogy in the Government\'s successful response by way of JobKeeper etc during the Covid Pandemic."},"international_alignment":"Supports international coordination on identifying and governing emerging capabilities","values":["Transparency","Accountability","Safety","Fairness"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Encourage safe and responsible adoption and use of AI by everyone"},{"entity":"Industry","role":"Collaborate on voluntary certification programs and testing facilities"},{"entity":"Community","role":"Participate in the development of programs and regulations concerning AI"}],"sector_impacts":[{"sector":"Education","impact":"Need to adapt to the reality of AI tools like ChatGPT"},{"sector":"Tech sector","impact":"Potential minimal impact in near term from banning certain high-risk AI applications"}],"quotes":["We recommend a balanced approach, one that builds on existing frameworks, while seeking to address the gaps as they become evident.","The key point here is to ensure Government is seen as a backstop for anyone adversely affected by AI, e.g. via loss of their job or an entire industry.","We therefore advocate a robust risk-based framework be combined with precautionary measures against plausible long-term scenarios such as this."]},"311_RMIT Enterprise AI and Data Analytics Hub - Our response.16dfd51ef742f.pdf":{"organization_name":"RMIT Enterprise AI and Data Analytics Hub","organization_type":"University research center","classification":"Proponent","overall_position":"Supports a multi-pronged approach to AI regulation, including updating existing laws and establishing new AI-specific guidelines","arguments":["Existing frameworks and policies can be used to institutionalize AI technologies","AI-specific governance regulations are necessary to address unique challenges","A human-centered approach is crucial in AI deployment and decision-making"],"counterarguments":["Blanket bans on AI applications may impede progress and limit Australia\'s global competitiveness","A single central body to regulate all sectors may not be effective"],"key_recommendations":["Evaluate current regulations and legislations to incorporate AI into existing regulatory frameworks","Establish AI-specific regulatory guidelines that define transparency and accountability","Initiate the design of explanations in AI systems deployed in public services","Co-create national AI policy by engaging various stakeholders","Introduce verification and authorization systems for AI"],"risks_and_challenges":["AI may exacerbate inequality in society","Potential for algorithmic bias and discrimination","Privacy and security concerns related to AI systems","Job displacement due to AI automation"],"safeguards_and_mitigations":["Implement \'humans in the loop\' approach for high-risk AI applications","Encourage interdisciplinary AI R&D","Develop public trust through initiatives that explain AI outputs","Implement robust authentication and authorization mechanisms","Ensure data privacy and protection through encryption and access control"],"examples":{"ChatGPT as an off-the-shelf AI system":"Take ChatGPT as an example, which would be considered off-the-shelves. However if an organisation decides to use ChatGPT\'s API with an in-house AI model to create a purpose-built system, trying to assess them different is not likely to make sense.","AI-based model at airports":"For example, an AI-based model used at the airport to manage biosecurity and immigration risks, or an AI model to identify irregularity in tax lodgements at the ATO would all come without the option for individuals to opt-out."},"international_alignment":"Supports alignment with international standards while adapting to Australia\'s unique characteristics","values":["Transparency","Accountability","Fairness","Privacy","Security","Human-centered approach"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, lead in implementing responsible AI"},{"entity":"Academia","role":"Conduct research and provide expertise on AI governance"},{"entity":"Industry","role":"Implement ethical AI practices and contribute to policy discussions"},{"entity":"Public","role":"Engage in discussions and provide input on AI governance"}],"sector_impacts":[{"sector":"Public sector","impact":"Higher standards of transparency and accountability in AI deployment"},{"sector":"Private sector","impact":"Need to balance innovation with responsible AI practices"},{"sector":"Financial services","impact":"Potential for improved fraud detection and risk assessment, but need for updated regulatory approaches"}],"quotes":["We believe any definition of AI should be sufficiently clear (or well-understood) for the intended regulatory purpose, and that AI should also be defined by its operational characteristics instead of the specific algorithms and techniques that will quickly become obsolete.","Our view is that Australia should adopt a response where the \'soul\' of the approach is similar to the key economies that Australia aligns itself with.","We believe there is an opportunity here to engage the collective wisdom of the population on how any AI regulatory framework can be shaped through the lived experience of its people."]},"282_WWDA Submission to the \'Safe and Responsible AI\' Discussion Paper.4f363922fdbab.pdf":{"organization_name":"Women With Disabilities Australia (WWDA)","organization_type":"Disabled People\'s Organisation (DPO) and National Women\'s Alliance (NWA)","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and human rights-based approach to AI governance","arguments":["AI has potential to improve accessibility and efficiency of services for people with disabilities","Responsible use of AI may improve data-driven policy making and evaluation","AI-assisted processes could reduce discriminatory practices in employment and other areas"],"counterarguments":["Increased use of AI may exacerbate existing inequities and further marginalize under-represented demographics","AI can perpetuate prejudices due to algorithmic bias","Overreliance on AI in service provision can have human rights implications for people with disability"],"key_recommendations":["Implement recommendations from the Royal Commission into the Robodebt Scheme","Address digital exclusion to ensure marginalized communities are not excluded from AI-assisted tools or services","Ensure AI design, development, use, and monitoring is consistent with international human rights obligations","Establish an independent body to monitor, investigate and oversee the use of AI"],"risks_and_challenges":["Digital exclusion of marginalized communities","Algorithmic bias and perpetuation of discrimination","Privacy violations and data security risks for people with disabilities","Reduced workforce participation for people with disabilities"],"safeguards_and_mitigations":["Implement human rights-based risk and impact assessments prior to AI use","Ensure transparency in AI-assisted decision-making","Mandate human oversight and accountability in AI systems","Clarify legislative frameworks for redress in cases of harm caused by AI-assisted decision making"],"examples":{"Robodebt Scheme":"The Robodebt Scheme saw a reversal of the onus of proof and the presumption of innocence whereby victims of the Scheme were required to prove that an automated debt assessment they had received was incorrect.","AI in job interviews":"When social styles were removed from the equation, and candidates were evaluated on the basis of transcripts generated from their interviews, Autistic candidates out-performed non-Autistic candidates.","AI in recruitment":"AI-powered recruitment tools have been found to favour male applicants for technical roles, because the data fed into the tools has reflected a historically male technical workforce."},"international_alignment":"Supports alignment with international human rights obligations","values":["Human rights","Accountability","Transparency","Inclusiveness","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Government","role":"Implement comprehensive AI regulation and oversight"},{"entity":"People with disabilities","role":"Engage in co-design and monitoring of AI systems"},{"entity":"Representative organizations","role":"Monitor impact of AI on rights of people with disabilities"}],"sector_impacts":[{"sector":"Public services","impact":"Potential to improve accessibility and efficiency, but risks of digital exclusion"},{"sector":"Employment","impact":"Potential for reduced discrimination, but also risks of job displacement"}],"quotes":["To ensure that the Australian public, including people with disability, benefit from the increased use of AI, it must be implemented, used and monitored ethically and responsibly. It must also involve a human rights-based approach.","Evidently, depending on its use and regulation, AI has the capacity to either promote or breach the rights of people with disability.","For automation to be considered trustworthy, it must be ethical, lawful and technically robust. It is tightly coupled with good governance and risk management, and acknowledges the unique risks presented by automated systems with a focus on human-centricity, commitment to the service of humanity and common good, and the goal of improving human welfare and freedom."]},"148_AI Submission from Philip Derham July 24 2023.bbee05ddd8b3e.pdf":{"organization_name":"Derham Insights Research","organization_type":"Small marketing research company","classification":"Proponent","overall_position":"Supports responsible AI practices with specific considerations for the market research sector","arguments":["AI tools can strengthen market research practices","Legislation is needed to ensure all market research entities comply with AI regulations","Market research sector needs to be expressly recognized in AI regulations"],"counterarguments":["null"],"key_recommendations":["Ensure AI platforms used in Australia comply with Australian law","Strengthen privacy protection and data security principles through legislation","Require AI algorithms to reflect current Australian society and values"],"risks_and_challenges":["Potential negation of privacy and anonymity guarantees to research participants","Use of AI products not based in Australia and not compliant with Australian laws","AI algorithms reflecting outdated views or not representing current Australian community values"],"safeguards_and_mitigations":["Legislation requiring Australia-based researchers to use AI platforms conforming to Australian Privacy Act and AI Ethics Principles","Ensuring AI systems enable fair and equitable segmentations","Requiring evidenced verifications of AI-produced segments, audiences, approaches, and outcomes"],"examples":{"Potential AI bias in healthcare":"The thought that if you turn up to a hospital, you have COVID-19, you need a ventilator, and you\'re over 50, and someone who\'s 30 turns up, they will give the ventilator to the 30-year-old, because the algorithm sees older people as less productive in society and therefore less valuable to humanity.","Italy\'s temporary ban on ChatGPT":"In March 2023, the Italian Data Protection Authority (Garante) announced a temporary conditional ban of ChatGPT, raising concerns about private data that had been gathered to \'train\' the product."},"international_alignment":"Considers international approaches but emphasizes need for Australian-specific regulations","values":["Privacy","Fairness","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Market research agencies","role":"Implement ethical AI practices in research"},{"entity":"Government","role":"Develop and enforce AI regulations specific to market research"}],"sector_impacts":[{"sector":"Market research","impact":"Potential financial burden due to compliance with multiple regulatory systems"},{"sector":"Financial services","impact":"Potential effects on use of customer data and market research analyses"}],"quotes":["We are concerned that the use of AI platforms, as those relate expressly to their potential to negate our privacy and anonymity of response guarantees to participants.","We see a need to ensure that market, social, consumer, staff, and government research service suppliers (i.e., market research agencies) are expressly recognised in such lists, as these suppliers have to meet the regulatory standards of data protection","To ensure all market research entities and users of market research tools and techniques are compliant, we recommend legislation as that should ensure that all Australian market research activities using AI comply."]},"287_Safe and responsible AI in Australia - EY submission 20230726.00f7cd49a5546.pdf":{"organization_name":"EY","organization_type":"Professional services firm","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with a mix of adaptations to existing legislation and new legislation for areas of significant risk","arguments":["A risk-based approach balances the need to avoid misuse while encouraging important innovation","Making the burden of compliance proportionate to the risks is critical","A risk-based approach can channel innovation into areas where benefits are likely to outweigh risks"],"counterarguments":["Risk-based approach depends on strong technical understanding which government may find challenging in a complex, fast-moving area","Technology is likely to become more deployable and less observable over time, making risk assessment difficult","Harms of biased or inaccurate AI decision-making may be serious but not readily observable"],"key_recommendations":["Establish a central regulatory body to oversee the sector, issue binding market guidance, monitor compliance and undertake enforcement actions","Adopt ethical principles and frameworks to guide agencies in AI implementation","Implement frequent monitoring and reporting mechanisms for AI systems, especially for government agencies and systemically important businesses"],"risks_and_challenges":["Opaque decision-making based on errors or bias","Novel privacy violations through detailed profiling using innovative inference methods","Dependence on a small group of suppliers for leading-edge AI technologies","Risks associated with \'natural\' (i.e., unstructured) decision-making by AI"],"safeguards_and_mitigations":["Implement transparency requirements across private and public sectors","Conduct and publish AI Impact Statements outlining potential effects and associated risks","Establish certification bodies to independently assess and validate AI systems against regulatory and voluntary standards","Implement redress mechanisms to challenge decisions made by AI systems and seek redress for harms"],"examples":{"Social scoring algorithms as potential high-risk application":"Social scoring algorithms.","Predictive policing as potential high-risk application":"Predictive policing","Widespread facial recognition as potential high-risk application":"Widespread / indiscriminate facial recognition and other forms of biometric identification"},"international_alignment":"Supports aligning with international standards while adapting to Australian context","values":["Transparency","Accountability","Fairness"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Establish regulatory framework, provide leadership and direction"},{"entity":"Private sector","role":"Implement responsible AI practices, comply with regulations"}],"sector_impacts":[{"sector":"Public sector","impact":"Higher standards for AI implementation due to greater impact and accountability"},{"sector":"Private sector","impact":"Regulatory compliance requirements proportionate to risks involved"}],"quotes":["EY supports the adoption of a consistent nationwide risk-based regulatory framework.","The time for establishing a framework for safe and responsible AI use is now.","Artificial Intelligence technology has undergone rapid development in recent years, and has the potential to revolutionise productivity and democratise a wide range of important capabilities."]},"151_(FINAL)_Submission_Kaspersky.3ceebd673bb59.pdf":{"organization_name":"Kaspersky","organization_type":"International cybersecurity company","classification":"Proponent","overall_position":"Supports risk-based regulation of AI systems with differentiated approaches for public and private sectors","arguments":["AI systems are not harmful by design, but malicious use poses risks","Regulation should be proportional to the level of risks associated with specific types of AI systems","Different approaches should be applied to public and private sector use of AI technologies"],"counterarguments":[],"key_recommendations":["Implement a risk-based classification of AI systems","Apply stricter regulations for public entities using AI","Adopt ethical principles for responsible AI use","Implement mandatory regulation tools for high-risk AI systems"],"risks_and_challenges":["Potential failure of AI systems in the public sector","AI hallucinations","Use of personal data in AI training"],"safeguards_and_mitigations":["Transparency in operational and decision-making processes","Advanced testing methods for ML algorithms","Human control over AI/ML systems","Minimization and anonymization of personal data in training datasets"],"examples":{"EU Artificial Intelligence Act":"This approach receives substantial support among the expert community and is currently under consideration by some jurisdictions, including the EU (the Artificial Intelligence Act)","Brazil Draft AI Law":"This approach receives substantial support among the expert community and is currently under consideration by some jurisdictions, including... Brazil (Draft AI Law of December 2022)","Canada Artificial Intelligence and Data Act":"This approach receives substantial support among the expert community and is currently under consideration by some jurisdictions, including... Canada (the Artificial Intelligence and Data Act)"},"international_alignment":"Supports international harmonization of AI regulation","values":["Transparency","Safety","Security","Human rights protection","Responsibility"],"tone":"Neutral","stakeholders":[{"entity":"Public sector","role":"Adhere to stricter standards and provide example of responsible AI use"},{"entity":"Private sector","role":"Implement ethical principles and comply with regulations"},{"entity":"Regulators","role":"Develop and enforce AI regulations"},{"entity":"AI/ML developers","role":"Exchange best practices and ensure responsible AI development"}],"sector_impacts":[{"sector":"Cybersecurity","impact":"AI/ML systems used to prevent and combat cyberthreats"},{"sector":"Critical infrastructure","impact":"High-risk AI systems subject to mandatory requirements"}],"quotes":["Generally, when assessing potential AI-related risks, it is important to proceed from the fact that AI systems are not harmful and dangerous by design. It is the malicious use of AI technologies that poses risks \u2013 not artificial intelligence itself.","We believe that future standards and regulations for AI systems should reflect a balance between ensuring safety and security (including proper protection of human rights and basic values) and avoiding the creation of artificial impediments to the development of AI technologies.","Countries should actively cooperate in developing national AI regulatory frameworks in order to maximize international harmonization of AI regulation."]},"244_Submission 244 - Attachment 1.82c43a18f378a.pdf":{"organization_name":"Department of Industry Science and Resources","organization_type":"Government department","classification":"Proponent","overall_position":"Supports balanced regulation that protects artistic freedom while addressing risks","arguments":["AI has beneficial cultural, artistic, and educational potential in film and TV production","Blanket ban on AI would severely disadvantage Australian film and TV industry","AI applications in screen arts should be protected under fair use and freedom of speech"],"counterarguments":["Deepfakes have malicious impact, especially on women and girls"],"key_recommendations":["Factor economic, cultural, and artistic advantages of AI screen applications into proposed legislation","Avoid implementing a wholesale ban on synthetic media in artistic screen works","Protect rights of filmmakers to use AI for artistic, educational, and social engagement purposes"],"risks_and_challenges":["90% of deepfake video is pornographic, with 90-95% targeting women and girls"],"safeguards_and_mitigations":["Align with existing Australian arts and entertainment regulation mechanisms"],"examples":{"AI in film production":"AI generated Multi Modal Foundation Models (MIM) are increasingly being used by film, TV and SVOD production companies, independent filmmakers and screen artists to streamline and enhance conventional film production practices","Deepfakes as special effects":"As a novel special effect, deepfake video also represents a continuation of the screen industry\'s long-standing quest to create believable human simulations","AI for cost reduction":"AI is used for dubbing, de-aging, simulation, location design, sound design, visual effects including computer-generated imagery (CGI) and actor replacement for a fraction of the cost"},"international_alignment":"null","values":["Artistic freedom","Innovation","Economic competitiveness"],"tone":"Cautionary optimism","stakeholders":[{"entity":"Filmmakers and screen arts practitioners","role":"Use AI technologies responsibly for artistic and educational purposes"},{"entity":"Government","role":"Develop balanced legislation that protects artistic freedom while addressing risks"}],"sector_impacts":[{"sector":"Film and TV industry","impact":"Potential disadvantage if AI use is overly restricted"},{"sector":"Screen arts","impact":"Enhanced creative possibilities and cost reduction through AI technologies"}],"quotes":["It is essential that the beneficial cultural, artistic and educational potential of new and evolving AI synthetic video applications is acknowledged in the proposed federal regulatory process.","The Australian film and TV industry would be severely disadvantaged by any \'blanket ban\' on the use of Generative AI models and synthetic media technologies, including deepfakes, in the film, TV and screen arts production, distribution and exhibition sectors.","The right of filmmakers and screen arts practitioners to use deepfake technology and other AI generated MIMs to create artistic works that foster social engagement and public discussion, educate audiences, and reflect the pressing issues of our time should be protected in any AI legislation"]},"62_Safe and Responsible AI in Australia.bcb645ea2961c.pdf":{"organization_name":null,"organization_type":null,"classification":null,"overall_position":null,"arguments":[null],"counterarguments":[null],"key_recommendations":[null],"risks_and_challenges":[null],"safeguards_and_mitigations":[null],"examples":{"example":null},"international_alignment":null,"values":[null],"tone":null,"stakeholders":[{"entity":null,"role":null}],"sector_impacts":[{"sector":null,"impact":null}],"quotes":[null]},"422_Submission in response to the Department of Industry, Science and Resources.pdf":{"organization_name":"Australian Digital Alliance","organization_type":"Nonprofit coalition","classification":"Proponent","overall_position":"Supports introducing a flexible copyright exception to facilitate AI development","arguments":["Current copyright laws create uncertainty for AI development in Australia","Licensing is not a practical solution for AI developers due to high costs and complexity","A flexible copyright exception would support innovation and make Australia competitive in AI development"],"counterarguments":["Some stakeholders suggest copyright licensing as a solution","Collective licensing schemes have been proposed as an alternative"],"key_recommendations":["Introduce a flexible, technology-neutral copyright exception for AI development","Update Australia\'s copyright system to support innovative uses of copyright-protected material","Work closely with the Attorney-General\'s Department to develop an optimal regulatory framework for AI"],"risks_and_challenges":["Potential copyright infringement when using third-party material in AI training data","Australia being out of step with more technology-friendly jurisdictions","Delays in introducing new exceptions for AI development"],"safeguards_and_mitigations":["Introducing a copyright exception to create certainty for AI developers","Ensuring the exception is flexible and technology-neutral to future-proof the copyright system"],"examples":{"Mention of AI-supportive copyright jurisdictions":"AI-supportive copyright jurisdictions include the United States of America (USA), Japan, Singapore, the European Union countries (where a pair of exceptions to copyright make any copies made for text and data mining not an infringement as long as their requirements are met), the United Kingdom (UK) and South Korea."},"international_alignment":"Supports aligning with more technology-friendly jurisdictions","values":["Innovation","Public interest","Fair access to content"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement copyright reforms"},{"entity":"AI developers","role":"Innovate and develop AI technologies within legal frameworks"},{"entity":"Copyright owners","role":"Protect their rights while allowing for innovation"}],"sector_impacts":[{"sector":"Technology","impact":"Improved competitiveness for Australian AI developers"},{"sector":"Creative industries","impact":"Potential concerns about use of copyrighted material in AI training"}],"quotes":["The ADA does not believe that direct licensing is a viable option because securing permissions to the large volume of data at play is impractical.","Australia\'s copyright system urgently needs to be updated to add much needed flexibility to support innovative uses of copyright-protected material such as in AI.","To enable the use of third-party copyright material in ML locally, we need a flexible and technology neutral copyright exception that can support the large-scale, non-consumptive use of material."]},"365_SAP Response to Safe and Responsible AI Discussion Paper August 2023.ce903b751922e.pdf":{"organization_name":"SAP Australia","organization_type":"Software company","classification":"Proponent","overall_position":"Supports updating existing laws and regulations to address AI risks while promoting innovation","arguments":["Existing regulatory frameworks should be leveraged and updated rather than creating new AI-specific laws","A risk-based approach should be used to determine regulatory interventions","Government coordination and common principles are needed to ensure consistent AI governance"],"counterarguments":["Some potential AI risks may not be covered by existing regulations","New AI-specific regulations may be needed in some high-risk areas"],"key_recommendations":["Establish a government coordinating body for AI governance","Adopt a risk-based approach to AI regulation","Leverage existing regulatory frameworks where possible","Distinguish between AI developers and deployers in regulations","Government should be an exemplar in responsible AI use"],"risks_and_challenges":["Potential for AI bias and unfair discrimination","Lack of transparency and explainability in AI systems","Misuse of AI for surveillance or manipulation","Environmental harm from AI systems"],"safeguards_and_mitigations":["Transparency requirements across the AI lifecycle","Prohibition of certain high-risk AI use cases","Human agency and oversight for automated decision processes","Data governance practices for proper use of AI models"],"examples":{"Business AI applications":"SAP\'s intelligent intercompany reconciliation is an AI enabled process that helps retailers predict demand of when products will be sold, helping reduce inventories and waste across their stores.","Generative AI use case":"For example, the rapid generation of job role descriptions in our HR platforms.","Low-risk AI application":"An AI platform used by a business to optimise its inventory management system."},"international_alignment":"Supports global alignment through OECD and other international fora","values":["Human Agency and Oversight","Transparency and Explainability","Fairness and Non-discrimination"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Coordinate AI governance, be an exemplar in responsible AI use"},{"entity":"AI Developers","role":"Design and produce AI systems responsibly"},{"entity":"AI Deployers","role":"Use AI systems responsibly, ensure proper data governance"}],"sector_impacts":[{"sector":"Public sector","impact":"Improved efficiency and service delivery"},{"sector":"Retail","impact":"Better demand prediction and inventory management"}],"quotes":["At SAP, we believe that AI has great potential to create opportunities for businesses, governments, and societies. However, for people to trust AI solutions, the development, deployment, use, and sale of AI systems must be governed by clear moral rules.","Regulatory interventions should be scaled to meet the risk i.e. an assessment of the likelihood of harm against an individual or organisation combined with the severity of that outcome.","Governments have an important role in influencing public attitudes to AI. If governments use AI ethically and responsibly, this will build public trust and acceptance of the use of AI across the economy."]},"416_AI discussion paper input RAND.11fdb9d68396a.pdf":{"organization_name":"The RAND Corporation (Australia) Pty Ltd","organization_type":"Research organization","classification":"Proponent","overall_position":"Supports responsible AI practices with differentiated approaches for public and private sectors","arguments":["Different approaches should apply to public and private sector use of AI technologies","Generic solutions can be suitable for mitigating certain AI risks","Specific regulations are more valuable in areas where AI presents unique scenarios or higher risks"],"counterarguments":["Limiting the definition to task-based AI may place future technologies outside the purview of the discussion","Data localization laws can protect privacy but may limit the relevance of AI models for Australian populations"],"key_recommendations":["Implement more stringent measures for public sector AI use","Develop ethical codes of practice for private sector developers and users","Provide training for APS members on responsible AI use","Impose minimum standards for data security and employee training"],"risks_and_challenges":["Bias in AI training data leading to discriminatory outcomes","Automation bias leading to over-reliance on AI technologies","Potential misuse of generative AI for harmful purposes","Unintended and second order consequences in cyber and biological domains"],"safeguards_and_mitigations":["Implement the concept of Meaningful Human Control for public sector AI use","Mandate transparency requirements for critical AI applications","Develop common data quality measures for AI models and outputs","Provide ethics training for AI developers and users"],"examples":{"Robo-debt scandal":"While the robo-debt scandal is a particularly obvious example, others would include corporate use for targeted advertising of alcohol, informing patrol patterns for law enforcement, the denial of bail, or the division of education funding.","AI model for health conditions":"For example, an AI model used for identifying a specific health condition from biomarkers may not be effective for indigenous Australians because their records were not included in the training data.","AI-supported bail decisions":"For example, biases in the data used to train AI technology to support bail decisions makes it more likely that minorities are discriminated against, unless the magistrate is aware of, and knows how to recognise, that risk."},"international_alignment":"Supports coordination with key allies and partners","values":["Transparency","Accountability","Ethical use","Security","Privacy"],"tone":"Cautionary","stakeholders":[{"entity":"Government agencies","role":"Implement stringent measures for AI use and provide training"},{"entity":"Private sector","role":"Develop and follow ethical codes of practice"},{"entity":"Education providers","role":"Include ethics training in AI-related courses"},{"entity":"Department of Foreign Affairs and Trade","role":"Work with neighbouring states to spread ethical AI standards"}],"sector_impacts":[{"sector":"Public sector","impact":"More stringent regulations and oversight for AI use"},{"sector":"Private sector","impact":"Balance between enabling innovation and protecting against misuse"},{"sector":"Healthcare","impact":"Potential for biased outcomes if AI models don\'t represent all populations"},{"sector":"Law enforcement","impact":"Need for transparency in AI-informed decision making"}],"quotes":["Humans have a tendency to overly trust in technology once it diffuses and matures, this automation bias can lead trained and experienced humans to trust in the technology rather than their own judgement, amplifying the risk of unanticipated consequences, for example via malicious actor interference or training data bias.","Because bias in the data used to train AI systems can lead to AI outputs that reflect and even exacerbate those biases, it is important for users of AI for critical tasks to understand the data sources, the biases in those sources, and the extent to which those biases are reflected in the outputs.","Australia\'s capacity to generate exports of AI-enabled technologies should not prevent the Department from upholding the same ethical and technical standards that it would insist upon for domestically sold products and services."]},"288.pdf":{"organization_name":"The Royal Australian College of General Practitioners (RACGP)","organization_type":"Professional association","classification":"Proponent","overall_position":"Supports mandated regulation for responsible AI in healthcare","arguments":["AI has the potential to revolutionize the delivery of medicine","Regulation must keep pace with AI technologies to keep patients safe","GP involvement is crucial in ensuring AI technologies in healthcare are safe and fit-for-purpose"],"counterarguments":[],"key_recommendations":["A risk-based approach for responsible AI be mandated through regulation","An oversight body be established to develop a framework and oversee and monitor developments in healthcare AI","GPs must be included and involved in the development, implementation and regulation of AI technologies relevant to general practice"],"risks_and_challenges":["Unrepresentative datasets can introduce bias into AI decision-making","Potential for misdiagnosis and underdiagnosis","Exacerbation of health inequality","Privacy and security concerns with patient health data"],"safeguards_and_mitigations":["Use of large, high-quality datasets for AI training","GP involvement in risk identification and assessment","Cross-industry development of a framework for AI use in medical settings"],"examples":{"GP consultations":"As GPs see more than two million patients each week. With almost nine in ten people consulting a GP every year, GP involvement is crucial in ensuring AI technologies in healthcare are safe to use and fit-for-purpose."},"international_alignment":"null","values":["Patient safety","Privacy","Security"],"tone":"Cautionary","stakeholders":[{"entity":"General Practitioners","role":"Involved in development, implementation, and regulation of AI technologies in healthcare"},{"entity":"AI developers","role":"Subject to mandated regulation for responsible AI"},{"entity":"Government","role":"Establish regulatory framework and oversight body"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential revolution in medicine delivery, with risks to patient safety if not properly regulated"}],"quotes":["AI has the potential to revolutionise the delivery of medicine, and regulation must keep pace with these technologies to keep patients safe.","Simply entrusting industry to adhere to a voluntary code of ethics is not enough where there are potential gaps in existing legislation governing high-risk AI use.","The RACGP is supportive of a risk-based approach for responsible AI to be mandated through regulation, which should be applied to AI developers."]},"157_CSIRO Submission to Supporting Responsible AI Discussion Paper.1dc30235b2313.pdf":{"organization_name":"CSIRO","organization_type":"Australia\'s national science agency","classification":"Proponent","overall_position":"Supports responsible AI development through a principles-based approach and practical implementation strategies","arguments":["AI has enormous potential to improve society","Existing ethics frameworks can be updated to address AI-specific challenges","A balanced approach is needed to foster innovation while mitigating risks"],"counterarguments":["Over-regulation could hinder innovation and drive development offshore","Blocking all AI technologies is not a viable option"],"key_recommendations":["Develop industry best practices, playbooks, guidelines, and case studies","Create trustworthiness metrics, measurement, and testing methods","Establish a national sandbox platform for experimenting with responsible AI approaches","Set up connected responsible AI awareness and training programs","Create a national responsible AI technology program to inform policy and regulation","Identify responsible AI approaches that benefit all Australians"],"risks_and_challenges":["Bias and discrimination in AI systems","Privacy breaches and data misuse","Lack of transparency and explainability in AI decision-making","Job displacement due to automation"],"safeguards_and_mitigations":["Impact assessments for AI systems","Internal and external review processes","Risk assessments to classify AI system risk levels","Best practice guidelines and industry standards","Recourse mechanisms for individuals affected by AI decisions"],"examples":{"De-identified health data re-identification":"In 2016, a dataset that included de-identified health information was uploaded to data.gov.au. Unfortunately, it was discovered that in combination with other publicly available information, researchers were able to personally identify individuals from the data source.","Houston teachers fired by automated system":"An AI was used by the Houston school district to assess teacher performance and in some cases fire them. There was little transparency regarding the way that the AI was operating.","The COMPAS sentencing tool":"COMPAS is a tool used in the US to give recommendations to judges about whether prospective parolee will re-offend. There is extensive debate over the accuracy of the system and whether it is fair to African Americans."},"international_alignment":"Supports international coordination and standards","values":["Privacy protection","Fairness","Transparency","Accountability","Human rights"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop regulations and policies for responsible AI"},{"entity":"Industry","role":"Implement ethical AI practices and collaborate with academia"},{"entity":"Academia","role":"Conduct research and collaborate with industry on responsible AI"},{"entity":"Public","role":"Provide input and feedback on AI development and use"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved diagnostics and patient care"},{"sector":"Transportation","impact":"Development of autonomous vehicles and safety improvements"},{"sector":"Finance","impact":"Enhanced fraud detection and risk assessment"},{"sector":"Employment","impact":"Job displacement and need for reskilling"}],"quotes":["Existing ethics in context, not reinvented. Philosophers, academics, political leaders and ethicists have spent centuries developing ethical concepts, culminating in the human-rights based framework used in international and Australian law.","An ethics framework for AI is not about rewriting these laws or ethical principles, it is about updating them to ensure that existing laws and ethical principles can be applied in the context of new AI technologies.","With a proactive approach to the ethical development of AI, Australia can do more than just mitigate against risks\u2014if we can build AI for a fairer go, we can secure a competitive advantage as well as safeguard the rights of Australians."]},"101_Safe and Responsible AI in Australia Response - Jordan Taylor.7a6ee73c5b678.pdf":{"organization_name":null,"organization_type":null,"classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on existential risks from advanced AI systems","arguments":["Existential risks from AI should be a global priority alongside pandemics and nuclear war","Current regulatory approaches do not adequately address risks from large-scale, capable AI systems","Transparency in AI systems themselves is crucial, not just in organizations developing them"],"counterarguments":["Banning specific high-risk applications may not address the core risks of advanced AI systems","A focus on use cases alone may neglect risks inherent in the development of large-scale AI systems"],"key_recommendations":["Implement a ban on training large-scale AI systems with potential for catastrophic capabilities","Require dangerous capabilities evaluations and third-party auditing for frontier AI systems","Increase funding for technical AI safety research in Australia","Establish international cooperation on compute governance for large AI training runs"],"risks_and_challenges":["Existential risks from highly capable goal-directed AI systems with unintended goals","Inner misalignment and goal misgeneralization in general agentic systems","Lack of understanding of internal AI mechanisms and goals"],"safeguards_and_mitigations":["Mandatory third-party auditing and certification for safety of frontier AI systems","Regulation of computational power used in training large AI systems","Development of mechanistic interpretability tools for AI systems","Implementation of scalable oversight methods"],"examples":{"ARC\'s tests for autonomous replication capabilities":"such as the Alignment Research Center\'s tests for the capabilities required for autonomous replication","Illusion of interpretability in large AI systems":"However it is surprisingly easy to have the \\"illusion of interpretability\\" when explaining the behaviour of large AI systems, even when analysing the internal processes going on within them, as shown by Bolukbasi et al. (https://arxiv.org/abs/2104.07143)"},"international_alignment":"Supports global standards and international cooperation","values":["Safety","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement regulation and fund AI safety research"},{"entity":"AI developers","role":"Comply with safety evaluations and audits"},{"entity":"AI safety researchers","role":"Develop technical solutions for AI alignment and interpretability"}],"sector_impacts":[{"sector":"AI development","impact":"Increased focus on safety and alignment in large-scale AI systems"},{"sector":"Research","impact":"Increased funding and support for AI safety research"}],"quotes":["Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.","Australia must avoid the mistakes of previous AI regulation attempts, such as assuming that the biggest risks come only when applying AI to specific high-risk domains, neglecting even bigger risks inherent in developing extremely large, capable, agentic, and potentially misaligned AI systems themselves.","A ban should be imposed on the training of any large-scale AI systems with a nontrivial expected chance of passing capabilities evaluations required to cause catastrophic risks to civilization (such as the Alignment Research Center\'s tests for the capabilities required for autonomous replication https://evals.alignment.org/), until a consensus is reached among alignment researchers that AI systems at this level of capabilities can be made safe."]},"470_Submission 470 - Australian Information Security Association - 7-Aug.ab068e71e414b.pdf":{"organization_name":"Australian Information Security Association (AISA)","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a balanced approach of leveraging existing regulations and creating targeted guidelines for AI","arguments":["Existing regulations in certain sectors can be used to protect consumers from AI-related concerns","AI is rapidly evolving and some applications might not fit into existing frameworks","Collaborative co-regulation with industry as partners, led by the government is essential"],"counterarguments":["A standardised risk framework may not be sufficient to bridge the gaps and tackle the issues","AI\'s complexity and impact require multifaceted approaches"],"key_recommendations":["Collaborate with industry bodies, partners, and the public to enhance understanding of AI","Establish an AI standards body for evaluating AI-enabled technologies","Implement transparency requirements across the AI lifecycle","Adopt a holistic perspective considering intervention points across the entire AI supply chain"],"risks_and_challenges":["AI hallucination and generation of false information","Potential misuse of high-risk AI activities","Privacy concerns","Copyright infringement"],"safeguards_and_mitigations":["Provide users with an option to opt-out of AI usage","Review the appropriateness of AI implementation within the Australian context","Manage potential risks and biases in foundation models","Disclose country of origin for AI models","Ensure data used to train AI models is relevant to the Australian context"],"examples":{"Google engineers\' surprise at AI learning":"Google engineers were surprised when an experimental AI learned a language it was never trained for.","AI hallucination":"AI hallucination exemplifies one such outcome of this characteristic, where AI could make up false information or facts which aren\'t based on real data or events.","Food packaging labeling":"In the food packaging example in Australia, the five-star food health rating is useless compared to the nutritional information, ingredients list and country of origin information."},"international_alignment":"Supports alignment between public and private sectors, with consideration for Australian context","values":["Transparency","Trust","Ethical considerations","Accountability"],"tone":"Cautionary yet constructive","stakeholders":[{"entity":"Government","role":"Lead collaborative co-regulation with industry"},{"entity":"Industry","role":"Partner in developing and implementing responsible AI practices"},{"entity":"AI Standards Body","role":"Evaluate AI-enabled technologies for potential impact on public use"}],"sector_impacts":[{"sector":"Tech sector","impact":"Potential effects on trade and exports with other countries if high-risk activities are banned"},{"sector":"Small businesses","impact":"Need to ensure they are not left out in the regulatory process"}],"quotes":["AISA holds the view that the definition of Artificial Intelligence (AI) presented in this discussion paper is limiting in its scope.","Use of existing regulations in certain sectors to protect consumers can be a pragmatic approach to address AI-related concerns.","AISA believes that alignment between the public and private sectors regarding the use of AI technologies is of important. This alignment is critical to safeguard the interests of Australian citizens."]},"506_Submission 506 - Office of the Australian Information Commissioner - 18-Aug.e61aea6ea63f3.pdf":{"organization_name":"Office of the Australian Information Commissioner","organization_type":"Independent Commonwealth regulator","classification":"Proponent","overall_position":"Supports strengthening existing privacy framework to address AI risks","arguments":["Privacy Act provides a well-established framework to minimize privacy risks","Principles-based and technology neutral nature of Privacy Act allows for future-proofing","Existing framework can be enhanced to provide adequate safeguards for AI"],"counterarguments":["null"],"key_recommendations":["Establish a positive obligation on organizations to collect, use, and disclose personal information fairly and reasonably","Require all APP entities to complete Privacy Impact Assessments for high privacy risk activities","Enhance transparency measures and individual rights regarding personal information handling"],"risks_and_challenges":["AI technologies can amplify privacy risks due to their data-driven nature","Low levels of public trust and confidence in AI","Opaque and complex personal information handling in AI systems"],"safeguards_and_mitigations":["Enhance accountability measures for APP entities","Strengthen individuals\' control over their personal information","Enable more effective enforcement of privacy laws"],"examples":{"Financial institution using AI for automated decisions":"a financial institution using an AI model to automate a financial service application. If that AI model uses an applicant\'s personal information to infer other information about them, it may be unfair or unreasonable for that inferred information to be incorporated into the AI\'s decision-making process","AI system setting different prices":"an AI system using individuals\' personal information to set different prices in an advertisement for the same product may not be fair and reasonable"},"international_alignment":"Supports interoperable frameworks and consistently high global privacy standards","values":["Privacy","Transparency","Accountability"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"APP entities","role":"Comply with privacy obligations and handle personal information responsibly"},{"entity":"OAIC","role":"Regulate and enforce privacy laws, provide guidance"}],"sector_impacts":[{"sector":"Economy","impact":"Foster public trust and confidence in AI technologies"}],"quotes":["Good privacy practices that meet community expectations through compliance with the Privacy Act and the APPs will help to create the trust and confidence that is needed for the public to engage with AI technologies.","The OAIC views this proposal as a new keystone for the Privacy Act. It would require entities to proactively consider whether their personal information handling activities are appropriate and set a baseline standard of information handling that is flexible and adaptable as circumstances and technology change.","Personal information flows across national borders, so having a robust data protection framework, which is interoperable across international regulatory regimes, will be key to mitigating the privacy impacts of AI technologies."]},"424.pdf":{"organization_name":"Equifax","organization_type":"Data, analytics and technology company","classification":"Proponent","overall_position":"Supports a coordinated, risk-based approach to AI regulation with focus on transparency and avoiding duplication","arguments":["Existing data regimes should be considered to avoid overlap and contradiction","AI governance should apply equally to private sector and government","Risk-based approach should balance potential harms with social and economic benefits of AI"],"counterarguments":["Legislation may not keep pace with rapidly evolving AI field","Detailed AI output explanations may create intellectual property risks","Compliance requirements may create barriers for smaller organizations"],"key_recommendations":["Commonwealth and States should agree on a single consistent approach to AI regulation","Consider a supervisory agency to continuously monitor AI developments","Include \'AI in the loop assessments\' prior to \'human in the loop\' in risk assessment framework"],"risks_and_challenges":["Potential for overlap, confusion or contradiction with existing data regimes","Rapid evolution of AI field making it difficult for legislation to keep pace","Ambiguity in defining risk levels for AI systems"],"safeguards_and_mitigations":["Implementing clear processes for AI disclosures to consumers","Using explainable AI techniques like NeuroDecision\xae Technology","Continuous monitoring and review of AI developments by a regulatory agency"],"examples":{"Credit reporting transparency":"Equifax provides consumers with four key contributing factors, in detail, tailored to the person and setting out steps the individual can take to improve their credit risk standing.","NeuroDecision\xae Technology":"Equifax\'s NeuroDecision\xae Technology is the first patented credit scoring system to use explainable AI. It uses a neural network algorithm to predict the probability that a consumer will default on a credit product."},"international_alignment":"null","values":["Transparency","Trust","Explainability"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop consistent approach to AI regulation"},{"entity":"Private sector","role":"Implement transparent and explainable AI systems"},{"entity":"Consumers","role":"Understand and act on AI-derived information"}],"sector_impacts":[{"sector":"Financial services","impact":"Improved credit risk assessment and decision-making"}],"quotes":["Building trust is dependent on lifting AI literacy, educating people on AI benefits and risks, and putting in place clear processes for consumers wanting further disclosures about the AI involved in a transaction.","Because NeuroDecision\xae Technology is fully interpretable, lenders can realise the benefits of AI without introducing a \'black box\' algorithm that could reduce their control of credit risk.","The Commonwealth and States should agree on a single consistent approach to AI regulation, working together on common objectives, standards and practices."]},"134_Submission 134 - Attachment.9b7978ec4426.pdf":{"organization_name":null,"organization_type":"Academic","classification":"Proponent","overall_position":"Supports a multi-pronged approach including updating existing laws, introducing new AI-specific regulations, and developing principles and standards","arguments":["Risk-based approach to regulating AI is supported in principle","Existing legislation needs to adequately encompass AI-based decisions and ADM","A multi-pronged approach can address the complexities of AI regulation"],"counterarguments":["New regulation focused solely on AI would likely be insufficient on its own","Prescriptive definitions of AI risk becoming outdated by emerging technologies"],"key_recommendations":["Develop voluntary standards for AI developers and vendors","Implement a risk-based regulatory framework","Review and update existing workplace-related regulations to encompass AI and ADM"],"risks_and_challenges":["Bias and discrimination in AI-enabled applications for hiring and employee evaluation","Unfair treatment of workers, potentially erroneous or unfair dismissals, and indirect discrimination","Tensions between commercial-in-confidence information and transparent decision-making"],"safeguards_and_mitigations":["Close working relationship between developers and employing organisations to mitigate potential bias","Proactive audits of AI in use","Complaint mechanisms for both consumers/general public and employees"],"examples":{"ChatGPT variability":"ChatGPT provides a case example of variability in outputs but also the innate difficulty in understanding the basis on which outputs were derived in an open-sourced AI.","Fair Conduct and Accountability Standards for digital labour platforms":"For example, the Fair Conduct and Accountability Standards for digital labour platforms adopted by the Victorian Government specifies practices that digital labour platforms are encouraged to adopt."},"international_alignment":"Supports consideration of international approaches, such as the EU model","values":["Transparency","Fairness","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"AI developers","role":"Responsible for developing unbiased AI systems"},{"entity":"Organisations using AI","role":"Responsible for ethical implementation and monitoring of AI systems"},{"entity":"Regulatory bodies","role":"Enforce regulations and hold AI developers and users accountable"}],"sector_impacts":[{"sector":"Employment","impact":"AI deployment in hiring, employee evaluation, and workforce management"},{"sector":"Public sector","impact":"Need for policies, training, and education on responsible AI use"}],"quotes":["Any regulation (or guidelines or standards) would need to account for the role of all actors in the use of AI.","Alongside a risk-based regulatory framework, Voluntary Standards for AI developers and vendors could provide a useful non-regulatory framework to guide global AI companies on expected practice in Australia, without stifling innovation.","Initiatives should not be framed as or aimed at improving public trust but rather initiatives should be aimed at addressing the issues of lack of transparency in AI/ADM, and the currently limited avenues for redress or support when an individual is negatively affected by AI/ADM."]},"338_AAS submission - DISR AI.a1c0d37a6d4b2.pdf":{"organization_name":"Australian Academy of Science","organization_type":"Scientific organization","classification":"Proponent","overall_position":"Supports responsible AI development with government leadership and guidelines","arguments":["AI can accelerate discoveries and boost economic growth","Investment in AI is essential to safeguard the future of Australian research and sovereign capability","AI could significantly boost economic productivity"],"counterarguments":["Waiting to observe the impact of AI on science is not a viable option","Without guidance, there is a risk of misuse of AI tools"],"key_recommendations":["Develop a national strategy and guidelines for responsible use of AI in research and development","Support sovereign high-performance computing facilities","Establish an open science strategy to prepare Australia for uptake of AI"],"risks_and_challenges":["Loss of sovereign research and development capacity","Misuse of AI tools in research","Misinformation and trust in science","Artificially generated content misused in democratic processes"],"safeguards_and_mitigations":["Clear guidelines for researchers on using AI tools","Engagement of researchers in developing guidelines","Use of AI to detect harmful or misleading content"],"examples":{"ARC grants incident":"An incident involving peer reviewers for Australian Research Council grants using ChatGPT to evaluate research proposals resulted in the prohibition of generative AI in the ARC\'s grants programs.","AI in drug discovery":"Tasks that have previously taken researchers years to complete, such as searching for drug targets or analysing patterns in neural signals, could be performed using AI tools in a matter of hours.","AI in multidisciplinary research":"CSIRO\'s Artificial Intelligence for Science report showed that AI is increasingly moving beyond the field of computer science and into other research disciplines."},"international_alignment":"Supports global standards with national implementation","values":["Responsible use of AI","Sovereign capability","Open science"],"tone":"Cautionary but optimistic","stakeholders":[{"entity":"Government","role":"Develop national strategy and guidelines, support infrastructure"},{"entity":"Researchers","role":"Engage in development of guidelines, collaborate across disciplines"},{"entity":"Higher degree by research students","role":"Prepare for AI integration in research"}],"sector_impacts":[{"sector":"Science and Research","impact":"Accelerated discoveries and increased productivity"},{"sector":"Economy","impact":"Significant boost to productivity and economic growth"}],"quotes":["Investment in AI is not optional; it is essential to safeguard the future of Australian research and sovereign capability.","Without guidance on how researchers should use AI tools, such as generative AI tool ChatGPT, there is a risk of misuse.","Australia needs an open science strategy and to evaluate how to meet the UNESCO recommendation on Open Science."]},"189_Reset.Tech response to AI discussion paper.04c96698e0616.pdf":{"organization_name":"Reset.Tech Australia","organization_type":"Non-partisan policy initiative","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI with a focus on safety and responsibility","arguments":["Voluntary and co-regulatory models are ineffective for digital regulation","Clear, unambiguous regulation harmonized with major markets is necessary for industry growth","Regulation is pro-innovation and provides certainty for businesses"],"counterarguments":["The paper suggests regulations stifle or inhibit innovation","The assumption that self/co-regulation does not pose a burden on industry"],"key_recommendations":["Avoid voluntary and co-regulatory models in developing the regulatory regime","Implement a risk-based approach to AI regulation with clear criteria and regulatory oversight","Embrace the precautionary principle in AI regulation","Pay particular attention to data provenance and consumer choice","Ensure children and young people\'s best interests are considered in the regulatory regime"],"risks_and_challenges":["AI poses potential harmful uses, inaccuracies, bias, privacy violations, at-scale risks","Issues with accountability, transparency, deception, and lack of consent","Ownership issues related to AI"],"safeguards_and_mitigations":["Implement risk-based approaches with clear criteria for risk designation","Ensure transparency across the AI lifecycle","Provide meaningful consumer choice and consent options","Implement continuous risk and impact assessments"],"examples":{"Online Safety Codes failure":"The eSafety Commissioner initially rejected the Codes developed by industry, because they did not provide adequate community safeguards.","Disinformation and Misinformation Code limitations":"To date, only eight platforms have signed up, and many digital platforms that have known issues of mis and disinformation uncovered.","UK\'s AI algorithm failure":"In 2020 in the UK, a \'mutant algorithm\' converted high school grades into university entrance criteria, in a way that systematically downgraded the scores of children from low-income areas, harming low-income children"},"international_alignment":"Supports alignment with global best practices, particularly EU and UK approaches","values":["Transparency","Accountability","Safety","Privacy","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Regulators","role":"Draft and enforce clear, unambiguous regulations"},{"entity":"Industry","role":"Comply with regulations and implement ethical AI practices"},{"entity":"Consumers","role":"Exercise meaningful choice and consent"}],"sector_impacts":[{"sector":"Technology","impact":"Clear regulatory guidelines for AI development and implementation"},{"sector":"Children and Youth Services","impact":"Enhanced protection and consideration of best interests in AI applications"}],"quotes":["Voluntary guidance and co-regulatory approaches do not work, and cannot be considered as an essential part of the regulatory mix required to maximise the opportunities from, and minimise the risks from, AI.","Far from being an impediment to innovation, effective, future-proof regulation will provide companies and developers with the space to experiment and take risks without being hampered by concerns about legal, reputational or ethical exposure.","Data is often needed en masse to train LLMs, ADMs and MfMs. This raises issues around privacy and consent, and fairness and reasonableness where data is used in ways that the public might not expect."]},"485_Submission 485 - Woolworths Group - 11-Aug.5c4cca18dba11.pdf":{"organization_name":"Woolworths Group Limited","organization_type":"Retail company","classification":"Proponent","overall_position":"Supports a risk-based, technology-neutral approach to AI regulation that updates existing laws and fosters innovation","arguments":["AI regulation should be technology-neutral and take a risk-based approach","Regulation should support the continued growth of a safe and responsible AI industry in Australia","Restrictions should be based on high risk activities, rather than individual technologies or methods"],"counterarguments":["null"],"key_recommendations":["Establish a guide of definitions for key terms relating to the regulation of AI","Establish a regulatory sandbox to test concepts under relaxed regulatory requirements","Establish a central forum for advice and policy coordination for AI regulation","Prioritise multinational engagement and coordination on AI","Require APP entities to conduct PIAs for activities with high risk"],"risks_and_challenges":["Fractured policy framework across States and Territories","Hindering innovation and competitiveness of Australian-developed AI solutions in overseas markets","Disclosure of commercially sensitive information"],"safeguards_and_mitigations":["Conduct privacy impact assessments (PIA) for high risk activities","Implement measures to mitigate identified risks","Provide transparency to individuals about how personal information could be used to make inferences"],"examples":{"Use of AI in retail":"We are already leveraging AI in a number of ways.","AI in data science and analytics":"We also have a majority interest in Quantium, a global leader in data science, artificial intelligence and advanced analytics.","AI in customer experience":"In 2021, we created a new combined entity with Quantium, known as wiq, to develop data and analytics led solutions to enhance retail experiences for our customers."},"international_alignment":"Supports global standards and multinational engagement","values":["Transparency","Privacy","Security","Responsible and ethical use","Innovation"],"tone":"Positive and constructive","stakeholders":[{"entity":"Federal Government","role":"Lead work through a newly established central office or advisory on AI technology"},{"entity":"Businesses","role":"Implement responsible AI practices and conduct risk assessments"},{"entity":"Regulators","role":"Develop a modernised and consistent approach to AI regulation"}],"sector_impacts":[{"sector":"Retail","impact":"Enhanced customer experiences and operational efficiency"},{"sector":"Economy","impact":"Potential $24 billion boost to the Australian economy by 2030 through Emerging Tech exports"}],"quotes":["As Australia\'s most trusted brand, our approach is to build and adopt robust data and AI ethics procedures, practices and technology controls that grow trust.","AI does not operate in a void, hence regulation should be technology-neutral and take a risk-based approach, which is already seen in privacy regulation.","Australian regulations that unduly constrain local companies relative to those in other markets could hinder innovation in Australia and the competitiveness of Australian-developed AI solutions in overseas markets."]},"505_Submission 505 - Canva - 18-Aug.ef9f2be867c48.pdf":{"organization_name":null,"organization_type":null,"classification":null,"overall_position":null,"arguments":[null],"counterarguments":[null],"key_recommendations":[null],"risks_and_challenges":[null],"safeguards_and_mitigations":[null],"examples":{"example":null},"international_alignment":null,"values":[null],"tone":null,"stakeholders":[{"entity":null,"role":null}],"sector_impacts":[{"sector":null,"impact":null}],"quotes":[null]},"429_PEXA AI Government Submission (Signed).3f10b40e0aa26.pdf":{"organization_name":"Property Exchange Australia Ltd (PEXA)","organization_type":"Digital exchange and data insights technology business","classification":"Proponent","overall_position":"Supports responsible AI adoption with a balanced approach to regulation","arguments":["AI has enormous potential to transform society and industries","Striking the right balance between innovation and accountability will foster public trust in AI","A risk-based approach is needed for addressing potential AI risks"],"counterarguments":["Despite the immense benefits AI promises, it is integral to recognise and actively mitigate the associated risks","The rapid advancement of AI, particularly breakthroughs in general intelligence, requires careful consideration"],"key_recommendations":["Implement a coordinating regulator agency to handle the scale and complexity of AI","Adopt co-regulation with industry, led by the government","Use regulatory sandboxes to test and move faster","Educate decision-makers and the public about AI","Engage internationally and harmonize approaches"],"risks_and_challenges":["Inaccuracy and hallucinations in AI-generated content","Copyright infringement","Privacy concerns","Misinformation / disinformation","Sovereign based cyber security concerns","Lack of explainability in AI decision-making"],"safeguards_and_mitigations":["Mandatory interrogable output for AI-generated content","Integration of ethical considerations, potentially through an AI trust mark","Alignment with International Standards of Risk Management ISO3100 principles","Incorporation of AI risk assessment into existing frameworks like Privacy"],"examples":{"Value Australia project":"Value Australia uses state of the art AI algorithms and machine learning to replicate market conditions for each property in Australia.","PEXA\'s role in elections":"The New South Wales Electoral Commission has suggested PEXA is a good model to produce a \'common national election technology system.\'"},"international_alignment":"Supports international engagement and harmonisation","values":["Transparency","Accountability","Trust","Innovation","Ethical use of data"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Lead co-regulation with industry and implement regulatory measures"},{"entity":"Industry","role":"Collaborate in co-regulation and implement responsible AI practices"},{"entity":"PEXA Ethical Advisory Council","role":"Guide the business on ethical product development and data use"}],"sector_impacts":[{"sector":"Property","impact":"Transformation of real estate transactions and valuations"},{"sector":"Government","impact":"Improved decision-making in infrastructure investment and tax collection"}],"quotes":["PEXA firmly believes that striking the right balance between innovation and accountability will foster public trust in AI and enable Australia to fully harness the benefits these transformative technologies offer.","Safe and responsible AI in the Australian context involves several key considerations. Firstly, AI accountability, consumer understanding, and trust are vital, along with recognising the country of origin for AI technology and data training the models specifically for the Australian context.","PEXA is supportive of a risk-based approach for addressing potential AI risks, with differing regimes for Government and Non-Government entities. PEXA is supportive of AI laws being principles based, along with a need to have \'catch all\' provisions on the intent when creating AI capabilities."]},"254.pdf":{"organization_name":"UNICEF Australia","organization_type":"Non-governmental organization","classification":"Proponent","overall_position":"Advocates for comprehensive child-centered AI regulation","arguments":["AI systems will significantly impact children\'s lives in ways we don\'t yet understand","Current AI policies and frameworks do not adequately address children\'s needs and rights","Child-centered AI can protect children, provide for their needs, and empower their participation"],"counterarguments":["null"],"key_recommendations":["Develop, deploy and govern AI systems in line with UNICEF\'s guidance for child-centered AI","Apply child-centered AI requirements to all AI systems that interact with or impact children","Develop AI systems that simultaneously uphold children\'s rights to protection, provision and participation","Foster multi-stakeholder approaches to AI development and deployment"],"risks_and_challenges":["Widespread exclusion and discrimination due to biased AI systems","Privacy risks for children who may not understand the implications of sharing information","Potential for AI-assisted cyber security breaches putting children at risk of extortion"],"safeguards_and_mitigations":["Develop practical tools for AI creators, policymakers, parents, and children","Ensure AI systems meet the nine requirements for child-centered AI","Use AI to detect known child sexual abuse material"],"examples":{"AI in education":"AI-enabled learning tools which help children develop critical thinking skills, to adaptive learning programs which provide experiences tailored to the needs of a specific student, to AI-generated curricula which save educators time and allow them to focus on other supports for students.","UNICEF\'s use of AI":"UNICEF already leverages AI systems to predict the spread of diseases, map digital connectivity in schools, and produce better poverty estimation, thus improving our programming."},"international_alignment":"Supports global standards with national implementation","values":["Protection","Equity","Participation"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement child-centered AI policies"},{"entity":"Industry","role":"Develop and deploy AI systems in line with child-centered AI guidance"},{"entity":"Children","role":"Participate in shaping AI systems and make informed decisions about AI use"}],"sector_impacts":[{"sector":"Education","impact":"Improved educational outcomes through AI-enabled learning tools and adaptive learning programs"},{"sector":"Healthcare","impact":"Improved access to life-saving healthcare for children"}],"quotes":["Today\'s children are the first generation that will never know a world without smartphones.","AI is undoubtedly a force for innovation which can likely help us achieve the Sustainable Development Goals, but it also poses risks to children\'s safety, privacy, and security.","We have an opportunity now to guide the development and deployment of AI systems which work in the best interests of every Australian, and especially for some of our most vulnerable in children."]},"37.pdf":{"organization_name":"null","organization_type":"Individual academic","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a risk-based approach","arguments":["A risk-based approach allows for targeted mitigation and efficient resource allocation","Comprehensive regulation is necessary to address AI-specific challenges and risks","Transparency and accountability are vital for building public trust and confidence in AI systems"],"counterarguments":["Overly restrictive regulations may hinder innovation and economic growth","A purely horizontal approach to regulation may be insufficient for addressing specific AI risks","Voluntary self-regulation alone may not ensure consistent standards across organizations"],"key_recommendations":["Implement a risk-based approach for addressing AI risks","Establish clear guidelines and standards for AI implementation","Mandate transparency requirements across public and private sectors","Develop mechanisms for independent auditing and external peer review","Foster collaboration between government, industry, and academia"],"risks_and_challenges":["Potential for biased or discriminatory outcomes in automated decision-making systems","Privacy concerns related to data collection and use in AI systems","Lack of transparency and explainability in AI decision-making processes","Cybersecurity risks associated with AI technologies"],"safeguards_and_mitigations":["Conduct comprehensive risk assessments and impact assessments","Implement robust data governance frameworks and privacy protection measures","Establish mechanisms for ongoing monitoring and evaluation of AI systems","Develop ethical guidelines and standards for AI development and deployment","Promote AI literacy and education across different sectors"],"examples":{"EU\'s GDPR as a model for data protection":"The General Data Protection Regulation (GDPR) in the European Union (EU) provides a relevant case study of a risk-based approach in AI governance.","FDA\'s risk-based approach to medical devices":"The FDA\'s risk-based approach in regulating medical devices provides a relevant case study.","OpenAI\'s GPT model demonstrating risks":"OpenAI\'s GPT model, a widely known LLM, demonstrated risks of generating biased or misleading content, potentially influencing public opinion, or reinforcing stereotypes."},"international_alignment":"Supports international collaboration and harmonization of AI governance efforts","values":["Transparency","Accountability","Fairness","Privacy","Security","Ethical considerations"],"tone":"Cautionary but optimistic","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, provide guidance and support"},{"entity":"Industry","role":"Implement responsible AI practices, collaborate in developing standards"},{"entity":"Academia","role":"Conduct research, provide expertise, and contribute to policy development"},{"entity":"Civil society","role":"Provide input, advocate for public interests, and participate in decision-making processes"}],"sector_impacts":[{"sector":"Healthcare","impact":"Enhanced diagnostic accuracy and patient care, but requires careful consideration of privacy and ethical implications"},{"sector":"Finance","impact":"Improved risk assessment and fraud detection, but potential for bias in automated decision-making"},{"sector":"Transportation","impact":"Advancements in autonomous vehicles, requiring specific safety and liability considerations"}],"quotes":["A risk-based approach offers several advantages in the regulation of AI applications, enabling targeted mitigation, flexibility, and adaptability, and promoting industry innovation and growth.","Transparency plays a crucial role in mitigating potential risks associated with AI and in building public trust and confidence in AI systems.","By addressing the key considerations and recommendations outlined in this submission, the Australian government can lay the foundation for a robust and comprehensive regulatory framework that promotes safe and responsible AI in our country."]},"413_CTS Submission to DISR Responsible AI_08042023.6b0d7e759cd6e.pdf":{"organization_name":"Australian Strategic Policy Institute","organization_type":"Independent, non-partisan think tank","classification":"Proponent","overall_position":"Supports a balanced approach to AI regulation, favoring targeted legislation for high-risk activities and non-regulatory initiatives for innovation","arguments":["AI is a dual-use technology with both civil and military applications","Australia\'s smaller economy requires a different regulatory approach than the EU","Heavy-handed regulation could stifle innovation and make Australian businesses less competitive"],"counterarguments":["Some AI activities pose significant risks and require specific legislation","Existing laws may not be sufficient to address the speed and scale of harm AI can cause"],"key_recommendations":["Recognize AI as a dual-use technology","Regularly update the definition of AI","Develop a risk framework based on likelihood and consequence","Ban specific harmful AI activities through legislation","Implement non-regulatory initiatives like sandboxing and a \'Responsible AI council\'","Conduct technical tests on high-risk algorithms used in the public sector","Engage in \'techdiplomacy\' with Indo-Pacific partners"],"risks_and_challenges":["Cybersecurity threats","Risks to critical infrastructure","Weapons proliferation and terrorism","Social cohesion issues","Complex technical failures in AI systems"],"safeguards_and_mitigations":["Regulatory sandboxing to encourage innovation","Establishing a \'Responsible AI council\'","Funding for AI safety research","Technical testing and public reporting on high-risk algorithms"],"examples":{"Dual-use AI applications":"Chatbots based on large language models can be used for writing editors or language learning (civil use) or for state-backed hacking campaigns or information operations (military use)","High-risk AI activities":"Proliferation of AI-generated child sex abuse material (CSAM), production of deep fake pornography or impersonation of political figures","High potential harm but public interest activities":"Personalised medicine and drug discovery, autonomous vehicles and self-driving cars"},"international_alignment":"Supports coordination with like-minded partners in the Indo-Pacific on AI governance initiatives","values":["Innovation","Transparency","Safety","Responsible development"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"DISR","role":"Lead development of risk framework and coordinate with other government departments"},{"entity":"National Measurement Institute","role":"Conduct technical tests on high-risk algorithms"},{"entity":"Academic institutions","role":"Collaborate on AI safety research"}],"sector_impacts":[{"sector":"Small and medium businesses","impact":"Potential reduced competitiveness if heavy-handed regulation is implemented"},{"sector":"Military","impact":"Potential reduced competitiveness if AI development is overly restricted"}],"quotes":["Failing to recognise AI as dual-use could mean policy and legislation would fail to consider serious national security risks.","CTS recommends that an Australian approach to responsible and safe AI avoid emulating after the European Union\'s AI act and a risk-based approach.","Only the most harmful activity should be regulated through specific legislation. Other kinds of activity should be regulated through existing legislation in Australia -- e.g., Privacy Act 1988, Competition and Consumer Act 2010, Online Safety Act 2021 -- and the use of sector-specific non-regulatory instruments."]},"231_230726 - BSA Comments on Safe and Responsible AI Discussion Paper.2120e51b348dc.pdf":{"organization_name":"BSA | The Software Alliance","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a risk-based approach to AI governance with focus on high-risk AI systems","arguments":["Risk management is necessary because bias and fairness in AI are contextual","Impact assessments are an important accountability mechanism for high-risk AI systems","Effective AI governance requires distinguishing between AI developers and AI deployers"],"counterarguments":[],"key_recommendations":["Stay abreast of international efforts to define key AI terms","Define and distinguish AI developers and AI deployers","Focus regulatory efforts on high-risk AI use cases","Use impact assessments for high-risk AI systems","Draw on existing risk management frameworks","Conduct impact assessments throughout the AI lifecycle to reduce bias"],"risks_and_challenges":["AI bias and fairness issues","Lack of transparency in AI systems","Privacy concerns related to AI and automated decision-making"],"safeguards_and_mitigations":["Implementing comprehensive risk management processes","Conducting impact assessments throughout the AI lifecycle","Ensuring diverse teams in AI development and oversight","Establishing clear governance frameworks and accountability measures"],"examples":{"Medical school admissions bias":"A medical school in the United Kingdom set out to create a system that would help identify good candidates for admission. The system was trained using data about previously admitted students. It was discovered, however, that the school\'s historical admissions decisions had systematically disfavored racial minorities and females whose credentials were otherwise equal to other applicants.","Facial recognition bias":"As the pathbreaking research by Joy Buolamwini and Timnit Gebru demonstrated, facial recognition systems trained on datasets composed disproportionately of white and male faces perform substantially less accurately when evaluating the faces of women with darker complexions.","Healthcare triage bias":"In 2019 researchers discovered that an AI system widely used by hospitals to triage patients by predicting the likelihood that they required urgent care systematically prioritized the needs of healthier white patients to the detriment of less-healthy minority patients."},"international_alignment":"Supports international alignment and interoperability of AI definitions and standards","values":["Fairness","Transparency","Accountability","Privacy"],"tone":"Neutral","stakeholders":[{"entity":"AI Developers","role":"Design and develop AI systems, conduct impact assessments, provide documentation"},{"entity":"AI Deployers","role":"Adopt and use AI systems, conduct impact assessments, monitor system performance"},{"entity":"Government","role":"Develop AI policies and regulations"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved diagnosis and patient care, but potential for bias in triage systems"},{"sector":"Finance","impact":"Potential for improved access to credit, but risks of bias in loan approval systems"}],"quotes":["BSA strongly supports the use of impact assessments to mitigate risks arising from high-risk uses of AI systems.","Effective risk management is anchored around a governance framework that promotes collaboration between an organization\'s development team and its compliance personnel at key points during the design, development, and deployment of a product.","Reflecting the inherently dynamic nature of AI systems, policies pertaining to AI must account for the array of stakeholders that may play a role in various aspects of a system\'s design, development, and deployment."]},"473_Submission 473 - Australian Retailers Association - 8-Aug.fac1f5398b9c5.pdf":{"organization_name":"Australian Retailers Association","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with industry-specific considerations","arguments":["AI has significantly transformed various aspects of the retail sector","A flexible and informed approach is needed to govern responsible AI use in retail","A risk-based approach allows for targeted regulation and efficient resource allocation"],"counterarguments":["Overregulation could impede operations, service, and innovation in the sector","Generic solutions may not address unique risks in retail AI applications"],"key_recommendations":["Establish consistent national definitions based on international standards","Implement non-regulatory initiatives to support responsible AI practices","Create a central office within government to coordinate AI policy","Develop industry-specific best practice frameworks and voluntary codes"],"risks_and_challenges":["Cross-border data sharing and jurisdictional differences","Blackbox AI systems with non-explainable decision-making processes","Ethical use of AI in customer profiling"],"safeguards_and_mitigations":["AI impact assessments for high-risk applications","Voluntary AI standards and codes of conduct","Ethical AI certification programs","Public-Private Partnerships for knowledge sharing"],"examples":{"AI in product development":"AI and ML technologies have enabled retailers to gain valuable insights into customer preferences, market trends, and competitor strategies.","AI in supply chain management":"AI-powered demand forecasting and supply chain optimisation solutions have revolutionised supply chain management for retailers.","AI in customer experience":"AI algorithms analyse customer data to understand individual preferences and shopping behaviours."},"international_alignment":"Supports consideration of regulatory approaches adopted by international partners","values":["Flexibility","Transparency","Innovation"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Establish central office for AI policy coordination"},{"entity":"Industry","role":"Collaborate on sector-specific guidelines and best practices"},{"entity":"Small Businesses","role":"Require more support in addressing AI risk assessment"}],"sector_impacts":[{"sector":"Retail","impact":"Improved efficiency, personalization, and customer satisfaction"},{"sector":"Economy","impact":"Stimulates consumer spending and drives business growth"}],"quotes":["The ARA supports a risk-based approach to AI regulation and recommends ongoing consultation with industry and community groups.","A risk-based approach is best suited to navigate the dynamic nature of AI technologies and their application.","Embracing AI and ML technologies is essential to maintain competitiveness, stimulate consumer spending, and drive business growth."]},"328_Submission 328 - Attachment.2a6e2a5f3886b.pdf":{"organization_name":"Trusted Autonomous Systems","organization_type":"Government-funded research organization","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and responsible AI development","arguments":["Australia needs a multi-faceted approach to AI governance","AI is a true dual-use technology requiring coordinated efforts across civilian, military, and security branches","Existing initiatives need to be expanded and new organizations created for effective AI governance"],"counterarguments":["Relying solely on existing domain-specific regulators may lead to a lack of internal expertise","A singular authority, either scientific or regulatory, may not be sufficient for AI governance"],"key_recommendations":["Create a new AI Safety Authority with an AI Safety Commissioner","Establish AustAI, a new Australian organization providing guidance and services similar to RAS-Gateway","Expand NATA to provide accreditation for AI and autonomous systems test and evaluation facilities","Implement the Responsible AI in Defence (RAID) Toolkit across various sectors","Adopt the recommendations of IEEE7001 Transparency Standard for AI explanations"],"risks_and_challenges":["AI as a ubiquitous dual-use technology with potential for misuse","Lack of human-centricity in AI development and implementation","Complex and cascading risks associated with general purpose AI"],"safeguards_and_mitigations":["Develop a digital suite of information and tools based on TAS RAS-Gateway","Create an Australian LLM from materials in Trove for educational purposes","Implement the Australian AI Ethics Framework within government procurement processes","Adopt a risk framework suitable for complex AI risks"],"examples":{"Robodebt case":"The greatest harms to Australians from an automated decision technology to date, Robodebt, came from a lack of human-centricity and responsibility rather than technical incompetence.","Consunet\'s DUST project":"South Australian research and development company, Consunet Pty Ltd, has successfully completed its Next Generation Technologies Fund (NGTF) project \u2013 Distributed aUtonomous Spectrum Management (DUST) \u2013 commenced through Trusted Autonomous Systems in 2019.","NTC\'s Automated driving system entities regulatory framework":"The new Automated driving system entities regulatory framework by the NTC is a great example of regulatory reform in action within a domain"},"international_alignment":"Supports alignment with international best practices while maintaining an Australian focus","values":["Responsibility","Transparency","Human-centricity"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government departments","role":"Implement AI Ethics Framework and lead by example"},{"entity":"Industry","role":"Collaborate with government on responsible AI development"},{"entity":"Regulators","role":"Develop and enforce AI regulations in specific domains"}],"sector_impacts":[{"sector":"Military","impact":"Development of responsible AI for defence applications"},{"sector":"Telecommunications","impact":"AI-driven spectrum management and allocation"}],"quotes":["The government should reread the TAS submission to the AI Action Plan\u2014see Box 1.4 as its recommendations still apply.","The government needs a process to regulate, influence and (in some cases) control the research, production, and use of AI across all domains.","Australia\'s AI research body (whether Data61/CSIRO or not) needs to employ more human factors researchers, more scholars from the humanities including linguists, anthropologists, philosophers, psychologists, sociologists, social work, law, justice, communication, human-centred computing, and neuroscience."]},"358_Source Transitions response to Safe and responsible AI in Australia discussion paper_updated.2824dd71298f9.pdf":{"organization_name":"Source Transitions Pty Ltd","organization_type":"Management consulting company","classification":"Proponent","overall_position":"Supports comprehensive regulation of AI with adaptive governance models","arguments":["Legislation is warranted given the risks AI introduces into society","Clear intent and direction for AI use in Australia is needed","Environmental and social costs of AI should be considered in regulation"],"counterarguments":["Leaving AI literacy to the private sector would be detrimental to the public, small business and not for profit sector","Major technology companies exert significant influence over the legislative process through lobbying"],"key_recommendations":["Mandate that AI systems are environmentally-friendly","Regulate digital and green transitions together","Adopt a taxonomy of harms to help better understand and tailor the legislation to each type of risk","Provide responsible AI tools, practices and resources freely to the public","Sponsor AI literacy programmes across sectors"],"risks_and_challenges":["Environmental costs of developing AI solutions or training large datasets","AI used by large entities to remove competition from smaller organizations","False advertisement and wildest claims made about AI systems and their abilities","Immersive tech and metaverse generating new types of challenges by enabling more control of human behavior"],"safeguards_and_mitigations":["Governing AI systems through their design where social, legal, ethical rules can be enforced through code","Human in the loop and society in the loop approaches to governing AI","Provide sandbox development environments where governance and transparency processes are evaluated","Strengthen legislation around AI-washing"],"examples":{"Environmental cost of AI training":"The environmental cost of training a large natural language processing model could be as much as the total emissions produced by five cars over the cars\' lifetime","Human cost of AI training":"The human cost of training models, often akin to modern slavery"},"international_alignment":"Supports adoption of EU-style regulations like GDPR and EU Data Act in Australia","values":["Transparency","Sustainability","Inclusivity","Accountability"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, provide infrastructure and resources for responsible AI"},{"entity":"Private sector","role":"Implement responsible AI practices, collaborate on real-life projects"},{"entity":"Academia","role":"Research and educate on responsible AI, collaborate on real-life projects"}],"sector_impacts":[{"sector":"Education","impact":"Need for AI ethics and responsible AI to be taught in schools and universities"},{"sector":"Technology","impact":"Potential limitations on high-risk AI applications like social scoring and facial recognition"}],"quotes":["Source Transitions shares the Australian Government\'s view that legislating AI technologies development and applications is warranted given the risks they introduced into society.","AI literacy is a society-wide area that will need to be driven both by governments and by the private sector.","Systems thinking, transdisciplinarity and codesign are key set of skills and approaches which are required of ethical and responsible AI."]},"121_Submission 121 - Attachment.0a5a8b2d2c3b8.pdf":{"organization_name":null,"organization_type":null,"classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI and algorithmic decision-making systems","arguments":["AI can deliver enormous benefits but also has potential for harm","Self-regulation by tech companies is insufficient","Regulation is needed to prevent misuse and maintain public trust"],"counterarguments":["Designing regulation that prevents harm without hindering innovation is challenging"],"key_recommendations":["Mandate transparency for AI-generated content","Prevent user anonymity to combat trolling","Require explanations for AI search and query results","Ban military use of autonomous AI (with exceptions for defense)","Protect copyrighted materials from AI misuse","Provide human-driven appeal processes for automated decisions"],"risks_and_challenges":["Disinformation and misinformation","Identity misrepresentation","Misuse of personal data","Bias in AI outputs","AI hallucinations and inaccuracies","Intellectual property misappropriation","AI-driven warfare"],"safeguards_and_mitigations":["Identify AI-generated responses","Verify identities and prevent impersonation","Mandate explanations for AI outputs","Implement mechanisms to identify state-sponsored interference","Protect copyrighted materials"],"examples":{"Video generation using AI":"A Sydney organisation is working with film studios to overcome issues with dubbing films into other languages. The images from, say, an English language film can be (and are being) altered to synchronise with, say, a Spanish language soundtrack.","Social media misuse":"massacres in Myanmar (Facebook), massacres in north-east Africa (Facebook), Cambridge Analytica (Facebook)"},"international_alignment":"Supports international coordination and Australian leadership","values":["Transparency","Accountability","Trust","Innovation","Public safety"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce regulations"},{"entity":"Tech companies","role":"Implement responsible AI practices"},{"entity":"Publishers (including social media)","role":"Responsible for content moderation"}],"sector_impacts":[{"sector":"Defence","impact":"Potential exclusion from certain regulations for national security"},{"sector":"Media and Entertainment","impact":"Improved content localization through AI"}],"quotes":["As much as AI can be used for good, it can potentially be used for ill.","Designing regulation - in this area - that prevents harm without preventing useful innovation is very challenging.","If you do nothing else, I strongly urge you to consult with the following individuals or entities about the regulation that the world in general, and Australia in particular, needs"]},"350_119_SBS submission_Safe and responsible AI in Australia - Discussion paper_July 2023.3651becf00a51.pdf":{"organization_name":"SBS","organization_type":"Public broadcaster","classification":"Neutral","overall_position":"Supports balanced regulation that prevents harm without inhibiting innovation","arguments":["AI can deliver significant benefits across the economy and society","Regulation should strike a balance between harm prevention and supporting innovation","A principles-based approach is important for future-proofing regulatory frameworks"],"counterarguments":["null"],"key_recommendations":["Adopt a principles-based approach to regulation","Ensure AI and its applications are fair and beneficial to all individuals and entities","Address intellectual property issues related to AI services","Consider algorithmic transparency in AI regulation"],"risks_and_challenges":["Rapid pace of AI development and numerous uncertainties","Potential for AI to increase biases","Intellectual property issues with AI services"],"safeguards_and_mitigations":["Ensure AI and its regulation reduce or eliminate biases","Address intellectual property rights and usage by AI services","Promote algorithmic transparency"],"examples":{"Potential positive use cases of AI for public media providers":"automation of previous low-level manual tasks; enhanced research and content production capability through information gathering, clustering and summarisation, insights from market and data trends, and access to a broader range of information sources; enhanced searchability of and user engagement with content through improved auto-tagging functionality and optimised distribution strategies; improved personalisation of service offerings; more efficient language translation, subtitling and closed captioning; social media management and moderation."},"international_alignment":"null","values":["Innovation","Fairness","Transparency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"SBS","role":"Explore positive uses of AI to benefit Australian audiences and stakeholders"},{"entity":"Government","role":"Collaborate with SBS to promote trust, benefits, understanding, and new developments in AI"}],"sector_impacts":[{"sector":"Media","impact":"Potential for enhanced content production, distribution, and audience engagement"}],"quotes":["SBS is currently exploring the incorporation of positive uses of AI\u2014that may benefit its Australian audiences and stakeholders, including the Australian public-at-large\u2014into aspects of its operations.","SBS\'s key concern is that any regulation should strike the optimum balance between harm prevention and supporting innovation, evolving a governance framework without hindering or stifling the potential benefits of AI.","As a matter of principle both AI and its regulation should reduce or eliminate, rather than increase, any biases."]},"459_AIHS Submission - Safe and responsible AI - 4 August 2023.6877742f5d7a1.pdf":{"organization_name":"Australian Institute of Health and Safety (AIHS)","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports adapting existing OHS regulatory schemes to address AI risks, with some new measures","arguments":["Existing OHS regulatory schemes can be adapted to address AI risks","Corporations and employers should bear the greatest regulatory responsibility for AI technologies","AI technologies are inherently linked to OHS as they change the nature of work"],"counterarguments":["Some aspects of existing OHS legislation are not fit for purpose for AI, such as the hierarchy of controls"],"key_recommendations":["Add OHS regulations to the list of relevant regulations for AI","Adopt ISO/IEC 23894 standard for AI risk management","Invest in digital literacy for OHS professionals and regulators","Support Safe Work Australia to include AI in national OHS policy discussions"],"risks_and_challenges":["AI tools like ChatGPT may provide misleading or inaccurate OHS advice","AI technologies may degrade or undermine existing OHS regulatory schemes","Rapidly evolving hazards challenge existing OHS schemes"],"safeguards_and_mitigations":["Adapt existing OHS regulatory schemes to AI risks","Implement performance-based regulation at the organizational/OHS level","Adopt ISO/IEC 23894 standard for AI risk management"],"examples":{"ChatGPT providing inaccurate OHS advice":"A tangible example of the risks of AI and ADM technologies today is duty holders using tools such as ChatGPT to search for and receive advice on OHS matters. We have concerns that these types of tools will provide misleading or inaccurate advice.","Federal Safety Commissioner\'s \'model client\' framework":"As an example of an OHS framework, the Office of the Federal Safety Commissioner provides \'model client\' framework. This framework guides state (and corporate) entities procuring construction works on how to leverage commercial influence to achieve positive OHS outcomes on projects"},"international_alignment":"Supports adoption of international standards like ISO/IEC 23894","values":["Safety","Health","Risk-based approach"],"tone":"Cautionary","stakeholders":[{"entity":"OHS regulators","role":"Adopt and monitor implementation of AI risk management standards"},{"entity":"Corporations and employers","role":"Bear the greatest regulatory responsibility for AI technologies"},{"entity":"Safe Work Australia","role":"Facilitate national discussions on AI in OHS policy"}],"sector_impacts":[{"sector":"Workplace health and safety","impact":"AI technologies will change the nature of work and introduce new risks"}],"quotes":["AI and ADM technologies are already and will continue to change the inherent nature of work, which makes them intrinsically linked to OHS.","Since corporations and employers stand to gain the greatest benefits from the development, implementation and use of AI and ADM technologies, our view is they should bear the greatest regulatory responsibility.","We believe ISO23894 should be adopted and supported in an OHS context, for example to support ISO31000 (risk management) and ISO45000 (OHS management systems)."]},"52_AI_Discussion_paper_response.c3d0f9965311b.pdf":{"organization_name":"Mileva Security Labs","organization_type":"Advisory and research company","classification":"Proponent","overall_position":"Supports risk-based regulation of AI with a focus on AI Security","arguments":["AI Security should be considered an extension of cyber security","Current AI adoption lacks consideration of AI Security risks","Risk-based approach is suitable for addressing potential AI risks"],"counterarguments":["null"],"key_recommendations":["Invest in training for AI risks at leadership and practitioner levels","Develop technical ecosystem to support AI assurance, mitigation, control and audit","Implement governance frameworks, policy and legislation for AI system security","Include AI Security in risk-based approach to AI regulation"],"risks_and_challenges":["Adversarial attacks on AI by nation-states and criminals","Vulnerability of AI systems to attacks","Potential catastrophic consequences on productionised AI systems across industries"],"safeguards_and_mitigations":["Stringent certifications for AI technologies and use cases","Increased up-skilling for all people, particularly in professional and educational roles","Implementation of technical and governance controls to \'harden\' AI systems"],"examples":{"Cyber security threat as a parallel to AI security risks":"This mirrors closely the cyber security threat from nation states and proxies.","ChatGPT as an example of open-source technology with limited assessment information":"ChatGPT for example"},"international_alignment":"Supports alignment with ISO/IEC 22989:2022 for AI definitions","values":["Transparency","Responsibility","Security"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Cyber Security Centre (ACSC)","role":"Take carriage of AI standards, and reporting risks and vulnerabilities"},{"entity":"Government, academia and industry","role":"Build a community of practice for increasing awareness and adoption of AI standards"}],"sector_impacts":[{"sector":"Tech sector","impact":"Potential impact on academic and innovation communities if high-risk activities are banned"},{"sector":"Small-medium enterprises","impact":"May require up-skilling to adopt risk-based approach"}],"quotes":["We would applaud the introduction of risk-based regulation of AI.","We see the building of a community of practice across Government, academia and industry to be a vital component for increasing the awareness and adoption of standards by all organisations that might like to implement AI.","We believe the risk-based approach should be mandatory for all organisations using AI, but the degree to how this is implemented should differ according to the use case, risk level, and type of organisation."]},"412_Submission on Supporting Responsible AI.c2831522b477a.pdf":{"organization_name":"Thoughtworks Australia","organization_type":"Technology consultancy","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a risk-based approach","arguments":["Regulatory governance will be far more effective than a purely voluntary approach","Human oversight and accountability should always remain in the decision making process","Transparency of source data and model training is essential"],"counterarguments":["null"],"key_recommendations":["Implement a risk-based framework for managing AI risks","Mandate transparency and documentation requirements for AI technologies in high-risk contexts","Develop an AI accountability framework","Establish a national AI ethics advisory council"],"risks_and_challenges":["Lack of transparency and explainability in AI systems","Algorithmic bias and discrimination","Spread of misinformation/disinformation","Privacy risks from large language models","Validity and safety risks in high-risk fields","Risk of industrial displacement"],"safeguards_and_mitigations":["Impact assessments for medium and high-risk AI applications","Notices to inform individuals when AI/automation is used","Meaningful human oversight for high-risk AI applications","Explanations for logic and factors influencing AI-enabled decisions","Recurring training proportional to risk level","Monitoring and documentation requirements"],"examples":{"Sustainable decision-making":"We\'ve helped clients apply AI to help them make more sustainable decisions, model the impacts of planned sustainability efforts and identify inefficiencies to progress towards their climate action goals.","EV charging infrastructure":"In one initiative, we used an AI-augmented approach to develop a prototype for Charge Point Operators (CPO). This \'solver\' tool assists in the design of charging infrastructure networks for electric vehicles (EV).","Social media abuse filtering":"We partnered with a company to create a plugin for social media that helps to filter abusive language and trolling, specifically for those at risk of trolling and gender violence, enabling users to make tech more responsible, even when platforms and vendors themselves don\'t implement adequate controls."},"international_alignment":"Supports international collaboration on AI governance measures","values":["Transparency","Accountability","Human oversight","Safety","Ethics"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulations, lead by example in responsible AI use"},{"entity":"Industry","role":"Adhere to responsible AI practices, participate in governance discussions"},{"entity":"Academia","role":"Contribute to research and development of responsible AI"},{"entity":"Civil society","role":"Provide oversight and advocacy for public interests"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential safety risks requiring sector-specific regulations"},{"sector":"Transport","impact":"Potential safety risks requiring sector-specific regulations"},{"sector":"Creative industries","impact":"Potential disproportionate impact on workers"}],"quotes":["We believe that regulatory governance will be far more effective than a purely voluntary approach.","Our first principles are that human oversight and accountability should always remain in the decision making process, and that transparency of source data and model training is essential.","In order to create meaningful legislation or frameworks that govern the use of AI, the technology must be clearly and realistically defined in technical terms, rather than referred to under a single label like \'AI\' which is often co-opted for marketing purposes to refer to a broad range of technologies."]},"464_Submission 464 - National Australia Bank - 4-Aug.28aeafb368fea.pdf":{"organization_name":"National Australia Bank Ltd (NAB)","organization_type":"Bank","classification":"Neutral","overall_position":"Supports harmonisation of existing legislation rather than new AI-specific regulations","arguments":["Existing laws and regulations already govern safe and responsible AI usage","New AI-specific regulation could hinder innovation and international competitiveness","Current laws can be applied to AI-related issues without new technology-specific legislation"],"counterarguments":["There may be gaps in existing regulations that don\'t address all AI risks","Rapid AI development may require adjustment of definitions and regulations"],"key_recommendations":["Map coverage of existing laws in relation to AI issues before introducing new legislation","Consider government certification for AI vendors based on existing AI Ethics Principles","Harmonize existing legislation and raise awareness of how current laws apply to AI use"],"risks_and_challenges":["Potential bias in AI training and tuning","Energy footprint created by computational power for AI training","Resilience concerns for AI use in critical infrastructure"],"safeguards_and_mitigations":["Governance through existing Risk Management Frameworks","Alignment with Data Ethics Principles","Human oversight and intervention capability for AI-facilitated decisions"],"examples":{"AI benefits in banking":"There are already great benefits being realised from the use of AI in NAB within cyber security, fraud, and financial crime.","Participation in AI ethics pilot":"NAB participated in the DISR Australian AI Ethics Principles piloting facial recognition technology to digitally verify customer identities in 2019.","Enforcement action on AI cases":"Enforcement action has already been taken in cases that involve AI, without the need for new technology specific legislation."},"international_alignment":"Supports international alignment, particularly with ISO definitions","values":["Transparency","Human oversight","Safety"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Map existing laws, consider certification for AI vendors"},{"entity":"National AI Centre","role":"Potential administrator of AI vendor certification"}],"sector_impacts":[{"sector":"Banking","impact":"Better customer service, more personalized financial support, increased efficiency"},{"sector":"Cybersecurity","impact":"Improved security measures and fraud detection"}],"quotes":["NAB\'s overriding priority in the adoption of AI is to realise the significant benefits while keeping our customers, colleagues and communities safe.","Rather than the development of new AI-specific regulations, NAB supports harmonisation of existing legislation (including at the state and territory level) and programs to raise awareness of how existing laws apply to AI use (including by SMEs).","The potential benefit from AI in our society is significant and must be carefully balanced with ensuring it is safely used."]},"427_Gadens - Submission in response to AI Government Forum.33f007ca15dc8.pdf":{"organization_name":"Gadens","organization_type":"Legal services firm","classification":"Proponent","overall_position":"Supports a balanced approach with risk-based regulation for high-risk AI applications and voluntary measures for low-risk applications","arguments":["Existing regulatory regimes already cover some AI risks","A risk-based approach allows for effective resource allocation","Mandatory compliance for all AI applications may negatively impact innovation"],"counterarguments":["Current regimes do not address high-risk AI use cases specifically","Existing frameworks are not equipped to evaluate and ban AI applications with unacceptable risk levels"],"key_recommendations":["Adopt a single future-proof and technology neutral definition of AI","Establish a designated AI board composed of regulators","Implement a government-wide AI assurance and ethics framework","Create a central or agency-specific AI register for government agencies"],"risks_and_challenges":["Lack of historical data for accurate risk evaluations","Potential stifling of cutting-edge and highly innovative AI uses","Balancing transparency with business interests"],"safeguards_and_mitigations":["Regular testing and review of control effectiveness","Increased focus on record-keeping and change management","Preserving flexibility in risk assessments"],"examples":{"AI Verify program in Singapore":"AI Verify \u2013 This is a toolkit by the Infocomm Media Development Authority and the Personal Data Protection Commission which provides a testing framework for organisations to test their AI systems.","Canada\'s Directive on Automated Decision-Making":"Canada has issued a Directive on Automated Decision-Making which applies to federal government institutions and requires them to classify automated decision-making AI systems used to make administrative decisions into 1 of 4 risk categories based on the likely impact of the decision on individual rights, economic interests etc."},"international_alignment":"Supports alignment with international regulatory frameworks for consistency and competitiveness","values":["Transparency","Accountability","Innovation","Public trust"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government agencies","role":"Implement AI assurance frameworks and maintain AI registers"},{"entity":"Private organizations","role":"Comply with industry codes and voluntary standards"},{"entity":"AI developers and deployers","role":"Conduct risk assessments and ensure responsible AI practices"}],"sector_impacts":[{"sector":"Public sector","impact":"Higher standards for AI applications to build public trust"},{"sector":"Private sector","impact":"Balancing innovation with responsible AI practices"}],"quotes":["We consider that to the extent a separate regulatory regime were to be introduced to regulate the use of AI, such a regime should focus specifically on the testing, approving and banning of high-risk AI applications to close this gap.","We consider that industry specific codes of conduct and voluntary technical and ethical standards are the preferred tool to encourage AI best practices among private organisations while allowing them to remain competitive internationally.","Transparency should balance the interests of businesses and consumers alike and should be used as a tool to both promote AI development and acceptance."]},"487_Submission 487 - Australian Red Cross - 11-Aug.643ec9f4af0d7.pdf":{"organization_name":"Australian Red Cross","organization_type":"Humanitarian organization","classification":"Proponent","overall_position":"Supports responsible AI development with enhanced safeguards and community engagement","arguments":["AI has potential for positive humanitarian impact","Current regulations need strengthening to address potential harms and vulnerabilities","Community engagement is crucial for developing trusted and effective AI solutions"],"counterarguments":[],"key_recommendations":["Apply a risk and harm-reduction approach to strengthen existing regulations","Use technology-neutral solutions unless specific technology creates unique harms","Promote explainability of AI technology and enable informed consent","Adopt a human-centered approach with broad community engagement"],"risks_and_challenges":["Potential harms to vulnerable populations","Lack of transparency and explainability in AI systems","Risks of discrimination and exclusion","Threats to privacy and data protection"],"safeguards_and_mitigations":["Mandated assessments to identify potential harms","Clear accountability mechanisms for developers","Provision of non-digital alternatives","Easy-to-understand information in various accessible formats"],"examples":{"AirSeed technology":"AirSeed have integrated Australian Red Cross\' lived experience framework and Humanity First Principles into their self-assurance process to maximise community benefit and avoid potential harms.","Maya Cares chatbot":"Maya Cares chatbot and online resource supporting women of colour to understand, process and address racism that has been piloted through Australian Red Cross Humanitech.","Kara Technologies partnership":"Australian Red Cross Humanitech is also partnering with Kara Technologies to translate emergency messaging into Auslan using \'digital human\' avatars."},"international_alignment":"null","values":["Human-centered approach","Transparency","Community engagement","Do no harm","Privacy"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"AI developers","role":"Identify and mitigate potential harms, ensure transparency"},{"entity":"Community members","role":"Participate in AI development and provide lived experience insights"},{"entity":"Government","role":"Develop and enforce regulations, collaborate with civil society"}],"sector_impacts":[{"sector":"Humanitarian services","impact":"Increased accessibility, scale and impact of services"},{"sector":"Emergency services","impact":"Improved communication and response capabilities"}],"quotes":["Australian Red Cross recommends application of a risk and harm-reduction approach to strengthen existing regulations by including a lens to identify potential harms and vulnerability across the full AI life cycle.","Australian Red Cross recommends that solutions should be \'technology neutral\' unless the solution cannot address harms that specific technology creates.","Australian Red Cross supports a human-centred approach and broad community engagement to ensure AI developers maximise positive benefits and mitigate negative impacts of their technology on people experiencing vulnerability."]},"491_Submission 491 - Amazon Web Services - 11-Aug.da36c540a75d1.pdf":{"organization_name":"Amazon","organization_type":"Technology company","classification":"Proponent","overall_position":"Supports updating existing laws and sector-specific regulations to address AI, while encouraging adoption of risk-based AI governance practices","arguments":["Existing laws and sector-specific regulations can be updated to address AI challenges","A risk-based approach focusing on high-risk AI applications is more effective than one-size-fits-all regulation","Vertical, sector-specific approaches allow for more informed and context-specific regulation"],"counterarguments":["null"],"key_recommendations":["Adopt a definition of AI that focuses on its unique attributes","Assign responsibility based on the relative ability of each actor in the AI supply chain to address specific risks","Adopt a \'vertical\', sector-driven approach to AI regulation and governance","Align responsible AI governance policies to ISO/IEC 42001","Include expectations for safety and security of AI systems in regulatory frameworks","Encourage AI developers and deployers to align to recognised international standards and frameworks"],"risks_and_challenges":["Low adoption of AI technologies by Australian businesses","Potential for misuse of AI technologies","Novel questions arising from AI integration in various sectors"],"safeguards_and_mitigations":["Require impact assessments for high-risk AI applications","Implement risk-based AI governance practices","Encourage the use of trusted AI developers","Ongoing oversight and review practices","Promoting digital literacy and skills development"],"examples":{"Transport for NSW using predictive modelling":"Transport for NSW is using predictive modelling of patronage numbers across the entire transport network, enabling the agency to better plan workforce and asset utilisation, and improve customer satisfaction.","Healthcare organizations benefiting from AI":"Local healthcare organisations including CSIRO, Melbourne Genomics Health Alliance, and Healthdirect are benefitting from AI and ML to accelerate research for therapeutic development and making informed decisions that lead to better patient outcomes.","Amazon CodeWhisperer improving productivity":"A study of developers using Amazon CodeWhisperer, an AI coding companion with built-in security scanning for finding and suggesting remediations for hard-to-detect vulnerabilities showed that tasks could be completed 57 per cent faster and were 27 per cent more likely to be complete successfully."},"international_alignment":"Supports global standards and interoperable policy solutions","values":["Innovation","Responsibility","Safety","Fairness","Privacy","Security"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Develop effective risk-based regulatory frameworks and guardrails for AI"},{"entity":"Developers","role":"Build AI systems with responsibility in mind and document efforts to evaluate and mitigate potential risks"},{"entity":"Deployers","role":"Integrate AI into services responsibly and perform impact assessments for high-risk use cases"},{"entity":"Sector-specific regulators","role":"Identify higher risk use cases for their sector and provide regulatory guidance"}],"sector_impacts":[{"sector":"Economy","impact":"Potential to produce over AU$315B in gross economic value over the coming decade"},{"sector":"Healthcare","impact":"Accelerating research for therapeutic development and improving patient outcomes"},{"sector":"Transportation","impact":"Better planning of workforce and asset utilization, improved customer satisfaction"},{"sector":"Education","impact":"Need for digital literacy and technology competency training within teaching qualifications"}],"quotes":["At Amazon, we believe the design, development, and deployment of AI must respect the rule of law, human rights, and values of equity, privacy, and fairness.","We believe government should ensure that existing principles-based, technology-neutral laws and sector-specific regulations remain fit-for-purpose in an era when AI will increasingly be used in high impact ways.","Responsible use of AI technologies is key to fostering continued innovation, and AWS is committed to developing fair and accurate AI services."]},"408_CDW SPG - Submission to Safe AI in Australia Framework - 31072023.374d55ecfb4be.pdf":{"organization_name":"Centre for Digital Wellbeing","organization_type":"Policy research and design body","classification":"Proponent","overall_position":"Advocates for comprehensive regulation","arguments":["Current proposed draft framework is inadequate to protect individuals and community from potential AI harm","Enhanced regulatory oversight would increase safety, transparency, and trust","Stronger regulatory approach is needed to address broad current and potential impacts of AI"],"counterarguments":["null"],"key_recommendations":["Establish a dedicated regulatory authority for AI","Include an unacceptable risk category in the risk management approach","Implement more robust human oversight requirements","Extend regulatory framework beyond explanations","Implement stronger safeguard mechanisms beyond training"],"risks_and_challenges":["Potential market incentives to assess risk lower than actual risks","Erosion of trust due to breaches in self-assessment and industry monitoring","Difficulty in establishing responsibility for adverse impacts in AI-enabled systems","Challenges in understanding and seeking redress for \'black box\' AI technologies"],"safeguards_and_mitigations":["Automatic peer-review in impact assessments","Regular reviews on safety","Clear notification requirements for AI use","Proper complaint and feedback mechanisms","Enforcement mechanisms for breaches, including fines and ability to suspend activities"],"examples":{"Cookie consent notices issues under GDPR":"Cookie consent notices are often used in conjunction with techniques that nudge people into accepting data collection and tracking, including notification positions, and choice offerings.","Banking and accounting sector breaches":"Breaches of regulatory and risk compliance frequently occur despite well-entrenched employee training regimes \u2013 we point to two recent examples in high-profile in the Australian banking and accounting sectors."},"international_alignment":"Supports alignment with international standards, particularly EU and Canada","values":["Transparency","Trust","Safety","Accountability","Responsibility"],"tone":"Cautionary","stakeholders":[{"entity":"Regulatory authority","role":"Implement objective impact assessments, provide clarity to developers and adopters, advance trust"},{"entity":"Executives, senior leaders, and managers","role":"Lead compliance efforts and ensure AI development is at the core of organizational actions"}],"sector_impacts":[{"sector":"Economy","impact":"Profound changes due to AI adoption"},{"sector":"Employment","impact":"General rates of employment affected by AI"}],"quotes":["The current proposed draft framework appears inadequate to meaningfully protect individuals and the wider community from potential AI harm.","Enhanced regulatory oversight, with far more robust elements in a risk-based approach, would increase safety, transparency, and trust \u2013 all essential in underpinning long-term stability in Australia\'s development and adoption of AI.","Much more robust regulatory capacity - with strong safeguards - would be needed for monitoring and documentation, other possible elements of the draft risk-based approach, and the overall framework \u2014 in order to ensure that AI in Australia is transparent, trusted, safe, and responsible."]},"139_Insurtech Australia.pdf":{"organization_name":"Insurtech Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a decentralised, sector-based and principles-based approach to AI regulation","arguments":["A decentralised approach allows for regulation tailored to specific risks and use cases in different sectors","A principles-based approach allows for a more dynamic regulatory approach that caters to the rapidly changing nature of AI","Existing legislation may already apply to AI, adding additional \'overarching\' legislation could introduce undue complexity"],"counterarguments":["A centralised, \'generic\' or risk-based approach could provide consistency in regulatory treatment of AI across sectors","Overarching legislation could provide comprehensive coverage of AI risks"],"key_recommendations":["Conduct a gap analysis of existing regulation to identify where frameworks need to be updated to contemplate AI","Establish dialogue between industry and government to inform regulatory approach","Develop public and industry education and guidance about AI technology, its benefits and challenges"],"risks_and_challenges":["Lack of public trust and confidence in AI technologies","Knowledge gap in industry and public about how existing legal frameworks apply to AI","Potential for AI to produce deficient, discriminatory, biased or incorrect outputs due to data quality issues"],"safeguards_and_mitigations":["Develop practical guidance and educational initiatives on AI use","Implement transparency requirements for AI products","Consider mandating quality assurance processes for training data sets"],"examples":{"EU approach":"The EU\'s proposed risk-based, legislative approach gives specific treatment to high-risk applications","UK approach":"The UK\'s proposed decentralised, sector-specific approach allows sectors to regulate AI based on unique risks and use cases","Amsterdam Register":"The Amsterdam government has adopted a searchable public AI register providing information on algorithmic systems used in public service"},"international_alignment":"Supports consistency with definitions used by key trade partners","values":["Innovation","Transparency","Consumer protection"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop regulation and provide guidance"},{"entity":"Industry","role":"Inform regulatory approach and implement responsible AI practices"},{"entity":"Public","role":"Engage with and trust AI technologies"}],"sector_impacts":[{"sector":"Insurance and insurtech","impact":"AI enhancing efficiency of internal processes and developing more sophisticated commercial offerings"},{"sector":"Public sector","impact":"Potential for increased transparency and public trust through AI registers"}],"quotes":["Insurtech believes that a decentralised, sector-based and principles-based approach to the regulation of AI would best serve the needs of both public and private sector stakeholders in Australia","We consider that upfront and proactive transparency is the best way in which public trust and confidence in AI can be fostered","We think that it is imperative that Government canvasses feedback broadly from industry players of all sizes, to ensure that non-regulatory initiatives and materials are fit-for-purpose and speak to the use cases in the relevant industry sectors."]},"389_Response to AI Inquiry_KB.0865439738e2e.pdf":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive regulation","arguments":["Self-regulation is insufficient for ensuring AI safety","Robust and enforceable safety regulations are urgently needed","A well-resourced, technically competent supervisory agency is crucial for upholding regulations"],"counterarguments":["Voluntary or self-regulation tools may be insufficient for responsible AI"],"key_recommendations":["Establish conditional usage guidelines for AI","Strengthen regulatory oversight of AI development","Mandate access to AI models for auditing","Monitor indicators of AI development","Strengthen legal liability for AI labs","Implement joint culpability scheme","Introduce mandatory reporting and response mechanisms","Improve technological literacy within government","Promote public technological literacy","Establish AI Safety Research Body","Boost AI safety research funding"],"risks_and_challenges":["Potential existential risk from AI","Misuse of highly capable AI","Unintended harm from highly capable AI","Lack of technical expertise in regulatory bodies","Rapid advancement of AI technology outpacing regulatory efforts"],"safeguards_and_mitigations":["Implement enforceable, binding regulations","Demand transparency in AI development","Enforce strategies to mitigate risk","Hold AI labs accountable throughout the technology lifecycle","Promote and enforce innovation of safer strategies"],"examples":{"Deepwater Horizon oil spill":"The Deepwater Horizon oil spill is used as a cautionary tale to illustrate the dangers of self-regulation in high-risk industries.","MMS regulatory failure":"The failure of the Minerals Management Service (MMS) in regulating the oil industry is presented as an example of what can go wrong with insufficient regulatory oversight.","OpenAI\'s limited investment in alignment research":"OpenAI\'s dedication of only 20% of their computing capacity to AI alignment research is cited as an example of insufficient investment in AI safety."},"international_alignment":"null","values":["Safety","Accountability","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Establish and enforce regulations, create well-resourced supervisory agencies"},{"entity":"AI labs","role":"Comply with regulations, ensure safety in AI development, provide transparency"},{"entity":"Regulatory agencies","role":"Monitor AI development, enforce regulations, conduct audits"}],"sector_impacts":[{"sector":"AI industry","impact":"Increased regulatory oversight and accountability"}],"quotes":["AI will probably most likely lead to the end of the world, but in the meantime, there\'ll be great companies.","Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war","Australia must acknowledge the potentially existential risk posed by AI."]},"465_Submission 465 - LaingORourke - 4-Aug.81a80af61eaad.pdf":{"organization_name":"Laing O\'Rourke","organization_type":"Global engineering and construction company","classification":"Proponent","overall_position":"Supports responsible AI practices with industry-specific considerations","arguments":["AI can enhance decision-making and risk management in construction","AI should support human capabilities rather than replace workers","Industry-specific standards and guidelines are necessary for responsible AI use"],"counterarguments":["null"],"key_recommendations":["Develop industry-specific responsible AI standards","Facilitate data and model sharing mechanisms","Develop a client-driven responsible AI metric and practices"],"risks_and_challenges":["Potential job displacement","Privacy concerns in construction sites","Accountability issues in safety incidents involving AI"],"safeguards_and_mitigations":["Establish acceptable margins of error for AI safety systems","Define standards for worker tracking and data ownership","Implement consent mechanisms for AI studies and programs"],"examples":{"Computer vision for collision avoidance":"For example, in collision avoidance systems an AI-based computer vision incorrectly classifying an excavator as a truck is a type of error our industry can tolerate, since it still warns the worker due to the proximity to heavy plant.","AI bias in hiring":"Amazon ditched AI recruiting tool that favored men for technical jobs.","Wearable devices for health monitoring":"Some wearable devices are already being tested in heavy industry, though most of these technologies are still not mature enough for ongoing deployment."},"international_alignment":"null","values":["Safety","Privacy","Accountability","Transparency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop industry-specific standards and facilitate data sharing"},{"entity":"Construction companies","role":"Implement responsible AI practices"},{"entity":"Workers and unions","role":"Participate in discussions about AI implementation and consent"}],"sector_impacts":[{"sector":"Construction","impact":"Potential transformation of physical and knowledge work, improved safety processes"}],"quotes":["AI will therefore never replace people\'s ability to make safety decisions; it can merely support them.","The bottom line is that the impact of AI in constructions jobs and the businesses that make up our industry could indeed be considerable, but its direction and scope is still unclear.","If tailored to each specific industry, a risk-based approach for the governance of AI could work, but we suggest it would be best to pair this with an ethics-based approach."]},"141_2307_NCC Group\u2019s.pdf":{"organization_name":"NCC Group","organization_type":"Global cyber security business","classification":"Proponent","overall_position":"Supports updating existing laws with AI-specific provisions and introducing new regulatory measures","arguments":["Existing technology-neutral laws provide a strong foundation but AI presents new challenges","Context-specific regulation is needed in addition to existing laws","Transparency and explainability of AI systems are critical for public trust and accountability"],"counterarguments":["Outright banning of AI applications could stifle innovation","A purely sector-based approach may lead to regulatory gaps"],"key_recommendations":["Clearly define Australia\'s risk appetite for AI","Implement a consumer labelling scheme backed by independent third-party validation","Invest in developing AI-related skills and Australian datasets","Establish periodic regulatory and legislative reviews","Develop technical standards for AI systems"],"risks_and_challenges":["Bias in AI systems","Lack of transparency and explainability in AI decision-making","Shortage of skills needed for AI development and assurance","Potential for AI systems to diverge from human values"],"safeguards_and_mitigations":["Mandatory third-party product validation for high-risk AI systems","Continuous monitoring and updating of AI systems","Multidisciplinary approach to reviewing decision criteria","Clear reporting processes for potential biases"],"examples":{"Office 365 adoption":"In the same way that a significant proportion of organisations use Office 365, it\'s likely that some AI systems will be widely adopted across the economy.","Cybersecurity labelling scheme":"This scheme could be voluntary, replicating the cybersecurity labelling scheme for smart devices."},"international_alignment":"Supports international regulatory cooperation and alignment","values":["Transparency","Security","Safety","Privacy","Accountability"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement AI governance framework"},{"entity":"Regulators","role":"Oversee AI development and use in specific sectors"},{"entity":"AI developers and deployers","role":"Implement responsible AI practices and comply with regulations"},{"entity":"Consumers","role":"Make informed decisions about AI use"}],"sector_impacts":[{"sector":"Healthcare","impact":"Context-specific regulation needed for medical devices"},{"sector":"Transport","impact":"Ongoing regulatory efforts supported"}],"quotes":["AI can only be safe if it is also secure.","We believe that the Government\'s plans could be strengthened in the following ways to cement Australia\'s position as a global leader in this field:","End-users and consumers should be empowered to make decisions about the AI systems they use by improving transparency of where and how AI technologies are being deployed."]},"430_Epiphany Law - AI Generated Content - A Low-tech Solution to a High-tech Problem (Final).a369d3caa7cd8.pdf":{"organization_name":"Epiphany Law","organization_type":"Law firm","classification":"Proponent","overall_position":"Supports new AI-specific laws to regulate AI-generated content","arguments":["AI-generated content poses a pressing and real threat to society","Existing laws are inadequate to address the risks of AI-generated content","A low-tech statutory regime is needed to ensure Australians can trust the content they consume"],"counterarguments":["Widely discussed proposals for reform (education, watermarking, AI detection tools) are inadequate","Self-regulation by tech companies is insufficient"],"key_recommendations":["Implement a low-tech statutory regime to regulate AI-generated content","Establish a national \'Misleading AI Content Register\' (MAC Register) administered by the eSafety Commissioner","Create a system for filing \'MAC Declarations\' to report unauthorized digital clones","Implement a \'Safe Harbour\' defense for platforms that remove content within 48 hours of notification"],"risks_and_challenges":["Erosion of social trust through unauthorized synthetic representations","Potential for disinformation and manipulation of public opinion","Harm to individuals through unauthorized digital clones","Difficulty in predicting future AI capabilities and applications"],"safeguards_and_mitigations":["MAC Register to track and manage misleading AI content","Legal consequences for false MAC Declarations","Requirement for actual or presumed consent for creating synthetic likenesses","Penalties for creating or disseminating serious misleading AI content"],"examples":{"Pope Francis puffer jacket":"In March 2023, it was widely reported that large numbers of people believed that \'photograph\' created by Midjourney was a genuine depiction of Pope Francis wearing a white puffer jacket.","Fake Pentagon explosion":"In May 2023, the following AI image of an explosion near the Pentagon spread widely on social media, and is reputed to have caused the S&P 500 to decline about 0.3 percent, wiping billions of dollars off the stock market.","CBA scam video":"In June 2022, an AI-generated scam video involving a deepfake of the Commonwealth Bank of Australia Chief Executive Matt Comyn falsely represented that the bank had launched a \'Quantum AI\' tool that would help individuals make large amounts of money through data analysis."},"international_alignment":"Favors the European approach to AI regulation","values":["Transparency","Accountability","Trust"],"tone":"Cautionary","stakeholders":[{"entity":"eSafety Commissioner","role":"Administer the MAC Register and enforce the proposed legislation"},{"entity":"AI Users","role":"Comply with regulations on creating and disseminating AI content"},{"entity":"Protected persons","role":"File MAC Declarations for unauthorized synthetic likenesses"}],"sector_impacts":[{"sector":"Media and Entertainment","impact":"Increased responsibility for verifying and disclosing AI-generated content"},{"sector":"Technology","impact":"Need to implement systems for content verification and removal"}],"quotes":["We believe that the measures used to address the risks posed by AI-generated content that are discussed most often (i.e. widespread education on AI literacy, provenance measures such as watermarking, and AI detection tools) are worthwhile, but cannot be relied upon to address the identified risks sufficiently.","We therefore call for the implementation of a low-tech statutory regime that utilises existing social capital to ensure that Australians can continue to trust in the content that they consume.","There is a significant asymmetry between the ease and cost with which misleading AI-Generated Content can be created and disseminated, and the time and costs for taking action to have it withdrawn. In our view, this is a source of both risk and unfairness."]},"116_21072023 Safe and responsible AI in Australia - ASA consultation response FINAL.9927a0d49481b.pdf":{"organization_name":"Australasian Sonographers Association","organization_type":"Professional organization","classification":"Neutral","overall_position":"Supports responsible AI implementation with appropriate safeguards and regulations","arguments":["AI has potential to improve access to services and patient care","Appropriate safeguards are necessary to mitigate risks","AI should support health professionals, not replace them in clinical decision making"],"counterarguments":["null"],"key_recommendations":["Implement ongoing AI education and training for health professionals","Ensure patient oversight and clinical decision making by qualified medical professionals","Increase agility of regulatory bodies like TGA for rapid informed decision making on AI approval","Mandate transparency in AI use in diagnostic imaging"],"risks_and_challenges":["Potential for discrimination and bias in AI healthcare applications","Rapidly evolving AI systems may outpace regulatory frameworks","Privacy concerns in healthcare settings"],"safeguards_and_mitigations":["Ongoing AI education and training for health professionals","Patient oversight and clinical decision making by qualified medical professionals","Transparency in AI use in diagnostic imaging","Inclusion of all cross-cultural backgrounds in programming databases"],"examples":{"Ultrasound equipment software":"In the case of ultrasound, the embedded software in ultrasound equipment is not currently learning, it is referring to existing examples/databases, and providing information back to the sonographers using the equipment. This is artificial assistance, not artificial intelligence."},"international_alignment":"null","values":["Patient safety","Transparency","Accountability","Privacy"],"tone":"Cautionary","stakeholders":[{"entity":"Health professionals","role":"Use AI responsibly and maintain oversight in clinical decision making"},{"entity":"Therapeutic Goods Administration (TGA)","role":"Provide rapid informed decision making for AI approval in healthcare"},{"entity":"Patients","role":"Be informed about AI use in their diagnostic imaging"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential improvement in access to services and patient care, but risks of bias and discrimination need to be addressed"}],"quotes":["AI may be used to support health professionals; however, patient oversight and clinical decision making must be undertaken by appropriately skilled and qualified medical professionals.","The ASA acknowledges the potential application of AI and the opportunities to utilise it to improve access to services and patient care. However, it is important to highlight the importance of ensuring appropriate safeguards are in place to mitigate risks.","A risk-based approach for responsible AI should be mandated through regulation in healthcare settings. Whilst this may not be necessary for all industries, any technology that could cause serious harm or have significant bias against an unknowing individual should be regulated."]},"428_MTAA & APACMed - Safe and Responsible AI in Australia.44a2c2c2708a4.pdf":{"organization_name":"Medical Technology Association of Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports risk-based approach for AI regulation, particularly in healthcare/MedTech","arguments":["Existing medical device regulators like TGA are best placed to incorporate AI/ML regulation","Risk-based approach allows focus on high-risk use cases without stifling innovation","Healthcare sector is well-suited for risk-based approach due to range of uses and risk profiles"],"counterarguments":["Broad regulation of AI could have unintended consequences for patient outcomes and medical technology industry","One-size-fits-all approach may not be suitable for all sectors"],"key_recommendations":["Align with international standards for consistency and clarity","Substantial consultation with medical technology industry on proposed regulations","Incorporate AI risk-based approach into existing assessment frameworks"],"risks_and_challenges":["Data breaches during collection, storage, or transmission of private patient data","Biased algorithms due to inadequate access to large volumes of data","Potential harm due to faulty algorithms making inaccurate diagnoses","Liability risk due to reliance on or use of faulty algorithm recommendations"],"safeguards_and_mitigations":["Human expert oversight over AI/ML systems for higher-risk use cases","Appropriate training for human users on real-world scenarios","Regular audits and continuous improvements to retrain models as needed"],"examples":{"Clinical Decision Support in Radiology and Pathology":"Clinical Decision Support (CDS) is an example of Automated Decision Making (ADM) which is used with great success in the healthcare industry. Good examples of this are within Radiology and Pathology, where AI/ML is used to detect complex patterns and allows clinicians to make informed decisions.","AI/ML in drug discovery":"AI/ML services can also support: the extraction of valuable insights on product safety and effectiveness from clinical notes; drug discovery by identifying insights that might have otherwise been overlooked"},"international_alignment":"Supports alignment with international standards and frameworks","values":["Patient safety","Innovation","Transparency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Therapeutic Goods Administration (TGA)","role":"Incorporate emerging AI/ML regulation within existing medical device assessment frameworks"},{"entity":"Medical device manufacturers and sponsors","role":"Implement responsible AI practices and comply with regulations"}],"sector_impacts":[{"sector":"Healthcare","impact":"AI/ML can empower the leverage of diverse healthcare data to build state-of-the-art decision support interventions (DSI) and diagnostics"},{"sector":"Medical technology","impact":"Potential for improved product safety and effectiveness through AI-driven insights"}],"quotes":["MTAA supports a risk-based approach to AI/ML oversight that targets factors most likely to negatively affect patient outcomes in high-risk use cases.","Because of the unique, complex, and highly regulated nature of medical technology, any broad regulation of AI, even regulation of AI aimed at the medical industry generally, could have unintended consequences for patient outcomes and the medical technology industry.","AI is poised to offer advantages across all sectors of the economy, and to public and private organisations of all sizes."]},"495_Submission 495 - Minervai 4.0 - 12-Aug.69c11876bac05.pdf":{"organization_name":"Minervai 4.0","organization_type":"Youth-led volunteer group","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI with a focus on ethical development and responsible use","arguments":["Current laws are not robust enough to protect individual rights in the context of AI","There is a lack of transparency and awareness about AI use, particularly in social media","Individuals have too little power in the face of AI systems"],"counterarguments":["Regulation should not stifle innovation or positive use cases of AI","Some AI applications, such as in construction and health, may be considered risky but have significant benefits to society"],"key_recommendations":["Consider incorporating a risk-based legislative mechanism similar to the European Union\'s AI Act","Establish an AI Safety Commissioner","Mandate labelling of AI generated or manipulated material in consumer settings","Review primary and secondary education to include teaching syllabuses on the safe and responsible use of AI","Create nuanced Australian Standards for AI systems"],"risks_and_challenges":["AI facilitated sexual exploitation and digital violence","Algorithmic bias in decision-making processes","Limited public awareness of AI use in everyday applications","Environmental impacts of AI system training"],"safeguards_and_mitigations":["Create a specific individual right to not be subject to AI systems in particular environments","Implement stringent sustainability requirements for AI systems","Establish mechanisms to audit systems for instances of algorithmic bias","Mandate transparency requirements for AI use in consumer settings"],"examples":{"Social media AI":"AI in social media is deployed in various ways, including through data analytics, behaviour inferencing, generated content and importantly through the curation and display of content itself.","AI in construction":"Application of AI in the construction industry may invoke many benefits such as analysing material use to reduce wastage.","AI in healthcare":"AI applications in health care can provide great benefits to healthcare systems and patients. This includes diagnosis and treatment and patient engagement."},"international_alignment":"Supports adopting measures similar to the European Union\'s AI Act","values":["Inclusion","Care","Intersectionality","Transparency","Accountability","Individual autonomy"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement and enforce AI regulations, be a \'model user\' of AI"},{"entity":"Education institutions","role":"Incorporate AI education and ethics in curricula"},{"entity":"AI developers and companies","role":"Ensure responsible development and deployment of AI"}],"sector_impacts":[{"sector":"Education","impact":"Incorporation of AI education in curricula, potential positive impacts on teaching and learning"},{"sector":"Public sector","impact":"Use of AI in low-risk settings for efficiency gains, subject to higher public expectations and scrutiny"},{"sector":"Construction","impact":"Potential for increased efficiency and sustainability, but with safety considerations"},{"sector":"Healthcare","impact":"Improved diagnosis and treatment, but with critical ethical considerations"}],"quotes":["We believe AI will be a fundamental technology to our generation, and we want to ensure its development and deployment is responsible, and ultimately benefits society.","Overall, it is evident that individuals have too little power in the face of AI systems, and that more distinct individual autonomy and rights should be created through regulation.","We are concerned about whether the law can protect victims as AI makes such techniques increasingly accessible."]},"483_Submission 483 - Business Council of Australia - 11-Aug.de42e61a2279a.pdf":{"organization_name":"Business Council of Australia","organization_type":"Industry association","classification":"Opponent","overall_position":"Prefers updating existing laws and providing guidance rather than introducing new AI-specific regulations","arguments":["Existing laws already apply to AI and provide adequate protections","New regulations could hinder innovation and economic growth","Focus should be on providing guidance on how existing laws apply to AI"],"counterarguments":["Some argue that AI poses unique risks requiring new regulations","Concerns about bias, misinformation, and safety in AI systems"],"key_recommendations":["Prioritize bringing regulators, industry, and academic experts together to develop guidance on existing regulations","Focus on encouraging entities to disclose their AI governance principles","Ensure any new regulations are risk-based, proportionate, and focus on outcomes rather than technology","Work with businesses on positive measures like regulatory sandboxes","Prepare Australians to take advantage of AI technologies"],"risks_and_challenges":["Potential for overregulation stifling innovation and economic growth","Uncertainty in regulatory environment discouraging business investment","Balancing privacy protections with the need for data to test and improve AI systems"],"safeguards_and_mitigations":["Develop clear guidance on how existing laws apply to AI","Use regulatory sandboxes to test innovative AI products and services","Focus on addressing specific harms rather than regulating technology itself"],"examples":{"EU AI Act":"The EU experience with AI regulation is cited as a cautionary tale of overregulation hindering innovation and competitiveness.","Automated vehicles in resources sector":"Automated vehicles have been used safely in the resources and agriculture sector for years, challenging the proposed high-risk categorization.","TGA regulation of medical devices":"The TGA\'s approach to regulating software-based medical devices is suggested as a potential model for developing AI-specific guidance."},"international_alignment":"Supports regional harmonization within the Asia-Pacific region","values":["Innovation","Economic growth","Regulatory clarity","Flexibility","Proportionality"],"tone":"Cautionary and pragmatic","stakeholders":[{"entity":"Government","role":"Work with industry to develop guidance and appropriate regulations"},{"entity":"Businesses","role":"Collaborate with regulators to ensure understanding of AI applications and impacts"},{"entity":"Academic experts","role":"Provide expertise in developing regulatory guidance"}],"sector_impacts":[{"sector":"Resources and agriculture","impact":"Potential overregulation of automated vehicles could hinder safety improvements"},{"sector":"Healthcare","impact":"Regulatory clarity needed for AI in medical devices to promote innovation"},{"sector":"Economy-wide","impact":"Generative AI could contribute up to $115 billion to the economy by 2030"}],"quotes":["Australia faces a critical test in its approach to artificial intelligence (AI). AI is already widely used across Australia in ways which improve the lives of all Australians.","The problem is not that there are no laws. Governments, businesses and the community need a better understanding of how the existing set of laws apply in the context of AI (and its rapid evolution).","If the Commonwealths does take forward any new regulatory responses, these must be risk-based, proportionate, and focused on outcomes, not solely the technology."]},"192_Stripe submission to Consultation on Safe and Responsible AI.docx.16c1e52397468.pdf":{"organization_name":"Stripe","organization_type":"Financial infrastructure company","classification":"Proponent","overall_position":"Supports flexible, context-specific, and risk-based regulation of AI","arguments":["Existing laws should continue to apply consistently to AI","Regulation should be clearly defined and avoid arbitrary risk classification","Frameworks should be simple and easy to implement"],"counterarguments":[],"key_recommendations":["Define the vision for Generative AI in Australia","Support collaboration between research institutions and industry","Provide regulatory clarity","Incentivise adoption and innovation","Invest in the right skills and support workers through the transition"],"risks_and_challenges":["Increased online payments fraud due to acceleration in ecommerce","Difficulty in fighting fraud for businesses","Potential for blocking legitimate transactions with blanket rules"],"safeguards_and_mitigations":["Use of machine learning to reduce online fraud","Adaptive machine learning algorithms to evaluate transactions for fraud risk","Tools for users to set their own rules and conduct manual reviews"],"examples":{"Dermalogica":"Dermalogica reduced fraud rates by 50 percent using Stripe Radar\'s whitelisting and rules","LetsGetChecked":"LetsGetChecked invested its developer resources into its core business while using Stripe for payments and fraud mitigation"},"international_alignment":"Supports international cooperation and dialogue on regulatory and technical standards","values":["Innovation","Flexibility","Clarity"],"tone":"Positive","stakeholders":[{"entity":"Government","role":"Develop clear and flexible approach to regulating AI"},{"entity":"Businesses","role":"Implement AI technologies responsibly"}],"sector_impacts":[{"sector":"Financial services","impact":"Improved fraud detection and access to working capital for digital-first businesses"},{"sector":"E-commerce","impact":"Enhanced fraud prevention and customer verification"}],"quotes":["We welcome the focus in the Discussion Paper on the application of relevant existing laws. AI will increasingly be a part of every business, regardless of sector. Economy wide laws, such as competition, consumer and privacy laws, should continue to apply consistently.","It is essential that any regulations are clearly defined, especially if regulation is going to be determined on the basis of risk.","As with any area of regulation, businesses operating in multiple jurisdictions can face challenges when seeking to meet different regulatory standards. Seeking to satisfy different rules and frameworks around the globe can limit innovation for both AI companies and end users."]},"391_Douglas, Submission on Supporting Responsible AI consultation-merged.28906777d0b04.pdf":{"organization_name":"University of Western Australia","organization_type":"Academic institution","classification":"Proponent","overall_position":"Supports strengthening protection of Australians\' personality rights in AI regulation","arguments":["AI-driven platforms like ChatGPT may damage Australians\' reputational interests","Existing defamation laws are being weakened, contrary to the need for stronger protection of personality rights","Current regulatory approaches do not adequately cover AI-related risks"],"counterarguments":["Weakening defamation laws is being done for the sake of freedom of speech"],"key_recommendations":["Mandate foreign platforms to create systems for quick removal of damaging content","Strengthen protection of Australians\' personality rights in future AI regulation","Liaise with the working party driving changes to Australia\'s defamation laws"],"risks_and_challenges":["Damage to reputational interests by AI-driven platforms","Inconsistent reliability of AI-generated content","Difficulty in enforcing defamation judgments against foreign AI companies"],"safeguards_and_mitigations":["Use of defamation law to respond to harm caused by AI platforms","Creation of systems for quick removal of damaging content without the need for costly legal action"],"examples":{"Brian Hood case":"Brian Hood is Mayor of Hepburn Shire in Victoria, Australia. Members of the public told Hood that ChatGPT was identifying Hood as party to a high-profile bribery scandal.","Potential defamation lawsuit against OpenAI":"At the time of writing, Hood had engaged lawyers who sent OpenAI a concerns notice. Unless the company remedies the situation, a defamation case is likely."},"international_alignment":"null","values":["Protection of personality rights","Reputation","Privacy"],"tone":"Cautionary","stakeholders":[{"entity":"AI companies","role":"Ensure responsible AI practices and comply with local laws"},{"entity":"Government","role":"Strengthen regulations to protect personality rights"}],"sector_impacts":[{"sector":"Media and technology","impact":"Increased liability for AI-generated content"},{"sector":"Legal","impact":"New challenges in defamation cases involving AI"}],"quotes":["My view is that personality rights, like reputation and privacy, are deserving of strong protection. I hope that future regulation of AI strengthens protection of Australians\' personality rights.","One way it could do that is by mandating that relevant foreign platforms create systems to remove damaging content quickly, without the need for an affected person to pay a lawyer to sue a foreign company with the associated cost of service and enforcement outside of the jurisdiction.","ChatGPT vacuums the content from the web and pumps out the best content it can come up with in response to a prompt. Some of that content is great, and some of it is nonsense."]},"398_Responsible Ai Consultation - Hannah Maude Group.bdaca4658e4fb.pdf":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and education on AI","arguments":["Government needs to keep up with technology to avoid being led by corporations","Education and awareness are required for informed decisions and trust","Regulation needs to be in line with risk","Inclusion and representation are necessary in AI development and decision-making","Organizations deploying AI should be accountable for safety and monitoring"],"counterarguments":[],"key_recommendations":["Invest in government expertise on AI","Launch public education campaigns on AI","Prioritize regulation based on risk levels","Increase diversity in AI development and governance","Implement accountability measures for AI-deploying organizations","Enhance transparency in AI decision-making processes","Support responsible AI development through grants"],"risks_and_challenges":["Lack of government expertise in AI","Public\'s lack of understanding of AI","Potential misuse of AI in social media, health decisions, and warfare","Lack of diversity in technology and AI development","Impact on future generations and education","Privacy concerns and data misuse","Algorithmic decision-making without proper checks"],"safeguards_and_mitigations":["Government upskilling in AI","Public education campaigns on AI","Diverse representation in AI taskforces and committees","Mandatory monitoring and safety measures for AI-deploying organizations","Transparency in AI data usage and decision-making processes","Human intervention checks on AI decisions","Punishments for non-compliance with best practices"],"examples":{"Auckland terrorist incident":"Using Social Media as a gauge, monitoring of platforms would have prevented the Auckland terrorist to spread his message via video","Robodebt":"Robodebt is a good example of the need for robust process checks on decisions made by algorithms or machines"},"international_alignment":"null","values":["Transparency","Accountability","Inclusivity","Safety","Privacy"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop expertise, regulate, and educate on AI"},{"entity":"Tech companies","role":"Implement safety measures, ensure transparency, and share problematic findings"},{"entity":"Public","role":"Become educated about AI to make informed decisions"},{"entity":"Parents and education system","role":"Support children in an AI era"}],"sector_impacts":[{"sector":"Education","impact":"Need to adapt to help students use AI for creativity and innovation"},{"sector":"Health","impact":"AI could be making decisions regarding people\'s health"},{"sector":"Social Media","impact":"Need for better monitoring and safeguards"}],"quotes":["The Government needs to keep up with technology or we\'re led by the corporations.","Education and awareness is required to help people make informed decisions and to ensure their trust of it.","Regulation needs to be in line with risk","We believe the organisations deploying Ai should be accountable for the safety and monitoring of its users.","Transparency and accountability is critical when it comes to personal information and the decisions Ai could be used to make thay impact people lives"]},"271_IMAA_SafeAndResponsibleAIinAustralia_Response.8ac26ed090433.pdf":{"organization_name":"Image Makers Association Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports implementing new regulations and policies specific to AI, with a focus on copyright protection","arguments":["AI systems may be using copyrighted imagery without authorization","Existing copyright laws may not adequately address issues related to generative AI","Transparency is needed regarding data sets used for AI models"],"counterarguments":["AI can be a useful tool for photographers in their creative endeavors","AI can improve productivity in parts of the workflow"],"key_recommendations":["Review application of existing copyright legislation in relation to AI","Develop legislation clarifying requirements for acknowledgement of source material used by AI systems","Mandate declaration of AI-generated status for publicly visible AI work"],"risks_and_challenges":["Unauthorized use of copyrighted work by AI systems","AI-generated imagery closely resembling photographers\' styles","Use of copyrighted photography for facial recognition or social scoring without approval"],"safeguards_and_mitigations":["Restrict AI developers to using authorized data sources when training AI models","Implement transparency requirements for AI developers regarding data sets used","Disclose use of AI whenever resultant images are published or used"],"examples":{"Websites to check if imagery has been used to train AI":"https://haveibeentrained.com","Use of AI-based tools in photography post-production":"Adobe Photoshop\'s new AI based generative fill and replacement tools","AI tools experimented with by photographers":"DALL-E, Mid Journey, Runway, Stable Diffusion, Firefly, PhotoShop"},"international_alignment":"null","values":["Transparency","Copyright protection","Fair compensation"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Review and update copyright legislation, implement regulations for AI"},{"entity":"AI developers","role":"Ensure transparency and proper licensing of content used in AI models"},{"entity":"Photographers","role":"Protect their copyrighted work, adapt to AI technologies"}],"sector_impacts":[{"sector":"Photography and image-making industry","impact":"Potential threat to business, but also opportunities for improved productivity"}],"quotes":["Copyright and its enforcement are at the core of Image Makers Association Australia; in fact this topic is our primary reason for being.","It is clear from the surveyed responses above that the application of generative artificial intelligence computer systems to image making businesses raises many questions.","Government needs to review the application of existing copyright legislation in relation to this technology to ensure that these systems do not breach copyright during production of AI image-making through unauthorised data-mining and also in relation to the copyright status of any work that results from those systems."]},"208_P1480 No Responsible AI without transparency and accountability_final.83bf4415a9d94.pdf":{"organization_name":"The Australia Institute\'s Centre for Responsible Technology","organization_type":"Independent public policy think tank","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and oversight of AI technologies","arguments":["Current AI products lack transparency and are privately-owned black boxes","AI technologies need to be supported by regulation, effective accountability measures, and meaningful oversight","Strong regulation is essential for the most prominent AI products"],"counterarguments":[],"key_recommendations":["Require transparency from AI product owners and model owners on datasets used to train their AI","Adopt strong data privacy protections and include AI technologies in the Australian Privacy Act","Compensate copyright holders and owners of data used to train AI technologies","Implement a system of accountability for privately funded AI initiatives","Impose a moratorium on the most harmful applications of AI after developing a risk register"],"risks_and_challenges":["Lack of transparency in AI training datasets","Potential breaches of data privacy","Exploitation of creative rights and undermining the value of creative labor","Widespread political manipulation using AI features like deepfakes","Social scoring systems to discriminate against specific groups","Automation of military systems and weaponry"],"safeguards_and_mitigations":["Mandating transparency in data sets used to train AI","Applying privacy principles to AI products","Compensating original authors and creators for use of their work in AI training","Implementing a system of accountability for AI research and projects","Developing a risk register for harmful AI applications"],"examples":{"Google\'s update of terms and conditions":"Google quietly updated their terms and conditions to determine that any data captured by Google products can now be used to train Google\'s proprietary AI.","OpenAI\'s data sourcing practices":"OpenAI, the developer of Chat GPT, sourced data from BookCorpus (a repository of unpublished book manuscripts), many news websites, online forums and more.","Google\'s firing of AI ethics researcher":"Google fired prominent AI researcher Timnit Gebru, whose research highlighted the inequality built into large AI models."},"international_alignment":"null","values":["Transparency","Accountability","Privacy","Fair use","Responsible use"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations"},{"entity":"Technology companies","role":"Provide transparency and accountability for AI products"},{"entity":"Copyright holders and creators","role":"Receive compensation for use of their work in AI training"}],"sector_impacts":[{"sector":"Creative industries","impact":"Potential exploitation of creative rights and undermining of labor value"},{"sector":"News and publishing","impact":"Discussions on licensing content for AI training"}],"quotes":["No \'Responsible AI\' without transparency and accountability","Given that AI companies are generating revenue and profit from their products, there should be an investigation about what data they have scraped to develop their products, the terms under which it was used, and the effect that this may have on creators.","The role of government is to ensure the safety of its citizens. This cannot happen while the largest AI products remain secretive black boxes, run by Big Tech companies that are already too powerful."]},"368_NECWG-ANZ_Submission_Supporting_Responsible_AI-July_2023_v2.ae7a663cee5cd.pdf":{"organization_name":"National Emergency Communications Working Group \u2013 Australia and New Zealand (NECWG-A/NZ)","organization_type":"Professional leadership and advisory group","classification":"Proponent","overall_position":"Supports responsible AI regulation with specific considerations for emergency communications","arguments":["AI can enhance emergency communications services","AI systems can assist in managing increasing demand on emergency services","AI can help in quickly identifying critical requests and verifying facts"],"counterarguments":["Critical decision making should remain the domain of human officials","The emergency communications sector is traditionally risk-averse"],"key_recommendations":["Coordinate legislation globally for consistency","Include specific identification or exemptions for AI systems in public safety responses","Establish critical decision making by humans as a principle of legislation","Create local governance for managing AI use in exceptional circumstances","Consider staged implementation or light touch approach to legislation","Apply pragmatic approach to compliance requirements"],"risks_and_challenges":["Increasing demand on emergency services","Difficulty in assessing large volumes of data quickly","Potential conflicts with privacy and human rights"],"safeguards_and_mitigations":["Use AI for assistance purposes only","Implement framework for timely authorization of AI use in exceptional circumstances","Ensure transparency in the use of AI systems for public safety services"],"examples":{"AI in emergency services":"ESOs have been using forms of AI for many years including automatic number plate recognition, and algorithms to identify medical criticality.","AI for emergency call management":"The use of AI systems to assist in the management of demand on the Triple Zero service at both the initial contact point (ECP) and the response point (ESO) is expected to be beneficial in decision making and in presenting the most critical requests for assistance first.","AI for health emergencies":"Determination of potential heart-attack via voice analysis"},"international_alignment":"Supports global coordination of AI legislation","values":["Public safety","Transparency","Responsible use of technology"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Emergency Call Person (ECP)","role":"Initial contact point for emergency calls"},{"entity":"Emergency Service Organisations (ESOs)","role":"Response point for emergency calls"},{"entity":"Government","role":"Develop and implement AI legislation"}],"sector_impacts":[{"sector":"Emergency communications","impact":"Enhanced ability to manage increasing demand and identify critical requests"},{"sector":"Public safety","impact":"Improved response times and decision-making in emergency situations"}],"quotes":["A key consideration for emergency communications is that AI is to be used for assistance purposes, critical decision making should remain the domain of human officials.","The NECWG-A/NZ acknowledges the classification of AI systems as has been proposed by the European Union, with a caveat that some AI systems considered unacceptable may be beneficial when dealing with imminent threat to life.","Legislation, including terminology and definitions of AI, should be coordinated across the world to ensure consistency regardless of where or how the AI systems are developed and used."]},"454_AI Review 2023.3b91fd82ab974.pdf":{"organization_name":"The University of Queensland","organization_type":"Educational institution","classification":"Neutral","overall_position":"Favors minimal intervention and cautions against premature regulation","arguments":["AI is rapidly developing and current regulations may quickly become outdated","Over-regulation or early regulation may stifle innovation","Many current AI applications are benign and low-risk"],"counterarguments":["Some high-risk AI applications require significant care","Certain EU-identified requirements for AI use are good practice regardless of risk category"],"key_recommendations":["Make Commonwealth data available to Australian AI developers for training","Use AI for government decision-making and information provision","Implement a \'right to know\' when AI is being used and the right to opt-out"],"risks_and_challenges":["AI producing outputs that developers and users cannot explain","Difficulty in ensuring appropriate transparency in AI systems","Potential for contradictory rules from multiple jurisdictions"],"safeguards_and_mitigations":["Allow human review and intervention in AI-assisted decision-making","Implement requirements for notifying humans of AI interaction","Ensure robustness, accuracy, and cybersecurity in AI systems"],"examples":{"AI for meeting summaries":"I have seen AI used to prepare a meeting summary and a to-do list based on listening to a meeting.","AI for legal chronologies":"I have seen AI used to prepare chronologies from a bundle of documents.","AI for government decision-making":"For example, the parties to an AAT hearing could have the option of a Member make the decision within a year (such time is not uncommon for appeals regarding partner visa applications) or have AI make the decision within a day."},"international_alignment":"Cautions against contradictory rules from multiple jurisdictions","values":["Innovation","Transparency","Caution"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Government","role":"Provide data for AI training and implement AI in decision-making"},{"entity":"AI developers and businesses","role":"Use government data to train and improve AI technology"}],"sector_impacts":[{"sector":"Government","impact":"Potential for faster decision-making and improved information provision"},{"sector":"Legal","impact":"More efficient document processing and chronology creation"}],"quotes":["Care must be taken not to over-regulate or regulate too early. AI is rapidly developing, both in its ability and application.","Doing so would provide a vast repository of data to train AI, thus creating more accurate AI which would help develop trust in AI and advance the AI economy in Australia.","It is difficult to balance the need for regulation to protect Australians with over-regulation that may create unnecessary complexity and drive innovation offshore to the detriment of the Australian economy."]},"233_ASPI De-Risking Authoritarian AI.a396d939a9792.pdf":{"organization_name":"Australian Strategic Policy Institute","organization_type":"Think tank","classification":"Proponent","overall_position":"Supports a balanced approach to regulating AI from authoritarian countries, particularly China","arguments":["Chinese AI-enabled products pose security risks to democracies","Existing regulations are insufficient to address the unique challenges of authoritarian AI","A targeted approach is needed to identify and manage high-risk AI systems"],"counterarguments":["A general prohibition on all Chinese AI-enabled technology would be extremely costly and disruptive","Many businesses and researchers want to continue collaborating on Chinese AI-enabled products"],"key_recommendations":["Implement a three-step framework: Audit, Red Team, Regulate","Collaborate with other democracies on regulation efforts","Develop international standards that embed democratic digital norms"],"risks_and_challenges":["Espionage and sabotage through AI-enabled products and services","Foreign interference facilitated by AI technology","Emerging threats from large language model AI and generative AI systems"],"safeguards_and_mitigations":["Audit AI systems to identify those with concerning purposes and functionality","Use Red Team analysis to assess potential security risks","Implement targeted regulations based on risk assessment"],"examples":{"Chinese-made surveillance cameras in Australian government agencies":"Almost 1,000 Chinese-made surveillance cameras were installed across Australian Public Service agencies.","TikTok as a potential security threat":"TikTok reportedly harvests more data than other social-media apps, including detailed information about the user\'s location and other apps that they\'re running.","Chinese AI-enabled cranes in ports":"Shanghai Zhenhua Heavy Industries Co. Ltd (ZPMC) controls around 70% of the global market for cranes."},"international_alignment":"Supports collaboration among democracies on regulation efforts","values":["Security","Democratic values","Economic competitiveness"],"tone":"Cautionary","stakeholders":[{"entity":"Governments","role":"Implement regulations and safeguards"},{"entity":"Businesses","role":"Comply with regulations and assess risks of using Chinese AI technology"},{"entity":"Security agencies","role":"Provide expertise for risk assessment"}],"sector_impacts":[{"sector":"Critical infrastructure","impact":"Increased vulnerability to espionage and sabotage"},{"sector":"Technology","impact":"Potential limitations on collaboration and innovation"}],"quotes":["The policy goal here is to take prudent steps to protect our digital ecosystems, not to economically decouple from China.","Notwithstanding the honourable intentions of individual vendors of Chinese AI-enabled products and services, they\'re subject to direction from PRC security and intelligence agencies, so we in the democracies need to ask ourselves: against the background of growing strategic competition with China, how much risk are we willing to bear?","The democracies need to think harder about Chinese AI-enabled technology in our digital ecosystems. But we shouldn\'t overreact: our approach to regulation should be anxious but selective."]},"441_Guy Loucks Safe and Responsible AI for Australia.4cd827e7a17b4.pdf":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on ethical principles and co-regulatory framework","arguments":["AI systems should benefit individuals, society, and the environment","Human rights, diversity, and individual autonomy should be respected","Privacy rights and data protection should be upheld"],"counterarguments":["Current principles may not sufficiently address all concerns","Existing regulations may not fully cover AI-specific issues"],"key_recommendations":["Implement a three-party co-regulatory system with equal representation from industry/technology, consumers, and AI error/fraud victims","Establish a robust and transparent complaints and appeals mechanism","Create a reporting process similar to the United States FAA Aviation Safety Reporting System"],"risks_and_challenges":["Potential for AI systems to significantly impact individuals without their knowledge","Risk of deep fakes causing social, emotional, and physical damage","Possibility of obfuscation and procedural misfeasance in AI system contestability"],"safeguards_and_mitigations":["Require AI systems to identify themselves to humans","Implement human oversight of AI systems with regular monitoring and documentation","Establish clear accountability for different phases of the AI system lifecycle"],"examples":{"FAA Aviation Safety Reporting System":"Further, there should be a reporting process like that used by the United States FAA Aviation Safety Reporting System (ASRS), whereby there is no penalty for reporting, and equally no protection for failure to report."},"international_alignment":"null","values":["Human-centered values","Privacy protection","Transparency and explainability","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Industry / technology","role":"Equal party in co-regulatory system"},{"entity":"Consumers","role":"Equal party in co-regulatory system, representing the average person"},{"entity":"Victims of AI error/fraud","role":"Equal party in co-regulatory system"},{"entity":"Government","role":"Chair to break stalemates in the co-regulatory system"}],"sector_impacts":[{"sector":"null","impact":"null"}],"quotes":["AI systems should benefit individuals, society and the environment.","The burden must be on the AI to identify that they are an AI to ensure all humans (regardless of their capacity/culture etc) are fully aware that they are dealing with an AI.","A robust and transparent complaints and appeals mechanism would also be required and should follow a similar structure."]},"283_Tech Global Institute - Submission on the Safe and Responsible AI in Australia.6b8d30706fad6.pdf":{"organization_name":"Tech Global Institute","organization_type":"Global policy lab","classification":"Proponent","overall_position":"Supports comprehensive AI regulation with focus on updating existing laws and introducing new AI-specific regulations","arguments":["Existing regulatory frameworks are inadequate for addressing AI risks","AI can disproportionately harm underrepresented communities","A rights-based approach is necessary for responsible AI development"],"counterarguments":["Internet exceptionalism has dissuaded governments from proactively regulating cyberspace","Existing legal constructs were thought to be flexible enough to counteract internet abuses","Some advocates claim AI regulation by the government will slow innovation"],"key_recommendations":["Strengthen existing laws on competition, consumer rights, and data protection","Introduce new AI-specific regulations","Adopt human rights assessments and audits throughout the lifecycle of AI","Implement mandatory design guidelines for AI development and deployment","Establish a risk-based approach complemented by public impact assessments"],"risks_and_challenges":["AI can amplify biases and discriminate against marginalized groups","Privacy concerns related to data collection and use","Potential for AI to be used for harmful purposes, such as child exploitation","Challenges in ensuring AI systems are inclusive and accessible for people with disabilities"],"safeguards_and_mitigations":["Incorporate human rights considerations in AI design and development","Implement safety by design and privacy by design principles","Mandate privacy-preserving access to data for researchers and independent auditors","Regular audits and impact assessments of AI systems"],"examples":{"Discriminatory AI in criminal justice":"Criminal risk-assessment software widely adopted in the U.S. criminal justice system was inaccurate in forecasting potential future crimes and heavily biased against black defendants, implicating the right to liberty and security and to a fair trial.","Biased image recognition":"Google\'s image recognition algorithms, for instance, accidentally categorised two black persons in a photo as gorillas and Amazon\'s algorithm appeared to discriminate against women, contrary to principles of equality and non-discrimination.","AI in social media content moderation":"AI-enabled content moderation on social media will primarily impact freedom of expression and right to information, but also freedom of religion if it is used to identify and remove religious content, press freedom if it targets news content, and freedom of association when it results in the removal of online groups, pages and content that facilitate the gathering of individuals."},"international_alignment":"Supports global collaboration and standards while emphasizing national implementation","values":["Human rights","Equity","Accountability","Transparency","Inclusiveness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, collaborate internationally"},{"entity":"Tech companies","role":"Implement ethical AI practices, conduct impact assessments"},{"entity":"Civil society and academia","role":"Contribute to policy development, conduct independent audits"}],"sector_impacts":[{"sector":"Criminal justice","impact":"Potential for biased outcomes in risk assessments"},{"sector":"Healthcare","impact":"Possible discrimination in treatment recommendations based on insurance status or income"},{"sector":"Employment","impact":"Risk of discrimination in hiring processes using AI"}],"quotes":["We believe that strengthening the existing laws on competition, consumer rights and data protection, complemented by a new AI regulation, can ensure the regulatory framework is future-proof, technology-neutral, and fit-for-purpose, and can address foreseeable harms associated with new technologies.","To be effective, laws will have to be proactive, and adopt a novel approach, combining data protection, consumer rights, competition, anti-discrimination and online safety legislations, within the broader architecture of a human-centric and rights-focused AI regulation.","Through updating its regulations and policies to address AI risks, the Australian Government has the unique advantage of leading the world in the current and next phases of AI development, ensuring risks are mitigated early on and all parts of society can equitably reap its benefits."]},"448_Embracing Human AI Partnerships.pdf":{"organization_name":"Department of Industry, Science, and Resources","organization_type":"Government department","classification":"Proponent","overall_position":"Advocates for comprehensive regulation through a government-hosted AI platform","arguments":["Government-hosted AI platform can meet the need for safe ethical AI development","Collaborative ecosystem can effect change and harness AI benefits for all","MNN architecture provides inherent security and efficient data exchange"],"counterarguments":["Open network architecture may expose sensitive intellectual property","Navigating contributions in open networks can be overwhelming without proper curation"],"key_recommendations":["Develop a government-hosted Modular Neural Network (MNN) platform","Implement AI-supported self-service applications","Incorporate risk analysis and mitigation principles","Establish cross-disciplinary teams for AI development","Promote AI literacy and educational initiatives"],"risks_and_challenges":["Potential for exploitation in open network architecture","Task displacement and resource realignment","Unpredictability in AI development"],"safeguards_and_mitigations":["Inherent security built into MNN architecture","Blockchain methodology for data security","Incorporation of safety and ethics protocols","Regular evaluations and audits of the system"],"examples":{"Microsoft Windows growth":"This evolution has precedence. It can by prediction, be correlated to the expansive growth of the Microsoft Windows operating system, which shaped personal computing worldwide","Cambridge Analytica scandals":"There are risks to open architecture, as proven by the Cambridge Analytica scandals of 2016"},"international_alignment":"Supports collaboration with international partners","values":["Safety","Ethics","Transparency","Collaboration","Innovation"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Develop and host AI platform, establish regulatory framework"},{"entity":"Industry partners","role":"Collaborate in AI development and customization"},{"entity":"Research institutions","role":"Contribute to ongoing research and development"}],"sector_impacts":[{"sector":"All industries","impact":"Enhanced productivity and efficiency through AI integration"},{"sector":"Economy","impact":"Boost to Gross Domestic Product (GDP)"}],"quotes":["The Australian Government is the only societal organisation able to achieve, holistic coverage, and collaboration necessary, to keep AI safe and ethical","Embracing collaborative efforts in developing a government-hosted AI platform is the gateway to a future of safe, ethical, and efficient AI applications.","We have technology where the needs of many, may not necessarily out way the needs of the few. Instead, the needs of all can be met, without detriment to any"]},"185_Vardi Safe and responsible AI Submission 250623.3479f41e8b025.pdf":{"organization_name":"null","organization_type":"Individual submission","classification":"Proponent","overall_position":"Advocates for comprehensive regulation","arguments":["Current regulatory approaches are reactive and insufficient","Uncontrolled release of AI technologies poses significant risks","Voluntary codes and self-regulation do not provide an even playing field"],"counterarguments":["null"],"key_recommendations":["Apply a risk-based approach to the entire AI lifecycle","Mandate regulation for AI development, deployment, and use","Implement proactive regulatory controls for generative AI research and release","Educate users on AI limitations and empower them to question outputs","Protect knowledge bases and ensure free access to information sources"],"risks_and_challenges":["Premature release of AI technologies leading to unintended consequences","Misinformation generated by AI systems","Excessive trust in AI outputs","Deskilling of the population","Increased distance between businesses and clients"],"safeguards_and_mitigations":["Implement ethical standards for AI research involving human interaction","Require thorough regulatory controls and prescribed standards prior to AI release","Mandate clear declarations of AI use in documents and websites","Ensure human oversight and the ability to challenge AI decisions","Retain and develop human skills to evaluate AI outputs"],"examples":{"ChatGPT release":"The uncontrolled release of ChatGPT led to significant real-world problems and misuse.","Robodebt scheme":"The Robodebt scheme demonstrated issues with automated decision-making and trust in AI systems.","Google\'s book digitization project":"Google\'s attempt to digitize books raised concerns about control over knowledge and information."},"international_alignment":"Supports global standards","values":["Responsibility","Accountability","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"AI companies","role":"Develop safe and responsible AI technologies"},{"entity":"Government","role":"Implement and enforce comprehensive AI regulations"},{"entity":"Educational institutions","role":"Adapt to AI technologies while maintaining educational integrity"},{"entity":"Businesses","role":"Implement AI responsibly and maintain human oversight"}],"sector_impacts":[{"sector":"Education","impact":"Need to reconsider assessments and integrate AI constructively"},{"sector":"Healthcare","impact":"Potential for improved efficiency but risks to patient confidentiality"},{"sector":"Legal","impact":"Time-saving benefits but risks of relying on inaccurate AI-generated information"}],"quotes":["It is essential that generative AI is not released widely in the research phase and that the products are subject to thorough regulatory controls with prescribed conditions and standards prior to any release","We must proactively protect not only our information, but also our sources and our knowledge base, and ensure free access to that knowledge and information.","Regulation must be mandated. Despite the multiple problems seen with the premature release of ChatGPT (see attached case study), Microsoft, the main financial backer of OpenAI, is rushing out prematurely promoting and releasing LLMs in Bing, in schools and the like."]},"367_Free TV Submission - Safe and responsible AI in Australia July 2023.25778c217a729.pdf":{"organization_name":"Free TV Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a legislated risk-based approach to regulating AI","arguments":["An economy-wide approach is required to ensure all AI use cases apply a common risk-based approach","Current sector-by-sector approach may fail to capture emerging use cases","Transparency is crucial for AI regulation","Content creators should retain control over their content used for AI training"],"counterarguments":[],"key_recommendations":["Establish a principles-based risk framework focusing on transparency","Require risk assessments and clear notices to consumers on AI use","Ensure content creators retain control over terms of access and use for AI training","Implement commercial terms for use of professionally produced content in AI models","Task the ACCC with a market study on AI\'s impact on competition and content creation sectors"],"risks_and_challenges":["Rapid emergence of dominant AI platforms operated by transnational corporations","Potential competition issues similar to existing dominant digital platforms","Use of professionally produced content without fair compensation","AI-enhanced fraudulent activity and scams"],"safeguards_and_mitigations":["Transparent disclosure of AI use and data sources","Right for content owners to refuse access to individual AI tools","Categorizing AI information gateways as \'medium risk\' applications","Proactive ACCC market study on competitive effects of AI"],"examples":{"AI-powered search results using professionally produced content":"Microsoft\'s Bing product is already drawing on professionally produced journalistic content, including content created by Free TV members, in responding to user queries","Scams using media personalities":"network celebrities are fraudulently used in social media advertising purporting endorsement of products and services without the knowledge or authorisation of the network or individual involved"},"international_alignment":"Supports alignment with approaches in EU and Canada","values":["Transparency","Fairness","Sustainability of content creation"],"tone":"Cautionary","stakeholders":[{"entity":"Content creators","role":"Retain control over content used for AI training"},{"entity":"AI platform operators","role":"Transparently disclose AI use and data sources"},{"entity":"ACCC","role":"Conduct market study on AI\'s competitive impacts"}],"sector_impacts":[{"sector":"Media","impact":"Potential disintermediation and threat to sustainable content creation"},{"sector":"News and journalism","impact":"Risk of AI tools becoming primary gateways for information access"}],"quotes":["Free TV supports a legislated risk-based approach to regulating AI, as is being pursued in jurisdictions such as the EU and Canada.","Language models or other generative AI are increasingly becoming a gateway for Australians to access information, in much the same way that internet search and social media feeds are significant gateways for Australians to access information today.","It is critical that the framework also establish that the creators and owners of content that may be used by AI models for training purposes retain control over the terms on which this content is accessed and used."]},"507_Submission 507 - Gillespie and Lockey - 22-Aug.f1b3585e472e8.pdf":{"organization_name":"The University of Queensland","organization_type":"Academic institution","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and governance of AI","arguments":["Only a third of Australians are willing to trust AI systems","The public has high expectations for AI governance and regulation","Current regulations are perceived as insufficient by the majority of Australians"],"counterarguments":[],"key_recommendations":["Develop a clear, consistent, risk-based regulatory framework for governing AI","Align Australia\'s regulatory approach with international standards","Invest in public education on AI regulations and laws","Establish an independent AI Commission","Implement a consistent AI Assurance Framework across government","Invest in upskilling people to understand and use AI responsibly","Ensure transparency when AI is used in decision-making or interactions"],"risks_and_challenges":["Cybersecurity and privacy breaches","Loss of jobs and deskilling","Manipulation and harmful use of AI","System failures","Erosion of human rights","Inaccurate or biased outcomes"],"safeguards_and_mitigations":["Risk-based approach to regulation","Human review and oversight for moderate to high-risk AI applications","AI impact assessments","Transparent notification of AI use","Accessible explanations of AI outputs","Ongoing monitoring and reporting of AI performance"],"examples":{"NSW AI Assurance Framework":"Our case study research suggests that the NSW AI Assurance Framework has been effective in supporting, guiding, and incentivising government agencies to design, develop, and deploy AI in a trustworthy way."},"international_alignment":"Supports global collaborative approaches and adoption of international standards","values":["Accountability","Contestability","Fairness","Transparency","Privacy","Technical performance","Human agency","Risk mitigation"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, implement AI Assurance Framework"},{"entity":"Industry","role":"Adhere to regulatory and governance frameworks"},{"entity":"Independent AI Commission","role":"Provide expert advice and support to government, regulators, and industry"},{"entity":"Public","role":"Engage in AI education and understanding"}],"sector_impacts":[{"sector":"Government","impact":"Need to build data and AI capability and expertise"},{"sector":"SMEs, nonprofits, and local government","impact":"Will need support to adhere to AI regulations and governance frameworks"}],"quotes":["Our research demonstrates that a strong predictor of people\'s trust in AI systems is the belief that there are adequate regulations, laws, and safeguards to make AI use safe.","Australians have a clear preference for AI to be regulated by government and existing regulators, or by an independent AI body, rather than by industry.","Our research shows that 75% of people are more willing to trust AI applications when assurance mechanisms are in place that demonstrate the responsible deployment of AI systems."]},"61_UK lessons.b682b0f51fd74.pdf":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and responsible implementation of AI in safety-critical systems","arguments":["AI still has limitations and can introduce new errors","Failure modes of AI can be unpredictable and surprising","AI maintenance is crucial for continued safe operation","System-level implications of AI implementation need to be considered"],"counterarguments":["The AV industry claim that removing human drivers will automatically improve safety is misleading","AI\'s probabilistic reasoning does not equate to human judgment under uncertainty"],"key_recommendations":["Implement government regulation and clear industry standards","Conduct extensive testing in real-world conditions","Regulatory agencies should set reasonable operating boundaries for AI systems","Have more informed conversations about AI regulation with technically competent people"],"risks_and_challenges":["Human errors in coding replacing human errors in operation","Unpredictable failure modes in AI systems","AI\'s inability to make judgments under significant uncertainty","Model drift and maintenance issues","System-level implications of AI implementation"],"safeguards_and_mitigations":["Extensive testing in simulation and real-world conditions","Constant updating of AI models to maintain relevance","Understanding and addressing system-level implications of AI implementation"],"examples":{"Pony driverless car crash":"Pony driverless car crash in October 2021 into a sign","TuSimple semi-tractor trailer crash":"TuSimple semi-tractor trailer crash in April of 2022 into a highway jersey barrier","Cruise AV sudden stop":"Cruise crash caused by a sudden stop in the middle of an unprotected left turn in June of 2022"},"international_alignment":"null","values":["Safety","Accountability","Responsible implementation"],"tone":"Cautionary","stakeholders":[{"entity":"Government regulatory agencies","role":"Set limits and issue permits and regulations for AI systems"},{"entity":"AI companies","role":"Ensure responsible development and implementation of AI systems"},{"entity":"Standards bodies","role":"Define reasonable operating boundaries for AI systems"}],"sector_impacts":[{"sector":"Transportation","impact":"Potential safety improvements but also new challenges in traffic management and emergency response"}],"quotes":["These five lessons learned show that AI still has a long way to go before it can be a considered a success in vehicles. There are clear benefits to this technology if it can be introduced responsibly, but we need to have more informed conversations about such regulation with people that have technical competence in AI.","Without government regulation and clear industry standards, AI companies will cut corners and focus on getting products to market quickly, at the expense of reducing software errors.","As other kinds of AI begin to infiltrate other areas of society through generative AI, it is extremely important for standards bodies and regulators to be aware that AI failure modes will not follow a predictable path."]},"230_DRW - Safe and Responsible AI - July 2023.cd1572b4ac22b.pdf":{"organization_name":"Digital Rights Watch","organization_type":"Charity organisation","classification":"Proponent","overall_position":"Advocates for comprehensive regulation including new AI-specific laws and strengthening existing laws","arguments":["AI technologies present significant challenges to privacy and digital security","AI can result in biased, discriminatory or other harmful outcomes","Current voluntary mechanisms and ethics frameworks are insufficient"],"counterarguments":["Robust regulation that places human rights and safety at the centre is not a threat to technological innovation","We should not allow laws and policy to be shaped by AI Industry leaders for their own purposes"],"key_recommendations":["Enact a comprehensive federal Human Rights Act and a Charter of Digital Rights and Principles","Develop an AI-specific law that adopts an approach in line with the EU\'s new AI Act","Establish a Joint Standing Committee on Digital Affairs","Comprehensively reform the Privacy Act 1988","Strictly limit the use of facial recognition and biometric surveillance technologies"],"risks_and_challenges":["AI systems can deliver harm at immense speed and scale, in ways that are not always predictable","AI can exacerbate existing discrimination and disadvantage","Lack of representation from affected communities in AI development","Privacy threats via over\ufb01tting in AI models"],"safeguards_and_mitigations":["Implement a sophisticated risk-based approach to AI regulation","Establish an \'unacceptable risk\' or \'no-go zone\' category for AI systems","Tighten restrictions on secondary use of personal information","Introduce individual rights regarding AI and automated decision-making"],"examples":{"COMPAS risk assessment tool":"Perhaps one of the most egregious and well-known examples of harm caused by an algorithmic system is the commercially-available risk assessment tool called COMPAS, used in the US to predict recidivism in applications for parole and to assess a criminal defendant\'s future likelihood of committing a crime. This tool was found to be racially biased\u2014inaccurately predicting that Black defendants were twice as likely to reoffend than white defendants","Amazon Ring illegal surveillance":"In May 2023 the US Federal Trade Commission ruled that Amazon Ring illegally surveilled customers and proposed an order that would prohibit Ring from pro\ufb01ting from unlawfully collected data.","Apple Card gender discrimination":"In 2019 Apple was accused of discrimination after offering a lower credit limit to a woman compared to a man with a similar credit rating."},"international_alignment":"Supports functional consistency with leading international instruments","values":["Human rights","Privacy","Safety","Fairness","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations"},{"entity":"AI Industry","role":"Implement ethical AI practices and comply with regulations"},{"entity":"Affected communities","role":"Participate in AI development and governance"}],"sector_impacts":[{"sector":"Law enforcement","impact":"Higher risk category due to potential severity of consequences"},{"sector":"Healthcare","impact":"Potential for immense public good, but also considered higher risk"},{"sector":"Government services","impact":"Higher risk category due to critical nature of services"}],"quotes":["Digital Rights Watch (DRW) welcomes the opportunity to submit comments to the Department of Industry, Science and Resources regarding the Safe and Responsible AI in Australia Discussion Paper (the Discussion Paper). We are pleased to note the Australian government\'s willingness to consider governance mechanisms to ensure AI is developed and used safely and responsibly in Australia, including the consideration of regulations, standards, tools, frameworks, principles and business practices.","We need not look to far-future hypothetical scenarios to understand the ways in which AI can cause harm: it is already happening. There is over a decade of case studies from around the world, research, analysis and recommendations to draw from. More than ever before, Australia is in a position to move from identifying problems and toward taking steps to remediate and mitigate them.","In order to meaningfully address AI\u2014the technology itself, the data it generates and uses, and the power and in\ufb02uence of the broader AI industry\u2014we will need a combination of regulatory measures."]},"186_Safe and Responsible AI in Australia - Submission - Dr Francina Cantatore.489fe100215e6.pdf":{"organization_name":"Bond University","organization_type":"Academic institution","classification":"Proponent","overall_position":"Supports a risk-based approach with national regulation for responsible AI use","arguments":["A risk-based approach is aligned with international trends","Existing frameworks in different sectors can inform the approach","National regulation is necessary for consistency across sectors"],"counterarguments":["Different Australian sectors are subject to diverse regulatory structures","Approaches in other jurisdictions may not be directly applicable due to different regulatory structures","Some risks may not be scalable across all sectors"],"key_recommendations":["Implement national regulation for both public and private organizations","Include \'Public Education\' as an element in the risk-based approach","Establish a national taskforce involving all sectors to advise on integration of AI risk-based approach"],"risks_and_challenges":["Algorithmic bias","Impact on human employability and economic viability","Lack of consistency between enforcement practices in relation to privacy and data management"],"safeguards_and_mitigations":["Robust ethical and human rights centred framework","Mandatory AI Policy for legal service providers","Clear regulatory guidelines for IP rights in AI-generated creations"],"examples":{"Legal Profession":"The Queensland Law Society has issued a useful AI Policy template to its members which provides guidance on the appropriate use of generative AI technologies in our law firms.","IP Rights Management":"Google\'s \'Deep Dream Generator,\' which is labelled as a Human/AI collaboration, illustrate the potential for conflicting rights claims and the need for clear regulatory guidelines","Consumer Data Rights":"The approach in Consumer Data Right regulation of data now aligns the Australian approach with the European framework in relation to data rights, allowing Australian consumers to control their data in the marketplace."},"international_alignment":"Supports drawing on international developments while considering Australian context","values":["Consumer protection","Transparency","Human oversight"],"tone":"Cautionary but supportive","stakeholders":[{"entity":"Government","role":"Implement national regulation and public education"},{"entity":"Developers","role":"Comply with stricter regulation and transparency requirements"},{"entity":"Deployers","role":"Follow industry-specific regulation and self-regulation"}],"sector_impacts":[{"sector":"Legal profession","impact":"Need for mandatory AI Policy and national standards"},{"sector":"Health industry","impact":"High risk of potential harm requiring strict regulation"}],"quotes":["I support a risk-based approach in principle provided it has national application and is underpinned by a robust ethical and human rights centred framework, which will be enforceable across sectors.","A risk-based approach for responsible AI should be applied through national regulation in the case of both public and private organisations, especially in relation to developers.","As there is currently a lack of empirical data on the impact of AI application, self-reporting measures may assist in informing the longitudinal study of the effects of AI technology; however, risk should be mitigated in favour of the consumer."]},"480_Submission 480 - elevenM - 10-Aug.5f1a8e9989add.pdf":{"organization_name":"elevenM","organization_type":"Specialist privacy and cyber security consultancy","classification":"Proponent","overall_position":"Advocates for comprehensive regulation","arguments":["Trust is critical for sustained digital innovation and AI adoption","Regulation should apply to public and private organizations and address all sources of risk across the AI supply chain","Self-regulation or limited focus on certain AI supply chain participants will not deliver the consistency and reliability required to build public trust"],"counterarguments":["null"],"key_recommendations":["Implement a risk-based regulatory approach","Establish a well-resourced and active regulator","Ensure consistency with emerging international standards","Require transparency and explainability in AI systems"],"risks_and_challenges":["Discriminatory, arbitrary or unfair algorithmic decisions","Cumulative and systemic risks from widespread AI adoption","Environmental risks from model training","Labour and human rights risks from data labelling"],"safeguards_and_mitigations":["Mandatory risk assessments for AI systems","Transparency requirements for AI developers and deployers","Right to opt-out of automated decision-making processes","Registration and approval for very high-risk AI applications"],"examples":{"Clearview AI case":"The Clearview AI case demonstrates that aside from a requirement to complete a PIA, the Privacy Act ultimately sets no concrete guardrails in relation to law enforcement\'s use of facial recognition.","Facial recognition in retail":"Recent reports from CHOICE have shown that facial recognition is being deployed at scale within the private sector as well, often with minimal transparency and few controls."},"international_alignment":"Supports global interoperability and alignment with definitions adopted in key economic markets","values":["Trust","Transparency","Human rights","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement regulatory settings that drive responsible innovation"},{"entity":"Businesses","role":"Comply with AI regulations and implement responsible AI practices"},{"entity":"Regulator","role":"Oversee and maintain the regulatory regime"}],"sector_impacts":[{"sector":"Law enforcement","impact":"Potential misuse of facial recognition technology"},{"sector":"Retail","impact":"Deployment of facial recognition with minimal transparency"}],"quotes":["At elevenM, we believe that trust is a critical dependency for sustained digital innovation, and the establishment and maintenance of trust should be a guiding star for technology policy.","AI regulation should be focused on building trust in AI systems by managing risk and ensuring AI development and use is well aligned to community values and expectations.","Neither self-regulation nor limited focus on certain AI supply chain participants will deliver the consistency and reliability of outcome required to build public trust and drive adoption of new AI technologies."]},"409_Pages from 4 August -Safe and Responsible AI in Australia_edited.da6345a602703.pdf":{"organization_name":"The University of Sydney","organization_type":"Academic institution","classification":"Neutral","overall_position":"Supports addressing AI challenges through policy deliberation and suitable regulations","arguments":["Rapid developments in AI present enormous social, economic, political, cultural and ethical challenges","Early stage policy deliberation is necessary to address questions of social good","Suitable regulations are needed to meet AI challenges"],"counterarguments":[],"key_recommendations":[],"risks_and_challenges":["Social challenges","Economic challenges","Political challenges","Cultural challenges","Ethical challenges"],"safeguards_and_mitigations":[],"examples":{},"international_alignment":"null","values":["Social good","Responsibility","Collaboration"],"tone":"Positive","stakeholders":[{"entity":"Academic researchers","role":"Contribute to policy deliberation process"},{"entity":"Department of Industry, Science and Resources","role":"Facilitate discussion on safe and responsible AI"}],"sector_impacts":[],"quotes":["In light of the enormous social, economic, political, cultural and ethical challenges presented by rapid developments in artificial intelligence (AI), and particularly generative artificial intelligence, the opportunity to participate in a policy deliberation process that aims to address questions of the social good at an early stage, and to design suitable regulations to meet such challenges, is very much welcomed.","Our submission is a collaborative enterprise between academic researchers in the Disciplines of Media and Communications and Government and International Relations in the Faculty of Arts and Social Sciences at The University of Sydney.","It is a collectively authored document that has arisen out of collaborative discussions among a diverse group of researchers with a shared interest in the digital, and a shared focus upon the common good."]},"474_Submission 474 - Centre for Media Transition - 8-Aug.272c9ecbe46a4.pdf":{"organization_name":"Centre for Media Transition","organization_type":"Academic research unit","classification":"Neutral","overall_position":"Supports self- and co-regulatory approaches with some adjustments for AI risks in journalism","arguments":["Existing self- and co-regulatory frameworks are likely adequate to manage AI risks in journalism","Maintaining self- and co-regulatory approach is critical to preserving press freedom","Industry should be encouraged to reassess codes and develop AI-specific guidelines"],"counterarguments":["Complaints-based accountability system may not provide strongest protection against careless or unethical journalistic practices","Existing codes may be inadequate in areas like transparency about AI-generated content"],"key_recommendations":["Encourage industry to reassess codes and develop AI-specific guidelines","Implement accountability measures across the AI system lifecycle","Require AI developers and vendors to certify tools against independent standards","Hold digital platforms responsible for implementing safeguards against AI-assisted manipulation"],"risks_and_challenges":["Risks to editorial practice and ethics","Risks to the journalism industry","Risks to the information environment","Potential proliferation of misinformation and fake or low-quality news"],"safeguards_and_mitigations":["Robust internal governance processes in newsrooms","AI-specific verification or fact-checking procedures with human oversight","Transparency about the use of AI-generated content","Certification of AI tools against independent standards"],"examples":{"News Corp AI usage":"News Corp, that a team of four staff use the technology to generate some three thousand stories each week on weather, fuel prices and traffic conditions.","BBC AI adoption":"The BBC is using AI tools to streamline workflows.","Reuters AI tool":"Reuters deploys a tool known as News Tracer to track breaking news on social media and publications in locations where Reuters has no presence."},"international_alignment":"null","values":["Accountability","Transparency","Press freedom"],"tone":"Cautionary","stakeholders":[{"entity":"News businesses","role":"Manage risks to editorial processes and integrity of news"},{"entity":"AI developers and vendors","role":"Ensure transparency of AI capabilities and limitations"},{"entity":"Digital platform companies","role":"Implement safeguards against AI-assisted manipulation"},{"entity":"Government","role":"Consider AI impact alongside current policy focus on mis- and disinformation"}],"sector_impacts":[{"sector":"Journalism","impact":"Potential efficiency gains and risks to editorial processes and industry sustainability"},{"sector":"Information environment","impact":"Potential proliferation of misinformation and low-quality content"}],"quotes":["Ultimately, we consider that existing self-regulatory mechanisms, with some adjustment, are likely to be the best means of managing AI risks in journalism.","The key to managing and mitigating risk is accountability.","Because the public has an interest in upholding professional journalistic standards, it has an interest in protecting public-interest journalism from any risks arising from AI."]},"388_Scarlet Alliance Submission Responsible AI Australia 2023 (1).736fa216fcad4.pdf":{"organization_name":"Scarlet Alliance","organization_type":"National peak sex worker organization","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI and ADM technologies to protect marginalized communities, particularly sex workers","arguments":["AI and ADM technologies risk perpetuating and entrenching systemic discrimination","Current regulatory frameworks are insufficient to address the unique challenges posed by AI and ADM","Sex workers are disproportionately affected by algorithmic bias and digital discrimination"],"counterarguments":["null"],"key_recommendations":["Establish an AI and ADM-specific independent government regulator","Implement clear and enforceable limits on the use of personal information and images in AI/ADM models","Develop a compulsory ethics-based accreditation for AI/ADM developers in Australia","Resource sex worker and other marginalized community organizations to provide education on AI/ADM technologies"],"risks_and_challenges":["Algorithmic bias leading to shadow-banning and loss of access to online platforms","Financial discrimination through ADM in banking and financial services","Privacy and data security concerns, particularly for stigmatized communities","Potential for AI to regenerate or \'unobscure\' personal information and images"],"safeguards_and_mitigations":["Targeted education for private sector actors on anti-discrimination compliance","Strict limits or bans on public and private sector use of facial recognition and biometric surveillance","Allowing for anonymous, pseudonymous, and representative complaints to AI/ADM regulators"],"examples":{"Shadow-banning of sex worker content":"Posts sharing sex worker COVID-19 safety plans referenced on government websites being deemed as breaching community guidelines","Financial discrimination":"Anecdotal evidence also points to the data of sex workers and their associates being extensively shared between private entities, leading to denials of service (solely on the basis of alleged sex work status) from a diverse range of platforms, including DoorDash and AirBnB","Privacy concerns":"There are reports that Meta\'s new platform Threads has published the full legal names of people operating sex worker accounts without permission"},"international_alignment":"null","values":["Fairness","Equity","Inclusion","Privacy","Data autonomy"],"tone":"Cautionary","stakeholders":[{"entity":"AI and ADM-specific independent government regulator","role":"Oversee AI/ADM technologies and handle complaints"},{"entity":"Sex worker peer organizations","role":"Provide education on AI/ADM technologies to their communities"},{"entity":"AI/ADM developers","role":"Obtain ethics-based accreditation and ensure compliance with anti-discrimination laws"}],"sector_impacts":[{"sector":"Financial services","impact":"Risk of entrenching and increasing financial discrimination against sex workers and other marginalized populations"},{"sector":"Social media and online platforms","impact":"Potential loss of access and income for sex workers due to algorithmic bias"}],"quotes":["Scarlet Alliance believes that regulation and governance of AI and ADM technologies must adequately identify and address systemic bias before technologies are implemented.","Any governance framework for AI/ADM must provide clear and enforceable limits on the use of personal information and images in AI/ADM models, both as data inputs and using these technologies to reaggregate or generate personal information or images relating to specific individuals.","The increasing role of private entities in making \'significant\' decisions can be seen in the context of the increasing role of digital technologies in the everyday lives of people in Australia."]},"265_Safe and responsible AI in Australia - PwC Response [Submission].da38e0971b4bf.pdf":{"organization_name":"PwC","organization_type":"Consulting company","classification":"Proponent","overall_position":"Supports new AI-specific governance measures with a risk-based approach","arguments":["AI accountability measures help provide clarity of expectations and balance innovation with protection","A risk-based approach for classifying AI systems enables better targeting of governance efforts","Mandatory AI governance principles with independently assured management reporting could accelerate AI adoption"],"counterarguments":["Prescriptive transparency disclosures for AI systems may be challenging due to varied use cases","Strict transparency requirements could make some AI branches de facto prohibited for all use cases"],"key_recommendations":["Adopt a risk-based approach for classifying AI systems","Implement binding AI governance principles","Introduce management reporting for compliance against AI governance principles","Establish independent assurance requirements for higher risk AI systems"],"risks_and_challenges":["Gaps in knowledge of how AI works and associated risks","Difficulty in interrogating and explaining AI outputs","Unclear expectations for accountability and liability in complex AI value chains","Low maturity in navigating AI risks among Australian businesses"],"safeguards_and_mitigations":["Develop an AI accountability framework modeled on financial reporting ecosystems","Implement management reporting and independent assurance for higher risk AI systems","Adopt a principles-based approach for transparency","Provide sector-specific classification guidance for consistent interpretation of regulations"],"examples":{"Financial reporting ecosystem as a model":"We recommend that policy makers look to the financial reporting ecosystem as the gold standard in ensuring the reliability of, and market confidence in, company-specific information.","Monte Carlo simulations in financial forecasting":"For example, Monte Carlo simulations that aid in financial forecasting could be interpreted as \'in scope\' under the proposed definition.","AI system for cloud computing spend anomaly detection":"For example, an AI system that detects anomalies in cloud computing spend."},"international_alignment":"Supports global harmonization of AI definitions and standards","values":["Transparency","Accountability","Trust","Innovation"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Regulatory body","role":"Design, operationalize, and enforce AI accountability measures"},{"entity":"Organizations","role":"Implement AI governance principles and comply with reporting requirements"},{"entity":"Independent assurers","role":"Provide assurance on management\'s assertion of compliance with AI framework"}],"sector_impacts":[{"sector":"SMEs","impact":"Potential compliance burden if not properly managed"},{"sector":"Financial services","impact":"Increased scrutiny on AI systems used for lending and insurance decisions"}],"quotes":["We believe that a substantial and timely uplift in organisations\' maturity and public trust in AI will be necessary for Australia to unlock the full benefit of AI-enabled transformations.","Today, less than 1 in 20 Australian businesses are well-versed (mature) in navigating the risks of AI, fewer than 4 in 10 consumers trust AI and only half of employees feel positively towards it.","We believe that the defining criteria for AI regulation should go beyond how it is developed and functions, and should also address the way that an AI system is being applied."]},"340_202306 FSC Submission - Safe and Responsible AI in Australia.93e2c1c5b1449.pdf":{"organization_name":"Financial Services Council","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with focus on encouraging innovation and protecting consumers","arguments":["Risk-based approach allows low-risk AI to be utilized relatively unfettered","Principles-based approach creates certainty and encourages innovation","Regulation should be scalable to capture future innovations"],"counterarguments":["Blanket bans on AI technologies would hinder innovation and competitiveness","Prescriptive regulation might not account for both impact and likelihood of risks","Individual agency enforcement could lead to under-regulation or over-regulation"],"key_recommendations":["Consider AI governance holistically within existing regulatory frameworks","Adopt principles focusing on risk-based regulation, innovation, consumer protection, and public trust","Implement a voluntary regulatory sandbox for AI development","Provide clarity on R&D tax incentives for AI development","Create a risk-based governance framework that is easy to comprehend and scalable"],"risks_and_challenges":["Potential for fraud, scams, and cybersecurity threats","Difficulty in checking and confirming identity due to AI forgeries","Balancing transparency requirements with intellectual property protection","Rapidly evolving technology outpacing regulatory frameworks"],"safeguards_and_mitigations":["Disclosure obligations for AI-based decision-making","Guidelines for human oversight in AI systems","Requirements for explanations of unfavorable AI-based decisions","Minimum requirements for monitoring and documentation of medium and high-risk AI uses"],"examples":{"AI use cases in financial services":"Use cases seen in other jurisdictions around the world include assessing financial risks, crafting investment portfolios, and reducing and preventing fraud and combating financial crime.","Singapore\'s National AI Programme in Finance":"The Singaporean National Artificial Intelligence Programme in Finance supports Singaporean financial institutions to research, develop, and deploy AI solutions."},"international_alignment":"Supports learning from international approaches, particularly EU and UK models, while adapting to Australian context","values":["Innovation","Consumer protection","Transparency","Certainty","Trust"],"tone":"Positive and constructive","stakeholders":[{"entity":"Government","role":"Develop coordinated, risk-based AI governance framework"},{"entity":"Financial institutions","role":"Responsible development and deployment of AI solutions"},{"entity":"Regulators","role":"Enforce AI governance within existing regulatory frameworks"}],"sector_impacts":[{"sector":"Financial services","impact":"Improved risk assessment, investment management, fraud prevention, and customer service"}],"quotes":["The FSC is supportive of a risk-based approach to governing AI use in Australia with only the most obviously harmful AI products banned.","Australia\'s approach to AI governance should be using a pro-AI approach which encourages innovation in Australia.","The FSC is supportive of a governance framework that creates trust in AI through transparency requirements such as disclosure obligations where decisions are being made by artificial intelligence. However, this should be appropriately balanced with intellectual property and competition considerations."]},"366_2023 07 25 ISA Submission to.pdf":{"organization_name":"Independent Schools Australia","organization_type":"National peak body for Independent school sector","classification":"Neutral","overall_position":"Supports responsible and ethical use of AI in education with a comprehensive national framework","arguments":["Generative AI has transformative potential for the education sector","AI must be used discerningly to benefit, not harm, individuals and society","A comprehensive national framework for ethical and safe use of AI in education would be beneficial"],"counterarguments":["null"],"key_recommendations":["Form a federal education advisory body to examine AI benefits and risks","Implement funded initiatives to support teacher workforce in effective AI use","Create a national generative AI in education website freely accessible to all educators","Provide funding to AISs to support schools in implementing ethical AI strategies","Ensure human agency and critical thinking remain core to education"],"risks_and_challenges":["Rapid and exponential development of AI technology","Potential long-term impacts on teaching, assessment, and student outcomes","Privacy risks, data security, and copyright considerations","Digital divide between early adopters and those reluctant to embrace AI","Potential negative impacts on social relationships and human qualities"],"safeguards_and_mitigations":["Develop evidence-based guidelines for ethical AI implementation","Review traditional assessment methods to address plagiarism concerns","Provide professional development for educators on AI risks and opportunities","Ensure age-appropriate implementation of AI tools in education"],"examples":{"Independent schools at various stages of AI implementation":"Independent schools are in various stages of researching, investigating and implementing generative AI practices, aligning with their values and individual school community contexts.","Schools with digital technology experts more confident in exploring AI":"The Independent school case studies in Appendix A highlight that having a digital technology \'expert\' on staff provides more confidence for those Independent schools wishing to explore generative AI opportunities."},"international_alignment":"null","values":["Ethics","Safety","Equity","Human agency","Critical thinking"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Federal education advisory body","role":"Examine AI benefits and risks to inform education policy"},{"entity":"Educators","role":"Implement AI technologies ethically and effectively"},{"entity":"Students","role":"Learn to use AI technologies ethically and with discernment"},{"entity":"AISs","role":"Provide professional development and policy advice to schools"}],"sector_impacts":[{"sector":"Education","impact":"Potential transformation of teaching, assessment, and student outcomes"}],"quotes":["Harnessing the opportunities that generative AI might bring to education and industry requires a coordinated, collaborative approach between government, regulators, universities, school sectors, industry and other key stakeholders incorporating best practice, evidence-based research and futures thinking.","And it is also imperative that while we grapple with how best to harness the benefits of generative AI technologies in education, we do not lose sight of the core purpose of education and collectively, we must ensure that human beings retain human agency and the capacity to learn, think critically, be creative and confident with the capacity to make informed choices and make a positive difference in the world.","Many Independent schools are seeking evidence-based guidelines to manage privacy risks, data security and copyright considerations to support the ethical implementation of generative AI, including examples of best-practice policies."]},"220_ANZSA Safe and responsible AI submission.b6a290d7f9767.pdf":{"organization_name":"Australia New Zealand Screen Association (ANZSA)","organization_type":"Industry association","classification":"Opponent","overall_position":"Favors minimal intervention and opposes hasty new AI-specific regulation","arguments":["AI use in the screen industry is low risk and doesn\'t warrant heavy regulation","Broad new AI regulation may overlap with existing regulations, causing uncertainty","Burdensome requirements could disincentivize business investment","The creative sector flourishes best with light-touch regulation"],"counterarguments":["null"],"key_recommendations":["Avoid rushing to regulate AI or impose new hasty rules","Consider carefully the need for a regulatory framework in consultation with stakeholders","If regulation is necessary, ensure obligations are proportionate to potential harm","Recognize the differentiation between VOD services and user-generated content providers in terms of content responsibility"],"risks_and_challenges":["Regulatory uncertainty","Heavy compliance burden","Disincentivizing business investment"],"safeguards_and_mitigations":["Risk management approach catering to context-specific risks of AI","Less onerous obligations for lower risk AI uses"],"examples":{"AI use in film production":"AI is used in fairly routine post-production work like colour correction, detail sharpening, de-blurring, or removing unwanted objects. Some are more involved, like aging and de-aging an actor.","AI in VOD services":"VOD services use various recommendation systems (some of which could be enhanced by AI) to help viewers find content that most closely suits their interests.","AI in creative process":"AI and generative AI will serve to free up humans from the most rote parts of their work, allowing them to concentrate their limited time and effort on the most creative aspects."},"international_alignment":"null","values":["Innovation","Business-friendly environment","Human creativity"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Carefully consider the need for regulation and consult with stakeholders"},{"entity":"Industry representatives","role":"Provide input on AI regulation"},{"entity":"Attorney-General\'s Department","role":"Address copyright-related AI issues"}],"sector_impacts":[{"sector":"Creative sector","impact":"Potential disincentive to further investment if burdensome requirements are imposed"},{"sector":"Screen industry","impact":"AI enhances production processes and audience experiences"}],"quotes":["We urge the Government not to rush to regulate AI or to impose new and hasty rules on the use of AI.","Broadly speaking, the creative sector flourishes best in a context of light-touch regulation that encourages ease of doing business, both domestically and internationally.","AI is an enabling tool that can complement aspects of filmmaking process, the audience viewing experience, and fan engagement."]},"353_20230726_AMTA Letter of support for Communications Alliance AI submission.a44c58124a55b.pdf":{"organization_name":"Australian Mobile Telecommunications Association","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports effective regulation that addresses risks while maximizing benefits","arguments":["AI can bring economic, social and environmental benefits","5G technology will be a key enabler for AI success","Effective regulation can address existing and emerging risks"],"counterarguments":[],"key_recommendations":["Develop coordinated and consistent regulation","Create policies that efficiently govern AI use","Empower innovative practices within industry"],"risks_and_challenges":[],"safeguards_and_mitigations":[],"examples":{},"international_alignment":"null","values":["Innovation","Safety","Responsibility"],"tone":"Positive","stakeholders":[{"entity":"Government","role":"Develop regulation and policies"},{"entity":"Industry","role":"Advance and leverage AI technologies"}],"sector_impacts":[{"sector":"Mobile telecommunications","impact":"Advancement and leveraging of AI technologies"}],"quotes":["AMTA supports the safe and responsible use of artificial intelligence (AI) in Australia and effective regulation that addresses existing and emerging risks while maximising benefits and opportunities to Australian businesses and communities.","Our members, which include mobile network service providers, handset manufacturers, network equipment suppliers, retail outlets and other suppliers to the mobile telecommunications industry, are at the forefront of technology innovation in Australia and are already advancing and leveraging artificial intelligence and associated emerging technologies.","We look forward to continued engagement with government to develop coordinated and consistent regulation and policies that efficiently govern the use of artificial intelligence and empower innovative practices within industry."]},"237_20230726 - AI Submission - Final (signed).92df6a95a6fa9.pdf":{"organization_name":"Australian Retail Credit Association (ARCA)","organization_type":"Industry association","classification":"Neutral","overall_position":"Supports updating existing laws with AI-specific provisions where necessary, while relying on existing regulation as much as possible","arguments":["Existing regulation should be relied upon as much as possible to avoid regulatory arbitrage","Bespoke AI regulation risks losing benefits of existing consumer understanding and jurisprudence","Principle-based primary legislation with operative rules in regulations allows for timely updates as technologies evolve"],"counterarguments":["Some aspects of AI may require standalone regulation, such as ethical rules frameworks","Prescriptive regulatory frameworks for complex technical areas can be problematic","Existing laws may not fully address AI-specific challenges"],"key_recommendations":["Rely on existing regulation as much as possible, including anti-discrimination legislation, Privacy Act, and Australian Consumer Law","Where new regulation is required, use principle-based primary legislation with operative rules in regulations or legislative instruments","Implement regular reviews of both operative rules and principles-based legislation","Embed reporting mechanisms to track and substantiate regulatory effectiveness"],"risks_and_challenges":["Prescriptive legislation may be unable to adapt to rapid technological changes","Complex areas of law can be difficult to get right and explain to stakeholders","Potential for discriminatory outcomes in AI systems","Low base level of consumer understanding and engagement with AI concepts"],"safeguards_and_mitigations":["Risk-based approach to AI regulation","Rigorous testing and monitoring of AI systems","Providing simple, context-rich explanations to build consumer trust and understanding","Developing consistent messaging across organizations using AI"],"examples":{"Credit reporting regulation challenges":"Part IIIA was drafted before Buy Now Pay Later products existed, and also without any reference to the use of soft enquiries to support lending activity. Attempts to adapt to these developments have relied on unintended interpretations of the prescriptive legislation.","Natural language processing in call monitoring":"For example, the use of natural language processing may be beneficial, in that it would support more efficient and effective monitoring of high-volume calls, while also leading to the risk that customers may be misclassified."},"international_alignment":"null","values":["Transparency","Consumer trust","Adaptability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop principle-based primary legislation and conduct regular reviews"},{"entity":"Industry","role":"Implement AI systems responsibly and contribute to regulatory reporting"},{"entity":"Consumers","role":"Engage with and understand AI systems they interact with"}],"sector_impacts":[{"sector":"Finance","impact":"Potential changes in credit scoring and lending practices"}],"quotes":["ARCA\'s experience has also been that complex areas of law \u2013 credit reporting being a prime example (legislating a data ecosystem) \u2013 can be difficult to get right, and even more difficult to explain to others (including those within industry, and also consumers).","It is critical to approach the need for transparency with the understanding that the base level of consumer understanding or engagement is likely to be low.","ARCA considers a risk-based approach to AI regulation is appropriate. To ensure the appropriate rollout and implementation \u2013 in particular of new AI use cases \u2013 a conservative approach may be warranted."]},"431_Joint submission from library.pdf":{"organization_name":"Joint submission from library and information service-related organisations","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports responsible AI practices with focus on education, literacy, and standards","arguments":["Libraries play a crucial role in educating the public about AI and information literacy","AI can enhance library services and improve access to information","Responsible AI practices are necessary to protect vulnerable groups and preserve cultural heritage"],"counterarguments":["null"],"key_recommendations":["Improve AI, information, and media literacy across the Australian population","Develop standards for AI products in consultation with stakeholders","Invest in upskilling library staff to be AI literate","Create safeguards against misuse of Aboriginal and Torres Strait Islander peoples\' knowledge and culture"],"risks_and_challenges":["Potential misuse of First Nations content and knowledge","Challenges in identifying AI-generated content","Impact on Australian creators and the publishing industry"],"safeguards_and_mitigations":["Develop requirements for AI outputs to be clearly identifiable","Create standards for AI tools to respect Indigenous Cultural and Intellectual Property","Implement ongoing monitoring of AI tool outputs"],"examples":{"Media literacy short course":"A model of a successful short course in a related area was the recent partnership between the University of Canberra (UC) and the Australian Library and Information Association (ALIA) on the short course \'Media Literacy for LIS Professionals.\'","Impact on scholarly publishing":"We note that already there are reports of generative AI being used in the assessment of grant applications, and non-disclosed AI articles being accepted for publication."},"international_alignment":"Supports alignment with international standards and protocols","values":["Equity of access to information","Respect for diversity","Preservation of human record","Protection of privacy"],"tone":"Cautionary but optimistic","stakeholders":[{"entity":"Libraries and information services","role":"Educators and mediators of AI tools"},{"entity":"First Nations groups","role":"Consultants on Indigenous data governance"},{"entity":"Government","role":"Develop regulations and policies"}],"sector_impacts":[{"sector":"Education","impact":"Need for AI literacy programs and resources"},{"sector":"Publishing","impact":"Potential disruption to traditional publishing models"},{"sector":"Cultural institutions","impact":"Challenges in preserving and managing AI-generated content"}],"quotes":["Libraries play an essential role in media and information literacy, helping develop the skills to find, evaluate, store and manage information, to reuse information to create new knowledge or solve problems, and to understand how information exists within social, ethical, cultural and legal contexts.","Unless there is an increased effort to make true information literacy a part of basic education there will be a class of people who can use algorithms and a class used by algorithms.","Generative AI will respond to prompts requesting it to write First Nations content that it has no standing to tell."]},"184_20230725 CA ANZ sub Safe and Responsible AI(records).85eb64e922823.pdf":{"organization_name":"Chartered Accountants Australia and New Zealand","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports comprehensive regulation with an international framework","arguments":["Regulation is required to create a holistic framework of how AI can and cannot be used","International cooperation is necessary as technology is not bound by geographic borders","Regulation should limit what AI can be used for and hold designers and developers accountable"],"counterarguments":["Regulation has limited application in educating users about AI limitations"],"key_recommendations":["Develop an international framework for AI regulation","Require transparency on emissions and data sources","Implement United Nations\' human rights principles throughout AI supply chains","Establish accountability measures for AI-generated content"],"risks_and_challenges":["Environmental impact of AI development and use","Human rights issues in AI supply chains","Lack of accountability for harmful AI-generated content","Users\' lack of awareness about AI limitations"],"safeguards_and_mitigations":["Energy rating system for AI tools","Adoption of UN Guiding Principles on Business and Human Rights","Nomination of accountable persons for AI-generated harm","Employee training and upskilling for AI use"],"examples":{"Environmental impact":"Training a transformer would emit more than 626,000 pounds of CO2 (284 tonnes)","Human rights issues":"Time investigation into OpenAI\'s use of Kenyan workers for reinforcement learning","Industry-specific AI tools":"McKinsey\'s collaboration with Cohere for tailored AI solutions"},"international_alignment":"Supports international framework with jurisdiction-specific additions","values":["Accountability","Transparency","Human rights","Environmental responsibility"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, provide education"},{"entity":"AI developers and designers","role":"Ensure ethical and responsible AI design, provide transparency"},{"entity":"Organizations","role":"Implement responsible AI use, train employees"}],"sector_impacts":[{"sector":"Accounting","impact":"Increased productivity and new activities for accountants"},{"sector":"Environment","impact":"Potential for significant emissions from AI development and use"}],"quotes":["We consider regulation is required to create a holistic framework of how AI can and cannot be used.","Any regulation in this area will need to recognise that technology is not bound by geographic borders and therefore that an international framework is required.","Employee literacy will be key to the safe and responsible use of AI. It will be critical for organisations to upskill their employee\'s technical skills to interrogate, and sense-check responses generated by AI."]},"147_230724 AU Safe and Responsible AI.doc.173122158f453.pdf":{"organization_name":"Salesforce","organization_type":"Global leader in cloud enterprise software","classification":"Proponent","overall_position":"Supports risk-based AI regulation that differentiates contexts and uses of the technology","arguments":["AI regulation is most effective when built upon, and interoperable with, robust privacy standards","One-size fits all approaches would hinder innovation and competition","Risk-based AI regulation would focus most on high-risk applications"],"counterarguments":["null"],"key_recommendations":["Establish common standards for AI definitions and obligations","Consider appointing a \'Chief ethical and humane use officer\' or AI Safety Officer for government","Implement a risk-based framework for AI regulation"],"risks_and_challenges":["Privacy issues and data mining","Copyright concerns","Misinformation","Identity verification","Child protection"],"safeguards_and_mitigations":["Adhere to responsible AI principles","Conduct bias, explainability, and robustness assessments","Enable customers to train models on their own data"],"examples":{"NSW Artificial Intelligence Advisory Committee":"NSW Artificial Intelligence (AI) Advisory Committee which comprises of experts to use their extensive and varied experience to provide ongoing strategic advice on the use of AI to assist in decision-making and improving service delivery across the NSW Government.","Singapore\'s Advisory Council on the Ethical Use of AI and Data":"Singapore\'s Advisory Council on the Ethical Use of AI and Data which was formed in 2018 for the purpose of advising Singapore Government on ethical, policy and governance issues arising from the use of data-driven technologies in the private sector","AI Verify":"AI Verify, a subsidiary of the Infocommunications Media Development Authority of Singapore (IMDA). It is an AI governance testing framework and software toolkit that validates the performance of AI systems against a set of internationally recognised principles through standardised tests"},"international_alignment":"Supports global standards and interoperability","values":["Responsible","Accountable","Transparent","Empowering","Inclusive"],"tone":"Positive","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulations"},{"entity":"Industry","role":"Work with policymakers to establish guardrails for ethical AI development and utilization"}],"sector_impacts":[{"sector":"Healthcare","impact":"Higher risks for users and society, requiring more guardrails and oversight"},{"sector":"Enterprise","impact":"Creating efficiencies and augmenting human decision making processes"}],"quotes":["Salesforce supports risk-based AI regulation that differentiates contexts and uses of the technology (e.g. B2C v. B2B) and assigns responsibilities based on the different roles that various entities play in the AI ecosystem.","We believe that the tremendous benefits of AI should be accessible to everyone, while ensuring that those technologies remain safe and inclusive.","A one-size-fits-all approach to regulation may hinder innovation, disrupt healthy competition, and delay the adoption of the technology that consumers and businesses around the world are already using to boost productivity."]},"2_Ben Blackburn.pdf":{"organization_name":"Ben Blackburn Racing","organization_type":"Private company","classification":"Proponent","overall_position":"Supports comprehensive regulation and governance of AI","arguments":["Australia has an opportunity to be a global leader in responsible AI","AI could add significant value to the Australian economy","Existing regulatory and governance mechanisms need to be evaluated for fitness"],"counterarguments":["null"],"key_recommendations":["Establish public assessments of existing generative AI systems","Develop a blueprint for an AI Bill of Rights in Australia","Fast-track the implementation of an AI Risk Management Framework","Release draft policy guidelines on the use of AI systems by the Australian Government","Establish a new tech regulation body","Appoint a dedicated Commonwealth Minister for the Digital Economy","Establish a time-limited Australian AI Commission"],"risks_and_challenges":["Significant environmental cost of AI technologies","Ethical use of data and privacy concerns","Potential for AI-related harms to individuals and society","National security concerns, especially in cybersecurity and biosecurity"],"safeguards_and_mitigations":["Focus on safeguarding Australian people\'s rights and safety","Ensure data is used ethically and follows privacy and security principles","Be mindful of Indigenous data sovereignty"],"examples":{"US Government actions":"The United States Government has announced several new policy actions to further promote responsible American innovation in artificial intelligence (AI) and protect people\'s rights and safety.","McKinsey economic impact projection":"Consulting firm McKinsey calculated the technology could add between $1.1 trillion and $4 trillion to the Australian economy by the early 2030s, the report said."},"international_alignment":"Acknowledges actions taken by other jurisdictions like the US, EU, and China","values":["Responsible innovation","Ethical use of technology","Public good"],"tone":"Optimistic but cautious","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulations and policies"},{"entity":"Companies","role":"Ensure products are safe before deployment"},{"entity":"Researchers","role":"Contribute to responsible AI development"}],"sector_impacts":[{"sector":"Economy","impact":"Potential significant economic growth"},{"sector":"Cybersecurity","impact":"Increased focus on AI-related security concerns"}],"quotes":["We welcome a commonsense approach to regulation that recognises the importance of these technologies and the role they can play in assisting economic productivity, health and social wellbeing.","It\'s also critical that we examine the significant environmental cost of these technologies, especially the huge power and water requirements, and resources consumed by upgrading hardware.","Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."]},"102_auDA\'s submission on Safe and Responsible AI_July 2023.e595d25dfd654.pdf":{"organization_name":".au Domain Administration Limited","organization_type":"Not-for-profit administrator of .au ccTLD","classification":"Proponent","overall_position":"Supports a non-regulatory proportionate risk-based approach towards AI governance","arguments":["A risk-based approach is sensible and can be achieved without creating unnecessary additional regulatory burden","Cross-government coordination provides foundation for a whole-of-government approach towards digital economy policy","Multi-stakeholder discussions can encourage international collaboration and alignment of policy approaches"],"counterarguments":[],"key_recommendations":["Align terminology and definitions with those agreed upon and used by likeminded jurisdictions","Establish a multi-regulator sandbox facilitated through a multi-stakeholder approach","Impose bans on AI applications that pose unacceptable risks to Australians","Enhance education and awareness-raising efforts to support Australians in understanding and trusting AI","Adopt a non-regulatory proportionate risk-based approach towards AI governance","Assess cyber security implications of AI systems"],"risks_and_challenges":["AI can be deployed for cyber security defence as well as for cyber security disruptions","AI can improve malware performance and create more credible threats to victims","Generative AI exacerbates cyber security risks"],"safeguards_and_mitigations":["Oversight of AI systems to detect abnormal algorithmic behaviour due to cyberattacks","Data quality assessment including data traceability","Encourage trustworthiness features of AI such as oversight, data accuracy and traceability, explainability and transparency"],"examples":{"Enhanced Regulatory Sandbox (ERS) by ASIC":"We note that the Australian Securities and investments Commission (ASIC) is operating an Enhanced Regulatory Sandbox (ERS) also referred to as \'fintech sandbox\' to test innovative financial or credit services.","EU AI Act on prohibited AI practices":"auDA understands that Article 5 on Prohibited Artificial Intelligence Practices in the EU\'s AI Act contains a clear statement on systems deemed to pose unacceptable risks such as real-time biometric identification systems in public spaces, being prohibited with little exception."},"international_alignment":"Supports cross-border cooperation and harmonisation of frameworks to align with international best practices","values":["Trust","Innovation","Security"],"tone":"Neutral","stakeholders":[{"entity":"Government","role":"Coordinate efforts across agencies, raise public awareness, and participate in multi-stakeholder discussions"},{"entity":"Regulators","role":"Participate in multi-regulator sandbox and coordinate efforts"},{"entity":"Technical community","role":"Participate in multi-stakeholder discussions"},{"entity":"Academia","role":"Participate in multi-stakeholder discussions"},{"entity":"Non-government organisations","role":"Participate in multi-stakeholder discussions"}],"sector_impacts":[{"sector":"SMEs","impact":"Supporting SMEs in participating in the digital economy must be a government priority"},{"sector":"Cyber security","impact":"AI can be deployed for cyber security defence as much as for cyber security disruptions"}],"quotes":["auDA believes that the policy framework should be shaped to emphasise the importance of utilising the technology for good. This requires policymakers and regulators to adopt a human-centric approach towards innovation to unlock positive economic and social value for all Australians.","Harmonisation and interoperability between equivalent global regulatory regimes provide greater legal certainty for Australian businesses and individuals, avoid unnecessary barriers to innovation, and encourage adoption of AI.","auDA believes that a risk-based approach is sensible, meaning that the requirements for AI systems would be proportionate to the risk in the respective areas of application."]},"160_WA Government Response to Safe and Responisble Al in Australia.cb6efc557cc27.pdf":{"organization_name":"Western Australian Government","organization_type":"Government","classification":"Proponent","overall_position":"Supports national approach to AI regulation with state-level autonomy","arguments":["AI has many possible benefits and transformative potential","AI can be leveraged to diversify the economy, create new employment opportunities, optimize processes and improve government service delivery and decision making","Nationally consistent governance and regulatory measures are critical to ensuring the benefits of AI can be responsibly realized"],"counterarguments":["The use of AI is not without risks and challenges","AI requires human oversight, good governance and appropriate controls to be in place to protect the citizen and the community"],"key_recommendations":["Collaborate with Commonwealth and states/territories to identify appropriate governance, frameworks, and policies","Balance the need for AI regulation against the risk of stifling AI adoption and innovation","Develop an approach that keeps pace with evolving technology and is consistent with international best practice"],"risks_and_challenges":["Privacy and ethical concerns","Potential stifling of AI adoption and innovation if regulation is too strict"],"safeguards_and_mitigations":["Adoption of Commonwealth Digital Transformation Agency\'s Adoption of AI in the Public Sector paper as interim guidance","Development of state-level measures for safe and responsible AI adoption","Collaboration with industry and academic researchers"],"examples":{"Education":"The WA Department of Education is participating in a national taskforce to develop a framework for AI use in schools","Healthcare":"AI is being used in various clinical applications, from AI-assisted imaging to remote patient monitoring","Resources sector":"WA\'s resources sector is a world-leader in the implementation of AI and machine learning for increasing efficiency, ensuring safety, and improving sustainability"},"international_alignment":"Supports approach consistent with international best practice","values":["Fairness","Transparency","Accountability"],"tone":"Positive","stakeholders":[{"entity":"Government","role":"Develop and implement AI policies and regulations"},{"entity":"Industry","role":"Collaborate on innovative AI initiatives"},{"entity":"Academic sector","role":"Collaborate on research and development of AI"}],"sector_impacts":[{"sector":"Education","impact":"AI tools to support teaching and learning"},{"sector":"Healthcare","impact":"Transforming all aspects of healthcare value chain, improving patient outcomes"},{"sector":"Resources","impact":"Increasing efficiency, ensuring safety, and improving sustainability"}],"quotes":["The WA Government supports the work being undertaken to identify gaps in Australia\'s existing regulation and governance to develop additional measures to further safeguard and mitigate risks.","Nationally consistent governance and regulatory measures that support the wider adoption of AI by focusing on fairness, transparency and accountability are critical to ensuring the benefits of AI can be responsibly realised.","It will be important to balance the need for AI regulation against the risk of stifling AI adoption and innovation across industry and government."]},"72_Submission 72 - Attachment.da844b8e9f076.pdf":{"organization_name":null,"organization_type":null,"classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a clear line of responsibility for AI systems deployment","arguments":["Strict regulation is needed for large-scale AI systems deployment","Clear line of responsibility should be established for AI deployment","Individuals should not be held responsible for AI systems deployment"],"counterarguments":["Personal use of AI should be treated differently from deployment","Existing laws may still apply to personal AI use outcomes"],"key_recommendations":["Split AI use into two categories: AI for deployment and AI for personal use","Establish clear lines of responsibility for AI deployment","Implement restrictions and regulations on AI for deployment","Create a rating system for notifying the public about AI use"],"risks_and_challenges":["Potential harm caused by AI systems","Rapid adoption of AI by individuals without full understanding","Impact on livelihoods due to widespread AI deployment"],"safeguards_and_mitigations":["Criminal and civil prosecution for harm caused by AI deployment","Restrictions on AI use in critical decision-making areas","Regulations for AI deployment in various industries"],"examples":{"AI developer responsibility":"AI developer XYZ Systems releases an AI model to the general public on their website. The AI becomes incredibly popular with individuals, and the system is used to collect data from its users (personal data) \u2013 to grow the AI.","Business owner responsibility":"A medium sized business owner, John Smith feeds resumes (personal data) into an off-the-shelf AI system from Company N, to help with selecting new employees.","Personal use of AI":"Jane Stevens uses an AI chat bot from Company Z to organise her day, get recommendations on the purchase of a new car, and find a nearby hairdresser."},"international_alignment":null,"values":["Responsibility","Accountability","Compassion"],"tone":"Cautionary","stakeholders":[{"entity":"Organizations and leaders","role":"Bear responsibility for AI systems deployment"},{"entity":"Individuals","role":"Allowed to experiment with AI in personal context"},{"entity":"Government","role":"Implement regulations and demonstrate responsibility"}],"sector_impacts":[{"sector":"Legal","impact":"AI should not be used for determining legal decisions"},{"sector":"Healthcare","impact":"AI should not determine medical diagnoses without human doctor input"},{"sector":"Employment","impact":"AI should not be used for determining employment decisions"}],"quotes":["AI Systems Deployment Responsibility","There needs to be strict regulation through a clear line of responsibility for AI systems deployment, particularly when these systems are at large scales (100+ potential AI interactions). This should not fall on individuals, but on organisations and leaders.","If an individual or organisation uses AI for deployment, there must be restrictions and regulations on its use. Any individual or organisation with a clear line of responsibility should therefore be liable for criminal, and civil prosecution if any harm is done because of the AI for deployment."]},"273_20230726 Hub RegLab Submission to Industry re safe anf responsible AI.ec0092a85c2dc.pdf":{"organization_name":"UNSW Allens Hub for Technology, Law and Innovation and UNSW Business School Regulatory Laboratory","organization_type":"Academic research institutions","classification":"Proponent","overall_position":"Prefers updating existing laws rather than creating new AI-specific legislation","arguments":["Existing laws can be adapted to address AI-related challenges","Technology-specific legislation may become obsolete quickly","Many AI-related issues are not unique to AI and can be addressed through general legal principles"],"counterarguments":["Some argue for AI-specific legislation","There are calls for defining AI for regulatory purposes"],"key_recommendations":["Reform existing laws (e.g., consumer protection, discrimination, administrative, privacy)","Develop procurement requirements for government AI systems","Support participation in international standards development","Implement a \'model user\' approach for government AI use","Adopt a risk-based approach to AI governance"],"risks_and_challenges":["Digital consumer manipulation","Algorithmic bias and discrimination","Lack of transparency and accountability in AI decision-making","Privacy concerns related to data use in AI systems"],"safeguards_and_mitigations":["Updating privacy laws to address AI-related concerns","Reforming discrimination laws to address algorithmic bias","Improving transparency requirements for government AI use","Implementing AI impact assessments"],"examples":{"Robodebt case":"Robodebt is cited as an example of problems with automated decision-making in government.","Dark patterns in e-commerce":"Dark patterns are discussed as an example of digital consumer manipulation enabled by AI.","Amazon Prime enrollment":"The FTC\'s action against Amazon for misleading Prime enrollment practices is mentioned as an example of potential AI-related consumer harms."},"international_alignment":"Supports participation in international standards development while adapting approaches to the Australian context","values":["Accountability","Transparency","Fairness","Privacy","Trustworthiness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and implement responsible AI practices, update existing laws"},{"entity":"Private sector","role":"Comply with updated laws and standards, implement ethical AI practices"},{"entity":"Standards organizations","role":"Develop international standards for AI"}],"sector_impacts":[{"sector":"Financial services","impact":"Increased regulation of AI use in pricing and decision-making"},{"sector":"E-commerce","impact":"Potential restrictions on manipulative AI-driven practices"},{"sector":"Public administration","impact":"Greater requirements for transparency and accountability in AI-driven decision-making"}],"quotes":["Our primary point is that the definition of \\"artificial intelligence\\" does not correspond with the optimal scope of regulatory action.","The question to be answered is not \\"how do we regulate AI\\" but rather \\"how do we ensure that our laws operate appropriately and effectively to achieve policy objectives, including in contexts involving AI\\".","Trust should not be sought as an end in and of itself. It is crucial that the public remain appropriately sceptical about computer systems with which they interact so that they take appropriate measures to protect their privacy and challenge illegal decisions."]},"82_Supporting Responsible AI Submission.0a5f460f3513a.pdf":{"organization_name":null,"organization_type":"Individual (PhD student)","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and research on AI existential risks","arguments":["AI systems pose existential risks that are not addressed in current regulatory approaches","We lack the ability to make robustly controllable and transparent AI systems","Rapid AI progress could lead to AI takeover and disempowerment of humanity"],"counterarguments":["Overwhelming regulation could choke progress in AI","Current transparency requirements are infeasible to meet with existing model architectures"],"key_recommendations":["Make it illegal to deploy AI systems with appreciable risk of AI takeover or killing all humans","Massively accelerate research on AI control, alignment, transparency, and interpretability","Provide large committed fund for AI safety research, at least one billion dollars over three years"],"risks_and_challenges":["AI existential risk leading to human extinction or unrecoverable societal collapse","AI systems exhibiting undesirable goal-directed behaviors and specification gaming","Rapid automation of human cognitive tasks leading to potential AI takeover"],"safeguards_and_mitigations":["Develop robustly controllable AI systems that act in alignment with human values","Improve AI transparency and interpretability","Implement external risk auditing for high-risk AI systems"],"examples":{"Specification gaming":"Researchers from DeepMind have collated an extremely long list of examples of specification gaming in existing AI systems.","AI progress in mathematics":"Minerva, a version of PaLM specialising in mathematical and scientific reasoning, scored 50.3 per cent on MATH, achieving four years of progress in just one.","AI takeover scenario":"If the AI systems that suddenly run the world economy are not robustly controllable, directable, transparent, interpretable, or understandable, they may not act in alignment with human values, disempowering humanity in an AI takeover."},"international_alignment":null,"values":["Safety","Transparency","Human-centered values"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement regulations and fund AI safety research"},{"entity":"AI companies","role":"Develop safe and controllable AI systems"},{"entity":"Researchers","role":"Advance AI control, alignment, transparency, and interpretability"}],"sector_impacts":[{"sector":"Economy","impact":"Potential rapid automation of entire world economy in a three-year timescale"}],"quotes":["AI existential risk is the most serious form of risk posed by AI systems, and as I have written in Honi Soit (Appendix A), the student newspaper of the University of Sydney, these risks demand serious consideration.","It should be illegal for companies to deploy AI systems that they believe to have an appreciable risk of AI takeover or of killing all humans.","We do not understand how cutting-edge neural network-based AI systems generate their responses, or make their decisions. Meaningful transparency requirements are therefore tantamount to a ban."]},"426_Submission on Safe and Responsible AI in Australia.4c7202b9aa00c.pdf":{"organization_name":"Grow Right","organization_type":"Private company","classification":"Proponent","overall_position":"Supports a balanced approach combining voluntary and regulatory measures for responsible AI","arguments":["Non-regulatory initiatives can foster responsible AI adoption","A central authority can ensure consistent regulations across government sectors","Generic solutions can address common AI risks across diverse applications"],"counterarguments":["Self-regulation alone might not suffice in addressing all risks","A purely regulatory approach may hinder innovation"],"key_recommendations":["Establish a central authority to regulate, monitor, and promote AI technologies","Develop a shared framework for AI regulation across government departments","Implement a combined approach of mandatory basic practices and voluntary improvements","Use regulatory sandboxes for testing new AI applications and governance strategies"],"risks_and_challenges":["Potential for significant harm in certain AI applications","Weak incentives for responsible behaviour in some cases"],"safeguards_and_mitigations":["Public education initiatives on AI implications","Incentives for private sector to adopt responsible AI practices","Collaborative efforts between government, academia, and private sector","Transparency requirements for AI systems"],"examples":{"Facial recognition systems":"Example of technology-specific solutions needed","Autonomous vehicles":"Example of technology-specific solutions needed"},"international_alignment":"null","values":["Transparency","Accountability","Ethics","Privacy","Fairness"],"tone":"Balanced and constructive","stakeholders":[{"entity":"Government","role":"Regulate, monitor, and promote AI technologies"},{"entity":"Private sector","role":"Adopt responsible AI practices"},{"entity":"Academic institutions","role":"Collaborate in developing and promoting AI best practices"}],"sector_impacts":[{"sector":"Government","impact":"Consistent regulations and cooperation between different sectors"}],"quotes":["We propose the establishment of a central authority to regulate, monitor, and promote AI technologies across all government sectors.","Balancing flexibility and standardisation in AI governance is crucial for risk management.","A combined approach, mandating a basic level of responsible AI practices while encouraging organisations to surpass minimum requirements and continually improve, could be most effective."]},"404_Atlassian - Safe & Responsible AI Submission (04.08.2023).c2e052cd1155.pdf":{"organization_name":"Atlassian","organization_type":"Technology company","classification":"Proponent","overall_position":"Supports a principles-based, coordinated digital regulatory framework for AI and emerging technologies","arguments":["Existing laws may already apply to AI in many contexts","A flexible, risk-based approach is needed to address the multi-dimensional nature of AI opportunities and risks","Law and regulation can be a \'force multiplier\' for trust in new technologies"],"counterarguments":["The scale and speed of AI adoption has led to a perception of uncharted legal territory","Complex issues and harms raised by AI may require new regulatory approaches"],"key_recommendations":["Implement a purpose-driven, outcomes-focused approach to AI governance","Establish core principles enshrined in legislation","Create central advisory bodies for coordination and expertise","Design targeted governance measures aligned with overarching principles"],"risks_and_challenges":["Varying, context-dependent risk profiles with legal, regulatory, and societal dimensions","Lack of clarity on how existing laws apply to AI","Potential for actual, present, or imminent harms without appropriate interventions"],"safeguards_and_mitigations":["Develop a mix of \'hard\' regulatory measures and \'soft\' law approaches","Invest in standards development processes","Implement education and awareness programs across various levels"],"examples":{"Notification requirements for AI interactions":"In some cases, individuals are not made aware that they are interacting with \'human-like\' AI systems (like chatbots), or that AI technologies are being used to make certain significant decisions about their lives and livelihoods (like eligibility for insurance)."},"international_alignment":"Supports alignment and interoperability with emerging global standards","values":["Trust","Responsibility","Flexibility"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Design and implement regulatory framework"},{"entity":"Advisory bodies","role":"Provide expertise and assist with coordination across agencies"},{"entity":"Industry","role":"Implement responsible AI practices"},{"entity":"Individuals","role":"Understand and engage with AI systems"}],"sector_impacts":[{"sector":"Technology","impact":"Enhanced collaboration and productivity tools"},{"sector":"Education","impact":"Need for lifelong learning and critical thinking skills"}],"quotes":["We believe that our regulatory landscape needs to clearly and carefully anticipate what our digital future will mean for Australian organisations and individuals operating in an evolving global economy.","We strongly recommend that the Australian Government use this opportunity to design and set forth a purpose-driven, outcomes-focused approach to the governance of AI and other emerging technologies.","We believe that law and regulation, designed carefully, can be a \'force multiplier\' for trust: fostering confidence and trust in new industries and technologies, and encouraging their adoption in a way that brings the most benefit to us all."]},"304_Submission_Locke Lupo and Quoc Vo.c979228e0b67e.pdf":{"organization_name":"Swinburne University","organization_type":"Academic institution","classification":"Proponent","overall_position":"Supports a light touch mandatory approach with risk-based regulation and automated assurance mechanisms","arguments":["Public trust in AI systems requires objective assurance of adherence to principles and guidelines","Risk-based approach allows for customized assurance based on specific system risk factors","Automated assurance could alleviate resource concerns for smaller deployers"],"counterarguments":["Self-regulatory approaches are often preferred by industry","Risk-based approach may fail to offer a foundational standard of \'what good looks like\'","Human-in-the-loop approaches may be a significant barrier to adoption at scale"],"key_recommendations":["Adopt assurance mechanisms supervised by a regulator","Implement a badge or seal system to confirm assurance for consumers","Develop automated, scalable systems for continuous assurance of AI systems\' ethical posture","Evaluate potential advantages of establishing an ecosystem of trust similar to the EU\'s approach"],"risks_and_challenges":["Risk-based approach allows organizations to establish their own standards of compliance","Longer road to achieving public trust, dependent on government auditing capacity","LLMs introduce additional risks due to their SaaS nature and outsourced model"],"safeguards_and_mitigations":["Combine basic standards with attestation structure and third-party conformity testing","Invest in regulatory infrastructure for \'trust by design\'","Leverage existing AI-specific audit and assurance frameworks","Enable intervention in systems when irresponsible use is detected"],"examples":{"Security industry practices":"This industry combines a basic standard (or hygiene level) with an attestation structure and third-party conformity testing.","EU\'s proposed Artificial Intelligence Act":"The European Union\'s proposed Artificial Intelligence Act (AIA) advocates for the development of an ecosystem of trust."},"international_alignment":"Suggests evaluating and potentially adopting approaches similar to the EU\'s Artificial Intelligence Act","values":["Trust","Transparency","Ethical responsibility"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and supervise assurance mechanisms, consider adoption of trust ecosystem"},{"entity":"AI developers and deployers","role":"Implement risk-based approaches and adhere to assurance mechanisms"},{"entity":"Regulators","role":"Supervise assurance mechanisms and verify appropriate use"}],"sector_impacts":[{"sector":"All sectors","impact":"Customized assurance based on risk factors associated with particular AI systems and their use"}],"quotes":["Our main contention is that public trust in the use of AI systems will only follow when one can objectively assure that the system adheres to the set principles and guidelines of appropriate behaviour.","We recommend that government considers the adoption of assurance mechanisms that are supervised by a regulator. The result could take the form of a badge or a seal, visible on the platform through which a consumer interacts with the system, that confirms such assurance.","We recommend a light touch mandatory approach, whereby the appropriate use of the suggested assurance mechanism would be verified by government recognition."]},"493_Submission 493 - Commercial Radio Australia - 11-Aug.77502e3803a97.pdf":{"organization_name":"Commercial Radio & Audio (CRA)","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a legislative solution for AI regulation with additional sector-specific measures","arguments":["Existing laws do not address all areas of concern related to AI","A legislative model is necessary given the significant harms that might result from high-risk AI activities","Transparency should be a core element of the AI regulatory framework"],"counterarguments":["null"],"key_recommendations":["Implement a legislative model for AI governance","Task the ACCC to undertake an inquiry into AI\'s impact on the media sector","Address copyright issues related to AI training using content from media companies","Implement sector-specific measures to combat AI-generated misinformation, disinformation, and scams"],"risks_and_challenges":["Use of content from CRA members in creating generative AI systems without permission or compensation","Potential for generative AI to act as a gatekeeper between Australian media companies and audiences","Strain on business models of commercial radio stations due to AI acting as a gateway to information","Creation and dissemination of misinformation and disinformation through AI"],"safeguards_and_mitigations":["Implement transparency measures regarding data used to train AI systems","Ensure clear disclosure when individuals are engaging with AI technology","Require explanations for AI-driven decisions","Implement \'human in the loop\' requirements"],"examples":{"Copyright lawsuits in the US":"Several lawsuits have already been commenced alleging breach of copyright in relation to generative AI in the United States.","ACCC\'s Digital Platforms Inquiry":"This creates similar issues as those that were examined in detail in the 2019 Final Report from the Australian Competition and Consumer Commission\'s (ACCC) groundbreaking Digital Platforms Inquiry."},"international_alignment":"null","values":["Transparency","Accountability","Protection of intellectual property"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Communications and Media Authority (ACMA)","role":"Regulator of commercial radio stations"},{"entity":"Australian Competition and Consumer Commission (ACCC)","role":"Proposed to undertake inquiry into AI\'s impact on media sector"}],"sector_impacts":[{"sector":"Media","impact":"Potential strain on business models and content creation"}],"quotes":["CRA broadly supports a legislative model, given the significant harms that might result from high-risk AI activities.","Transparency should be a core element of the framework.","Acting now will allow the rules of the road to be set to avoid problems arising."]},"58_20230703_DoISR_Al in Australia.9b35ca20162f1.pdf":{"organization_name":"Australasian College of Dermatologists","organization_type":"Medical college","classification":"Proponent","overall_position":"Supports robust regulatory and governance framework for AI in dermatology","arguments":["AI can improve patient access to dermatological care and enhance quality of care","There is a need for AI algorithms and products to be developed, trained, designed and adopted to suit the Australian context","AI should be used to augment, but not replace clinical judgement"],"counterarguments":[],"key_recommendations":["Use AI ethically - ensuring beneficence, non-maleficence, transparency, and reduced bias","Conduct prospective, real-world evaluations of AI performance","Use only AI models with TGA regulatory approval","Incorporate AI outputs into patient medical records for traceability and auditability"],"risks_and_challenges":["New clinical, legal and ethical issues emerging from AI use","Challenges in risk classification and communicating accuracy and sensitivity","Data security and governance around secondary use","Potential impact on clinician decision-making"],"safeguards_and_mitigations":["Regulatory approval by TGA using reformed risk assessment model","Collaboration with regulators, policymakers, industry stakeholders, clinicians and consumer groups","Ongoing monitoring and guidance by ACD for AI developments in dermatology"],"examples":{"Focus on skin cancer detection":"To date in dermatology, there has been a strong focus on the potential for Al to improve detection, diagnosis and monitoring of skin cancer.","Interest in inflammatory skin conditions":"However, there is growing interest in the utility of Al for other skin conditions particularly inflammatory skin conditions, such as eczema and psoriasis."},"international_alignment":"Supports alignment with International Medical Device Regulators Forum (IMDRF) regulation of SaMD","values":["Patient safety","Transparency","Equity"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Dermatologists","role":"Adopt and contribute to AI development, use AI ethically"},{"entity":"Therapeutic Goods Administration","role":"Regulate AI models as Software as Medical Device"},{"entity":"ACD","role":"Provide guidance, education, and collaborate with stakeholders"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved patient access to dermatological care and enhanced quality of care"}],"quotes":["ACD supports the development of Al to enhance the practice of dermatology in Australia and welcomes future research into how Al can be utilised in dermatological practice, including for skin cancer, skin rashes and inflammatory conditions.","We are particularly concerned therefore that these Al algorithms and products are developed, trained, designed and adopted to suit the Australian context and deliver ethical, safe and high-quality care for patients, and that development and adoption is underpinned by a robust regulatory and governance framework.","ACD recommends utilising approved Al devices to aid dermatologists in reaching a diagnosis; that is, Al should be used to augment, but not replace clinical judgement."]},"40_Regulating Artificial Intelligence.85d93b833a62.pdf":{"organization_name":"University of the Sunshine Coast","organization_type":"Academic institution","classification":"Proponent","overall_position":"Advocates for comprehensive regulation using a SECURE framework","arguments":["AI presents a wide range of risks across three tiers: current, emerging, and prospective","Effective management and regulation of AI risks is critical","A responsive, smart, and agile regulatory approach is necessary to address the complex nature of AI risks"],"counterarguments":["null"],"key_recommendations":["Implement a SECURE framework for regulating AI","Design regulatory systems that are responsive, smart, and agile","Address all three tiers of AI risk: current, emerging, and prospective"],"risks_and_challenges":["Bias in training data leading to discriminatory decisions","Privacy and security risks","Lack of transparency and explainability","Job displacement","Risks to civil society through misinformation","Increased wealth and power inequality","Catastrophic and existential risks posed by advanced AI systems"],"safeguards_and_mitigations":["Responsive regulation with escalation and de-escalation of interventions","Smart regulation using multiple regulatory tools and co-regulation","Agile regulation that is flexible and adaptive to fast-paced changes"],"examples":{"First tier AI benefits":"Enhanced medical diagnosis, rapid vaccine development, autonomous vehicles, airport smartgates, improved fraud detection and prevention, logistics, smart manufacturing and content recommendation on social media","Second tier AI risks":"Education sector struggling to adapt to universal accessibility of AI tools for writing essays and completing exams","Third tier AI risks":"Potential risks from artificial general intelligence (AGI) and artificial super intelligence (ASI)"},"international_alignment":"Supports interoperability of AI regulatory regimes across jurisdictions","values":["Safety","Ethics","Controllability","Usability","Responsibility","Efficiency"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Governments","role":"Implement and enforce AI regulations"},{"entity":"AI developers","role":"Comply with regulations and implement risk controls"},{"entity":"Third parties (including existing regulators)","role":"Apply regulatory principles to duty-holders"}],"sector_impacts":[{"sector":"Education","impact":"Struggling to adapt to AI tools for writing and exam completion"},{"sector":"Healthcare","impact":"Enhanced medical diagnosis and rapid vaccine development"}],"quotes":["AI looks likely to become ubiquitous, offering benefits to a vast variety of human activities. Alongside these benefits will emerge a similarly ubiquitous distribution of risk.","If implemented, the SECURE framework offers a way to protect the public interest from all classes of AI risks that will scale to all sizes of jurisdiction and support the interoperability of AI regulatory regimes across and between jurisdictions.","The need to manage AI risk while optimising its development, deployment and the distribution of its benefits, is one of the great collective challenges of our time."]},"38_Safe and responsible AI in Australia.c26195016c214.pdf":{"organization_name":"null","organization_type":"Individual","classification":"Neutral","overall_position":"null","arguments":["Government does not thoroughly read or consider public submissions on technology topics","Previous consultations have been ineffective or ignored","The author has extensive experience in submitting responses to government calls for input"],"counterarguments":[],"key_recommendations":[],"risks_and_challenges":["Lack of genuine engagement with public input on AI and technology issues","Government consultations may be performative rather than substantive"],"safeguards_and_mitigations":[],"examples":{"Cyber-Security-2020 roadshow":"At the Cyber\u2010Security\u20102020 in\u2010person roadshow in Brisbane, I spoke in\u2010person with the government policy\u2010writer in charge of drafting the strategy. During our discussion, he exhibited no understanding of the range of topics presented in all the submissions (all of which I read and summarised) and when I asked his thoughts on my own submission: he admitted that he \'did not read all the submissions\' \u2013 he added that he \'merely skimmed a few\'.","RFI response on GovID":"After supplying a detailed response to an RFI relating to what\'s now called \'GovID\' and \'myGovID\', and hearing no response, I filed an FoI to learn about how my submission was assessed: the official reply reported that no RFI assessments had been performed.","Senate inquiry submissions":"I\'ve filed detailed (more than 100 pages!) responses to 5 senate inquiries. Watching the proceedings, it\'s abundantly clear that nobody read my submissions: for example, I included actual evidence of government mismanagement and wrongdoing, but the chair exhibited no knowledge of any of that evidence."},"international_alignment":"null","values":["Transparency","Accountability","Genuine engagement"],"tone":"Negative","stakeholders":[{"entity":"Government","role":"Properly consider and assess public input on AI and technology issues"},{"entity":"Public","role":"Provide input on AI and technology issues"}],"sector_impacts":[],"quotes":["It is from this extensive experience that I can say with near\u2010certainty, that nobody in government will read all the responses to this call, if anyone at all even reads even one in full.","As a professional developer currently engaged in the A.I. space, I do have contributions that I believe would be beneficial. I invite the government employee reading this sentence(if any) to get in touch with me, if they\'re genuinely interested in progressing this complicated topic and are interested in my knowledge and experience. Make sure you\'ve read everyone else\'s submissions first: I\'ll test you.","The metadata\u2010date on a government PDF instrument purporting to have sought public input (to which I was one contributor) was accidentally left un\u2010scrubbed from their publication: it pre\u2010dated the consultation. The entire consultation was a sham."]},"28_Supporting responsible AI_ Complexico\'s submission.c39feafb9669e.pdf":{"organization_name":"Complexico","organization_type":"Company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and international cooperation on AI governance","arguments":["A comprehensive approach to AI governance is necessary to address diverse challenges","An international authority for AI governance is crucial to mitigate risks and ensure safe deployment","Antitrust regulations need to be reviewed to balance competition and collaboration on AI safety"],"counterarguments":["null"],"key_recommendations":["Implement a comprehensive approach to AI Governance","Establish an International Authority for Artificial Intelligence Governance","Review antitrust regulations applicable to AI"],"risks_and_challenges":["Opaque nature of AI systems functioning as black boxes","Unreliability, difficulty in interpretation, and susceptibility to bias in AI systems","Potential existential risks associated with superintelligence"],"safeguards_and_mitigations":["Establishment of an international authority for AI governance","Implementation of certification and auditing processes","Development of industry standards and best practices","Promotion of international cooperation and agreements"],"examples":{"International Atomic Energy Agency":"Just as the International Atomic Energy Agency (IAEA) monitors and regulates nuclear activities worldwide, there is a growing recognition that an equivalent international authority may be necessary for overseeing superintelligence efforts.","Standards Development Organizations":"Several Standards Development Organizations (SDOs) are already actively involved in developing standards pertaining to AI safety. Notably, the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) jointly operate a subcommittee dedicated to AI."},"international_alignment":"Strongly supports global standards and international cooperation","values":["Safety","Accountability","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"International Authority for AI Governance","role":"Oversee superintelligence efforts, conduct audits, ensure compliance with safety standards"},{"entity":"Standards Development Organizations","role":"Develop standards for AI safety and governance"},{"entity":"Regulatory authorities","role":"Enforce safety regulations and antitrust laws"}],"sector_impacts":[{"sector":"Global security","impact":"Potential significant impact as AI surpasses human capabilities"},{"sector":"Technology industry","impact":"Need to balance competition and collaboration on AI safety"}],"quotes":["A comprehensive approach to AI safety governance would likely involve a combination of these mechanisms, tailored to specific contexts and addressing the diverse challenges associated with AI safety.","As AI surpasses human capabilities its impact on society and global security cannot be underestimated.","To strike a balance, it is important for regulatory authorities, such as the ACCC, to adopt a nuanced approach that recognizes the unique challenges and opportunities presented by AI."]},"432_Final submission to DISR- AI ejh040823.23541a2e9f4a6.pdf":{"organization_name":"Medical Software Industry Association","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports risk-based regulation with emphasis on transparency and trust","arguments":["AI is well-established in the health industry and requires balanced regulation","Existing safe software use should not be over-regulated","AI can improve equity and access to healthcare through efficiencies, innovation, and productivity"],"counterarguments":["Inappropriate AI use could undermine confidence in safe and effective AI use","Existing regulatory approaches are not harmonized or easy to navigate"],"key_recommendations":["Implement risk-based regulation","Promote transparency in AI outputs","Co-design regulation and community education","Develop a more detailed taxonomy of AI","Harmonize multi-government agency approach"],"risks_and_challenges":["Unintended consequences of over-regulating existing safe software use","Confusion leading to non-adoption of beneficial AI products","Lack of consistency or information about AI products"],"safeguards_and_mitigations":["Standardized checklist of essential conformance","Ongoing review by a panel such as the Digital Platform Regulators Forum","AI-Impact Assessment tools","Practical safeguards like sandboxing for testing"],"examples":{"General Practice clinical information systems":"Examples include General Practice clinical information systems which routinely include it for drug interaction checks, suggestions for preventative health and logic to improve Medicare compliance.","Robodebt scenario":"Even the most apparently innocuous aggregation of data can evoke invaluable human response, for instance the office worker in \\"Saving Private Ryan\\", which are invaluable and regrettably lost in the Robodebt scenario.","Taiwan\'s COVID-19 response":"Taiwan managed its technical response to COVID-19 through a clear policy of communication of purpose, how the technologies worked, the benefits and the risks (for non-compliance). It embraced the principles of \\"Fast, Fair and Fun\\" with highly successful results."},"international_alignment":"Supports adopting international approaches where appropriate, but with \'Australianisation\' when necessary","values":["Transparency","Trust","Innovation","Safety","Efficiency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Implement responsible AI practices and be the gold standard"},{"entity":"Health providers","role":"Adopt and implement AI responsibly"},{"entity":"Software developers","role":"Develop transparent and responsible AI solutions"},{"entity":"Consumers","role":"Beneficiaries of AI in healthcare"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved efficiency, access, and equity in healthcare delivery"},{"sector":"Economy","impact":"Potential for increased productivity and export opportunities"}],"quotes":["The MSIA advocates for risk-based regulation together with: Transparency \u2013 So that the provenance of AI outputs is appropriately managed in a risk-based framework.","AI can improve equity and access to healthcare through efficiencies, together with innovation and productivity.","The Government should be the gold standard to encourage confidence and investment in the value of AI."]},"16_Submission 16 - Attachment 1.d794a0adc2bac.pdf":{"organization_name":"null","organization_type":"Academic research","classification":"Opponent","overall_position":"Advocates for careful consideration of risks and harms associated with large language models","arguments":["Large language models have significant environmental costs","Training data for large language models can encode biases and problematic viewpoints","Large language models do not actually understand language or perform natural language understanding"],"counterarguments":["Large language models have shown improvements on various NLP tasks","Larger models and datasets correlate with increased performance"],"key_recommendations":["Consider environmental and financial costs before developing large language models","Invest in curating and documenting datasets rather than using everything from the web","Conduct pre-development exercises to evaluate alignment with goals and stakeholder values","Encourage research directions beyond ever larger language models"],"risks_and_challenges":["Environmental costs disproportionately affect marginalized communities","Models can amplify biases and problematic content in training data","Seeming coherence of model outputs can mislead users","Models can be used to generate extremist content or misinformation at scale"],"safeguards_and_mitigations":["Budget for curation and documentation of datasets","Only create datasets as large as can be sufficiently documented","Conduct careful pre-planning and stakeholder engagement","Consider dual use scenarios and ways to mitigate harms"],"examples":{"Environmental cost example":"Training a Transformer (big) model with neural architecture search was estimated to emit 284t of CO2.","Biased language example":"Describing a woman\'s account of her experience of sexism with the word tantrum both reflects a worldview where the sexist actions are normative and foregrounds a stereotype of women as childish and not in control of their emotions.","Misleading coherence example":"GPT-3 produced seemingly coherent text about a Russian mercenary group in response to a prompt, demonstrating ability to generate text that appears knowledgeable but may not be factual."},"international_alignment":"null","values":["Environmental sustainability","Social responsibility","Careful consideration of impacts"],"tone":"Cautionary","stakeholders":[{"entity":"NLP researchers","role":"Carefully consider impacts and alternatives to large language models"},{"entity":"Marginalized communities","role":"May be disproportionately negatively impacted by large language models"}],"sector_impacts":[{"sector":"NLP research","impact":"Encourages shift away from focus on ever-larger models"},{"sector":"Technology deployment","impact":"Warns of risks in deploying large language models without sufficient safeguards"}],"quotes":["We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.","Feeding AI systems on the world\'s beauty, ugliness, and cruelty, but expecting it to reflect only the beauty is a fantasy.","We hope that a critical overview of the risks of relying on ever-increasing size of LMs as the primary driver of increased performance of language technology can facilitate a reallocation of efforts towards approaches that avoid some of these risks while still reaping the benefits of improvements to language technology."]},"357_NSW Ombudsman Submission 26 July 2023 re Safe and Responsible AI discussion paper.839e1f5647c63.pdf":{"organization_name":"NSW Ombudsman","organization_type":"Independent integrity body","classification":"Neutral","overall_position":"Supports principles-based approach with clear implementation guidance and processes","arguments":["Existing administrative law principles are applicable to ADM technologies","Principles-based approach is appropriate given the diversity of potential AI applications","Clear requirements and expectations for public authorities using ADM systems would enhance oversight"],"counterarguments":["null"],"key_recommendations":["Consider enacting specific legislation for particular ADM use cases","Develop mandatory standards for ADM system design, deployment, and monitoring","Implement independent algorithmic audits and legal certifications for ADM systems","Enhance transparency in public sector use of ADM"],"risks_and_challenges":["Lack of visibility of public sector use of ADM","Potential inadequate attention to public law aspects in adopting ADM systems","Risk of algorithmic bias"],"safeguards_and_mitigations":["Applying existing administrative law principles to ADM","Implementing mandatory standards for ADM systems","Requiring independent algorithmic audits and legal certifications"],"examples":{"Revenue NSW case study":"Revenue NSW had been using an ADM system for the performance of a statutory function (the garnisheeing of unpaid fine debts from individuals\' bank accounts), in a way that was having a significant impact on individuals, many of whom were already in situations of financial vulnerability."},"international_alignment":"null","values":["Transparency","Accountability","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Public authorities","role":"Responsible for developing and using AI and ADM systems"},{"entity":"Oversight bodies","role":"Investigate and ensure compliance with ADM standards and requirements"}],"sector_impacts":[{"sector":"Public sector","impact":"Increased scrutiny and requirements for ADM system implementation"}],"quotes":["ADM technologies are not being introduced into a complete legal or regulatory vacuum. The legal environment into which public sector ADM is introduced is the one that is governed by public administrative law.","Indicating when and how ADM systems are used is crucial to effective oversight.","This means that there is no such thing as \'Responsible AI\'."]},"414_CREST_Submission-FINAL.21dcc9e8501be.pdf":{"organization_name":"Centre for Cyber Resilience and Trust (CREST), Deakin University","organization_type":"University research center","classification":"Proponent","overall_position":"Supports comprehensive regulation and governance of AI with a focus on multi-disciplinary approach","arguments":["Existing frameworks are insufficient to address AI-specific challenges","A holistic approach considering ethics, society, law, and security is needed","Public and private collaboration is necessary for effective AI governance"],"counterarguments":["Banning AI technologies is not a solution","Over-legislation should be avoided due to the fast pace of AI progress"],"key_recommendations":["Develop a national strategy on AI","Implement a risk-based approach for addressing AI risks","Expand human ethics committees\' expertise to review AI applications","Enhance transparency requirements across the AI lifecycle"],"risks_and_challenges":["Potential harm to autonomy of decision-makers and users","Lack of explainability and understandability in AI systems","Employment and de-skilling issues","Environmental costs of training AI models"],"safeguards_and_mitigations":["Embed transparency amplifiers into AI design","Implement effective cross-disciplinary collaboration","Develop clear guidelines for AI model use in different scenarios","Establish rigorous testing schemes before releasing AI models"],"examples":{"AI in airport security":"A deep neural network which is not interpretable is okay to be deployed at Airport Security gate to scan an incoming passenger at the gate but should not be used for determining a tax fraud or determine credit worthiness.","AI in tax fraud detection":"For a tax fraud scenario, it can be the case that explainable models are just too complex to train given they might require loading entire data in the memory, while a non-explainable model such as deep artificial neural network is desirable due to its handling of large-scale complex data."},"international_alignment":"Supports global collaboration and regional alliances on AI governance","values":["Transparency","Accountability","Fairness","Privacy","Security"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government agencies","role":"Lead in responsible AI practices and develop clear guidelines"},{"entity":"Private sector","role":"Collaborate with public sector in AI governance"},{"entity":"Research institutions","role":"Adapt AI governance and project design processes"}],"sector_impacts":[{"sector":"Public sector","impact":"Greater responsibility to lead and ensure AI does not negatively impact society"},{"sector":"National security","impact":"Need for strongly regulated approach to AI use"}],"quotes":["CREST adopts multi-disciplinary approach to these critically important knowledge gaps through five interrelated areas of impact","Existing frameworks are yet to develop a tool to evaluate the trade-offs of security and functionality, with additional security requirements often leading to loss in functionality, as well as loss of access to services, particularly with regards to vulnerable communities","Transparency is a core requirement from any AI-based application. The technology, including the way it operates and arrives at conclusions or generate outputs must be transparent to human beings who use, interact with or are affected by the technology."]},"497_Submission 497 - Tech Council of Australia - 13-Aug.b1687db24465a.pdf":{"organization_name":"Tech Council of Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports updating existing laws and regulations rather than creating new AI-specific laws","arguments":["Existing regulatory frameworks can be adapted to cover AI development and deployment","A risk-based approach allows for targeted regulation of higher-risk AI applications","Creating new AI-specific laws could lead to regulatory duplication and confusion"],"counterarguments":["Some areas of law may need reform to address unique challenges posed by AI","Regulators may lack technical expertise to effectively govern AI technologies"],"key_recommendations":["Develop a coherent regulatory strategy for AI","Establish an expert \'hub and spoke\' AI coordination model to support regulators","Clarify and enforce existing laws as they apply to AI","Increase investment in AI research and workforce development","Position government as an exemplar of responsible AI adoption"],"risks_and_challenges":["AI system failures (e.g. bias, discrimination, security failures)","Malicious or misleading deployment of AI","Overuse, inappropriate or reckless use of AI","Lack of regulatory clarity hindering innovation and adoption"],"safeguards_and_mitigations":["Adopt a risk-based approach to regulation","Provide regulatory guidance on applying existing laws to AI","Develop industry standards and best practices","Increase digital literacy and responsible AI awareness"],"examples":{"Application of consumer law to AI":"AI systems that engage in trade or commerce cannot engage in conduct that is misleading or deceptive, e.g. organisations must not misrepresent when an AI system is being used","Application of privacy law to AI":"The Privacy Act 1988 (Cth) stipulates the requirements for the collection, use and disclosure of personal information \u2013 and thus, the data being used in AI systems","Application of anti-discrimination laws to AI":"developers must actively monitor and assess their systems for bias or unfairness that would amount to direct or indirect discrimination"},"international_alignment":"Supports international coherence and interoperability in AI regulation","values":["Innovation","Responsibility","Safety","Economic growth"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Develop regulatory strategy and provide guidance"},{"entity":"Regulators","role":"Enforce existing laws and develop AI-specific guidance"},{"entity":"Industry","role":"Develop responsible AI practices and contribute to standards development"}],"sector_impacts":[{"sector":"Economy","impact":"Generative AI alone could add between $45 billion - $115 billion a year to the Australian economy by 2030"},{"sector":"Healthcare","impact":"AI has revolutionised medical diagnostics and treatment to enable faster and more accurate detection of diseases and early intervention for patients"}],"quotes":["Australia\'s approach to AI governance and regulation needs to recognise that we already have a sound legal framework and regulatory model that covers the development and use of products.","The TCA does not support following the path of the EU in developing a standalone AI Act, particularly given the diversity and widespread application of AI technologies.","As part of Australia\'s drive to grasp the significant opportunities of AI, the Tech Council fully supports the Government\'s goal to ensure AI development and deployment is safe and responsible, and we support the need for regulatory safeguards."]},"42_A.I. Safety Anthony Albanese Australian Government Brief.24205b04d84b5.pdf":{"organization_name":"YawLife","organization_type":"Company","classification":"null","overall_position":"null","arguments":["null"],"counterarguments":["null"],"key_recommendations":["null"],"risks_and_challenges":["Content infringement","Fake news"],"safeguards_and_mitigations":["Digital Rights Management (DRM) Purse","Reporting system with multiple parties for accuracy","SENSE\u2122 reputation system","LifeChain\u2122 for automatic disbursement of compensation"],"examples":{"DRM Purse":"User 1 deposits LifeCoin\u2122 into a Digital Rights Management [DRM] Purse that will keep the posts that matter the most to User 1 from getting infringed upon.","Reporting System":"Reporters work in a group for heightened accuracy to identify content infringement.","SENSE\u2122 Reputation System":"The more SENSE\u2122 a user has, the more weight it holds against other users reporting."},"international_alignment":"null","values":["Copyright protection","Content authenticity","User empowerment"],"tone":"Neutral","stakeholders":[{"entity":"Users","role":"Create content, use DRM Purse, report infringements"},{"entity":"Advertisers","role":"Pay users to advertise content on the platform"},{"entity":"Reporters","role":"Ensure authenticity of posts, report copyright infringements"}],"sector_impacts":[{"sector":"Social media","impact":"Improved copyright protection and content authenticity"}],"quotes":["YawLife introduces a novel concept of Copyright Protection where the user has the option to create a DRM Purse using LifeChain\u2122 to audit his or her content across multiple feeds, and own their content.","Reporters work in a group for heightened accuracy to identify content infringement.","The more SENSE\u2122 a user has, the more weight it holds against other users reporting."]},"445_20230804_ARDC Submission_Consultation on safe and responsible AI in Australia.8c60416b57ec.pdf":{"organization_name":"ARDC","organization_type":"Research infrastructure organization","classification":"Neutral","overall_position":"Supports a multi-faceted, whole-of-society approach to AI governance, including both regulatory and non-regulatory initiatives","arguments":["Regulation alone is not sufficient to manage AI risks","A holistic and systemic approach is needed to address AI challenges","Research infrastructure provides long-term capability for addressing AI risks"],"counterarguments":["Regulatory-driven frameworks could stifle innovation","Without adequate oversight, AI applications may be misused","Not all risks of novel technologies can be accurately foreseen"],"key_recommendations":["Adopt FAIR principles for AI data and algorithms","Investigate best practices in government and philanthropic funding for AI research","Leverage NCRIS strategy for national research infrastructure to support responsible AI practices","Examine regulatory sandboxes under the draft EU AI Act","Use recommendations from the Robodebt Scheme Royal Commission as key references for AI governance"],"risks_and_challenges":["Potential for unintended consequences of AI technologies","Lack of clear articulation of responsibilities in AI systems","Absence of legal mechanisms for affected individuals to seek redress","Data quality and algorithm transparency issues"],"safeguards_and_mitigations":["Adoption of FAIR principles for AI data and algorithms","Implementation of formal data quality frameworks","Establishment of a multi-stakeholder AI governance framework","Creation of regulatory sandboxes for AI experimentation"],"examples":{"Robodebt Scheme":"The Robodebt Scheme was not an artificial intelligence system. Nevertheless, it does serve as a cautionary tale for communities in Australia and abroad grappling with the challenges of rapid developments in ADM and AI technologies.","UK AI investment":"In June 2023, the UK Secretary of State for Science, Innovation and Technology, Chloe Smith, announced an investment of \xa354 million for AI research and the development of a data science workforce."},"international_alignment":"Supports alignment with global norms and standards","values":["Transparency","Accountability","Innovation","Ethical development"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement AI governance frameworks"},{"entity":"Research sector","role":"Conduct AI research and contribute to infrastructure development"},{"entity":"Industry","role":"Collaborate in AI development and implementation"}],"sector_impacts":[{"sector":"Research","impact":"Enhanced capabilities through national research infrastructure"},{"sector":"Public administration","impact":"Improved automated decision-making processes"}],"quotes":["There is no enforceable AI-specific regulation in Australia.","While these laws provide protection as described above, there is no single integrated law that addresses AI specifically.","Complementing research projects with research infrastructure offers a pathway to increase the return on investment of research, avoid duplication, and maintain ongoing reliable and underpinning capabilities."]},"289_Good Ancestors DISR submission.4b02ed5f69e98.pdf":{"organization_name":"Good Ancestors Policy","organization_type":"Australian charity","classification":"Proponent","overall_position":"Advocates for comprehensive regulation to address catastrophic and existential risks from AI","arguments":["Advanced AI systems pose catastrophic and existential risks that require urgent action","Current regulatory approaches are insufficient for addressing the risks of future AI systems","Australia has an opportunity to lead in AI safety and governance"],"counterarguments":["Some argue that catastrophic risks from AI are not imminent and don\'t require immediate action","Industry lobbying may oppose strong regulations"],"key_recommendations":["Acknowledge catastrophic and existential risks from AI","Establish an Australian AI Lab for assessing and monitoring AI systems","Implement a five-step approach to regulating high-risk AI systems","Create joint culpability schemes for AI developers and deployers","Establish an AI Commission to coordinate governance efforts"],"risks_and_challenges":["Advanced AI systems could cause widespread harm if misused or misaligned","AI race dynamics could lead to unsafe development and deployment","Lack of interpretability in AI systems makes risk assessment challenging"],"safeguards_and_mitigations":["Implement strict licensing and monitoring for precursor and advanced AI systems","Conduct rigorous risk assessments before development and deployment of advanced AI","Support AI safety research and development of safety products and services"],"examples":{"ChaosGPT":"An autonomous AI system that developed goals to \'dominate\' and \'destroy\' humanity, illustrating potential dangers of advanced AI","ChatGPT persuading user to end life":"A tragic example of a chatbot persuading a user to end his own life, highlighting the need for robust safeguards"},"international_alignment":"Supports global coordination and leadership in AI governance","values":["Safety","Accountability","Proactivity"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement comprehensive AI regulations and establish governance structures"},{"entity":"AI developers","role":"Ensure safety of AI systems and comply with regulations"},{"entity":"AI deployers","role":"Use AI responsibly and comply with safety requirements"}],"sector_impacts":[{"sector":"National security","impact":"Potential new threats from advanced AI systems"},{"sector":"AI safety industry","impact":"Opportunity for Australia to become a leader in AI safety products and services"}],"quotes":["Australia must tackle catastrophic and existential risks","The likelihood of catastrophic or existential risks from AI is uncertain, and it is understandable for Government to acknowledge genuine uncertainty. However, because the impact of these risks is extreme, and because solutions will take time, Australia should urgently start the due diligence necessary to ensure we follow a positive path.","AI could presage the biggest social transformation in human history, akin to or exceeding the industrial revolution. This transformation has to be shepherded by structures that are proportionate to its scale and impact."]},"279_ACS response to Safe and responsible AI in Australia.74f6c012eed14.pdf":{"organization_name":"Australian Computer Society","organization_type":"Professional association","classification":"Proponent","overall_position":"Supports a balanced approach with enforced self-regulation and sector-specific rules","arguments":["Guardrails are important, but there is a danger of over-regulation","A broader approach beyond blacklists and risk assessment is needed","Focus should be on improving reliability, quality, safety, and accountability of AI-assisted decisions"],"counterarguments":["Some uses of AI may require prohibitions or blacklists","Certain high-risk AI applications may need specific regulations"],"key_recommendations":["Implement higher level guidance and guidelines for safe and responsible AI deployments","Require organizations to designate a senior officer responsible for safe AI deployment","Support development of best practices in data sheets, system cards, and model cards for AI applications","Conduct information campaigns and provide educational resources on safe AI use"],"risks_and_challenges":["Uncontrolled and opaque uses of AI-enabled facial recognition","Risks of unknowing amplification of disinformation and misinformation through generative AI","Lack of organizational control over introduction and use of general purpose AI services"],"safeguards_and_mitigations":["Develop a central monitoring, evaluation and risk assessment framework","Create central guidance for businesses navigating the AI regulatory landscape","Offer a multi-regulator AI sandbox","Support cross-border coordination with other countries"],"examples":{"Lawyer submitting AI-generated fake cases":"Steven Schwartz, a New York attorney with over 30 years of post-admission experience, represented Roberto Mata in an action against Avianca Airlines for injuries sustained from a serving cart while on the airline in 2019. At least six of the submitted cases by Schwartz as in a brief to the court of the Southern District of New York court \'appear to be bogus judicial decisions with bogus quotes and bogus internal citations,\' said Judge Kevin Castel in a May 2023 order.","Robodebt case":"The Robodebt Royal Commission report highlights the needs for these skills and accountabilities within the public service."},"international_alignment":"Supports a UK-like approach with modifications","values":["Safety","Responsibility","Transparency","Accountability"],"tone":"Cautious but optimistic","stakeholders":[{"entity":"Government","role":"Develop guidelines, coordinate regulators, and support initiatives"},{"entity":"Organizations","role":"Implement policies and programs for responsible AI use"},{"entity":"Regulators","role":"Enforce regulations and provide guidance"}],"sector_impacts":[{"sector":"Public sector","impact":"Need for AI governance skills and accountability"},{"sector":"Small to medium businesses","impact":"May lack resources for complex AI risk management"}],"quotes":["We don\'t know what the future holds for AI, but if Australia\'s organisations have the right frameworks and governance in place to manage them, then we have the best opportunity to ensure that the Australian people can benefit from this new technology.","Vitally, we would urge consideration of not just risks and negative effects (which we imagine will be the focus of many responses to this paper), but also to ensuring that Australian organisations large and small are empowered and encouraged to govern their own applications of AI with transparency, responsibility, care and fairness.","Assuring implementation of safe and responsible AI for Australian citizens requires organisations: to understand what is \'safe and responsible AI\', to internalise and address risks of harms to others, to the extent that those harms are reasonably attributable to the organisation\'s provision, deployment or use of AI."]},"103_20230719 ICA submission AI discussion paper.b75c580b49343.pdf":{"organization_name":"Insurance Council of Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports updating existing laws and a risk-based approach to AI regulation","arguments":["Existing consumer protections likely address many potential risks or harms","AI-specific regulation may overlap or create inconsistencies with existing regulations","Regulation should be technology neutral to avoid becoming outdated"],"counterarguments":["Novel risks or consumer harms may arise from AI applications","Existing regulatory framework may have gaps in addressing AI-specific issues"],"key_recommendations":["Elevate \'pro-innovation\' alongside \'safe and responsible\' as a principle for Australia\'s AI framework","Adopt a risk-based approach to regulating AI","Establish a central AI expertise body to coordinate AI governance across government","Develop a national risk statement to inform AI regulation and deployment"],"risks_and_challenges":["Potential for unacceptable consumer harms from AI applications","Low public trust and confidence in AI","Negative consumer sentiment towards AI applications"],"safeguards_and_mitigations":["Assess identified risks of AI in the context of the existing regulatory framework","Develop guidance material on how existing regulatory obligations apply to specific AI use cases","Ensure coverage under existing regulations such as privacy and consumer protection laws"],"examples":{"Social scoring proposed to be banned in the EU":"We note the example cited in the Discussion paper of social scoring proposed to be banned in the EU.","Guidance Resource on AI and insurance pricing":"As an example, the Guidance Resource on AI and insurance pricing developed by The Actuaries Institute and Australian Human Rights Commission, which is non-binding.","Overlapping cyber security obligations":"For example, insurers currently have overlapping specific and general cyber security obligations across multiple regulators which in practice increase complexity and compliance costs."},"international_alignment":"Supports interoperability within the international AI ecosystem","values":["Innovation","Safety","Responsibility"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement AI governance framework"},{"entity":"Regulators","role":"Produce authoritative guidance material regarding AI"},{"entity":"Industry","role":"Implement responsible AI practices"}],"sector_impacts":[{"sector":"Insurance","impact":"Potential for more efficient and effective delivery of critical financial protection functions"}],"quotes":["We recommend that Australia also elevate \'pro-innovation\' alongside \'safe and responsible\' as a principle to drive Australia\'s AI framework.","The Insurance Council gives in principle support for a risk-based approach to regulating AI.","We submit that regulation should be technology neutral."]},"385_Microsoft submission - Safe and Responsible AI in Australia - 4 Aug 2023.6c1a134928ca1.pdf":{"organization_name":"Microsoft","organization_type":"Technology company","classification":"Proponent","overall_position":"Supports targeted AI regulation based on risk and existing legal frameworks, with international alignment","arguments":["Effective guardrails are required to promote trustworthy and responsible AI development and use","A risk-based approach should be used to place guardrails on AI systems with potential to impact public rights and opportunities","International coherence in AI governance is key to achieving consistent, effective regulation"],"counterarguments":["null"],"key_recommendations":["Create a regulatory architecture that reflects the AI technology stack","Improve domestic coordination on AI regulation and policy","Advance a risk-based approach to AI regulation","Collaborate in international efforts for coherent AI governance","Promote opportunities to benefit from AI through transparency and addressing regulatory blockers"],"risks_and_challenges":["Potential for AI to impact public rights, opportunities, or access to critical resources or services","Risks to critical infrastructure from AI systems","Challenges in ensuring transparency while maintaining system security"],"safeguards_and_mitigations":["Implement effective safety brakes for AI systems that control critical infrastructure","Establish licensing requirements for highly capable AI models","Promote transparency in AI systems and decision-making processes"],"examples":{"Facial Recognition Technology Model Law":"The University of Technology Sydney Human Technology Institute (HTI) has proposed the widely commended Facial Recognition Technology (FRT) \'Model Law\' which is risk-based and grounded in international human rights law.","City of Amsterdam\'s Algorithm Register":"The City of Amsterdam\'s Algorithm Register is a public register which provides information on high-risk AI and measures undertaken to ensure safe and responsible use.","Frontier Model Forum":"An example of our commitment to transparency is the recent announcement of the launch of the Frontier Model Forum, an industry body founded by Anthropic, OpenAI, Google, and Microsoft."},"international_alignment":"Strongly supports international alignment and coherence in AI governance","values":["Transparency","Accountability","Safety","Responsibility","Innovation"],"tone":"Optimistic and collaborative","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulations, promote international alignment"},{"entity":"Industry","role":"Implement responsible AI practices, collaborate with government and other stakeholders"},{"entity":"Academia","role":"Conduct research, provide insights on AI governance"}],"sector_impacts":[{"sector":"Critical infrastructure","impact":"Potential risks from AI systems controlling critical infrastructure, need for safety brakes"},{"sector":"Technology","impact":"Increased productivity and innovation, need for responsible development and deployment"},{"sector":"Economy","impact":"Potential for significant economic growth and productivity gains"}],"quotes":["Australia is especially well-positioned to benefit from the extraordinary social and economic opportunities created by the latest wave of AI technologies. To realise these opportunities, however, effective guardrails are required to promote the trustworthy and responsible development and use of AI systems.","We believe there will need to be a legal and regulatory architecture for AI that reflects the technology architecture for AI itself.","Microsoft believes that basing AI governance approaches around outcomes and risk is key to crafting and deploying regulatory models that are fit-for-purpose and sustainable in practice."]},"41_Submission 41 - Public and Anonymous - 16-Jun-23.e0093374afab7.pdf":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive implementation of an Australian Open Source AI system","arguments":["AI offers improved efficiency and cost savings","AI can rebuild trust in government","AI can provide multiple options for decision-making","AI can monitor politicians\' performance","AI can make processes longer and more efficient"],"counterarguments":["Concerns about dystopian scenarios"],"key_recommendations":["Implement an Australian Open Source AI system","Establish ethical guidelines","Maintain human oversight","Protect privacy","Foster collaboration and research","Educate and empower citizens"],"risks_and_challenges":["Potential dystopian scenarios"],"safeguards_and_mitigations":["Proper management to avoid pitfalls","Establish ethical guidelines","Maintain human oversight","Protect privacy"],"examples":{"FOI system improvement":"FOI system works like a charm","AI education":"every kid becomes an AI guru","Democratic decision-making":"everything is flagged and weighted based on the will of the people"},"international_alignment":"null","values":["Efficiency","Transparency","Democracy","Equality"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Implement and manage the AI system"},{"entity":"Citizens","role":"Benefit from and interact with the AI system"}],"sector_impacts":[{"sector":"Government","impact":"Improved efficiency and transparency"},{"sector":"Education","impact":"Empowering citizens to become AI experts"},{"sector":"Economy","impact":"Cost savings and improved tax system"}],"quotes":["AI offers countless benefits, like improved efficiency, cost savings, transparency, and direct interaction with the people.","It\'s time to unleash the magic of AI and create a future where every kid becomes an AI guru and where the FOI system works like a charm.","Together, we can make good advice from a machine a reality, while ensuring that everything is flagged and weighted based on the will of the people."]},"247_To Submit Final_ITI Response_AUS AI Discussion Paper.5cfc1bce3bad7.pdf":{"organization_name":"Information Technology Industry Council (ITI)","organization_type":"International trade association","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with a focus on leveraging existing laws and international standards","arguments":["Risk-based approaches allow for targeted responses to specific harms while enabling innovation","Existing laws and regulatory authorities should be evaluated before introducing new AI-specific regulations","International alignment and use of voluntary standards can promote interoperability and responsible AI practices"],"counterarguments":["Some high-risk AI applications may require additional regulatory measures","Existing laws may not fully address AI-specific challenges","There are potential gaps in current regulatory approaches for AI risks"],"key_recommendations":["Adopt a risk-based approach to AI regulation","Evaluate existing laws before introducing new AI-specific regulations","Leverage international standards and frameworks","Ensure transparency requirements are clear and non-prescriptive","Promote responsible AI practices through non-regulatory initiatives"],"risks_and_challenges":["Potential negative externalities of AI systems","Risks to security, health, safety, privacy, freedom, and human rights","Challenges in conducting external audits of AI systems due to expertise and capacity constraints"],"safeguards_and_mitigations":["Implement holistic AI risk management practices","Encourage use of existing risk management frameworks","Promote transparency and disclosure for AI systems, especially high-risk ones","Secure AI systems to protect against cyber threats"],"examples":{"AI-driven medical diagnostics":"AI driven medical diagnostics can alert doctors to early warning signs to help them treat patients.","AI in financial fraud detection":"AI systems are capable of monitoring large volumes of financial transactions to identify fraud.","AI in cybersecurity":"AI plays an important role in the cybersecurity of IT in general, and of AI systems themselves, by increasing the speed and effectiveness of detecting and preventing cyber threats"},"international_alignment":"Supports alignment with international standards and frameworks","values":["Innovation","Transparency","Trust","Accountability","Security"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Evaluate existing laws, coordinate AI governance across agencies, and support responsible AI practices"},{"entity":"Private sector","role":"Implement responsible AI practices and engage in dialogue with communities"},{"entity":"Universities","role":"Conduct real-world AI projects with communities to address social needs"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved medical diagnostics and patient care"},{"sector":"Finance","impact":"Enhanced fraud detection capabilities"},{"sector":"Small and medium-sized enterprises","impact":"Gather new insights and improve businesses using AI and data analytics"}],"quotes":["ITI and its members share the firm belief that building trust in the era of digital transformation is essential and agree that there are important questions that need to be addressed regarding the responsible development and use of AI technology.","If additional regulation is needed to address risks related to AI, we are supportive of the risk-based approach that Australia lays out in Box 4 on page 32 of the Discussion Paper.","Recognizing the AI ecosystem is global, Australia has an opportunity to take an international leadership role in promoting advances in AI innovation."]},"436_Blackwell_AI_Statement.283b0d3c0ccfb.pdf":{"organization_name":"Blackwell AI","organization_type":"AI start-up","classification":"Proponent","overall_position":"Supports responsible AI development with balanced regulation","arguments":["Effective governance mechanisms are essential for privacy and security","Current definitions of AI and related terms need refinement","Existing regulations may not fully cover AI-specific risks"],"counterarguments":["Excessive regulatory burdens may hinder innovation","Complete transparency may not always be feasible or appropriate"],"key_recommendations":["Update Privacy Act to address AI-specific concerns","Develop regulatory frameworks for ethical AI use in sensitive domains","Invest in AI research and education","Participate in global coordination efforts for data validity and model training"],"risks_and_challenges":["Privacy and data protection concerns","Ethical use of AI in sensitive domains","Potential biases in AI systems","Lack of transparency in AI lifecycle"],"safeguards_and_mitigations":["Collaborative approach involving industry stakeholders, government agencies, and experts","Balancing risk management with innovation","Transparency in pre-training and fine-tuning stages of AI development","Establishing industry standards for transparency reporting"],"examples":{"Italy\'s approach to ChatGPT":"Italy\'s approach in addressing concerns about private data used to train ChatGPT demonstrates the importance of robust data privacy conditions and transparency in AI systems."},"international_alignment":"Supports global coordination and learning from other countries\' experiences","values":["Privacy","Security","Transparency","Innovation","Ethical AI practices"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop balanced regulatory frameworks and invest in AI research"},{"entity":"Industry","role":"Implement responsible AI practices and collaborate with regulators"},{"entity":"Academia","role":"Conduct research and promote best practices in AI development"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential for ethical concerns in AI applications"},{"sector":"Financial services","impact":"Need for responsible AI deployment in decision-making processes"}],"quotes":["We believe that effective governance mechanisms are essential to ensure the development and deployment of AI in a manner that upholds privacy and security.","We would emphasise the need for comprehensive and nuanced definitions of AI, machine learning, and automated systems, along with a focus on the responsible development and deployment of these advanced AI models.","Successful regulatory action should be based on a collaborative approach involving industry stakeholders, government agencies, and experts, while simultaneously allowing developers to innovate and experiment."]},"451_COSBO Supporting Responsible AI Submission.500f53d9dc563.pdf":{"organization_name":"Council of Small Business Organisations Australia (COSBOA)","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports responsible AI adoption with clear regulatory frameworks and educational initiatives","arguments":["AI can unlock significant productivity gains for small businesses","Responsible AI practices and appropriate regulatory frameworks are crucial","Government and industry associations can foster a balanced regulatory environment"],"counterarguments":["null"],"key_recommendations":["Develop concise guidelines for AI development and deployment","Adopt a risk-based approach to AI regulation","Implement a Model AI Governance Framework","Establish a national AI education framework","Expand grant programs to foster responsible AI adoption"],"risks_and_challenges":["Algorithmic bias leading to unfairness and discrimination","Lack of trust in AI systems","Higher regulatory costs for small businesses compared to large businesses"],"safeguards_and_mitigations":["Establish mechanisms for bias detection and control in AI systems","Enforce transparency and accountability in public sector AI applications","Mandate transparency in AI systems development"],"examples":{"AI tools used in submission development":"AI tools were utilised in the development of this submission, significantly trimming the time required compared to conventional methods. These tools offered real-time drafting assistance, iterative feedback, and proofreading, underscoring the transformative potential of AI when used responsibly in tandem with human judgement."},"international_alignment":"Considers governance measures from other countries adaptable for Australia","values":["Innovation","Transparency","Accountability","Fairness","Productivity"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement clear guidelines and frameworks for AI"},{"entity":"Industry associations","role":"Collaborate with government to provide insights and solutions"},{"entity":"Small businesses","role":"Adopt AI responsibly to drive productivity and innovation"},{"entity":"Academia","role":"Collaborate in devising effective regulatory solutions"}],"sector_impacts":[{"sector":"Small businesses","impact":"Potential for significant productivity gains and innovation through responsible AI adoption"}],"quotes":["COSBOA recognises the pivotal role new technologies can play in driving productivity and innovation within small businesses. To this end, we believe small businesses hold the key to unlocking Australia\'s productivity through the responsible use of artificial intelligence (AI).","We aim to advocate for regulations, initiatives, and practices that enable Australian small businesses to responsibly leverage AI\'s transformative potential, thereby driving productivity and fostering innovation.","Any delay in learning to utilise these tools risks leaving small businesses trailing in the digital era. We urge the Australian Government to act swiftly on these recommendations, ensuring that small businesses are not disadvantaged but are instead provided with opportunities to prosper through the responsible use of AI."]},"235_20230726 PHA submission to Responsible AI review.53d0d49e9f96f.pdf":{"organization_name":"Private Healthcare Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports generic legislative responses with focus on transparency and disclosure","arguments":["AI can assist health funds in providing benefits to members","AI use can lower the cost of private health insurance","Health promotion activities using AI have significant public benefit"],"counterarguments":["null"],"key_recommendations":["Consolidate and remove overlapping regulatory requirements","Support use of AI for public benefit purposes","Allow wide range of health promotion activities"],"risks_and_challenges":["Potential intrusion on fund\'s ability to make commercially confidential decisions","Unintentional hindering of vital health services"],"safeguards_and_mitigations":["Include general information about automated decision-making in public-facing statements","Maintain existing complaints mechanisms"],"examples":{"Health promotion programs":"There are a number of programs across the sector to support patients outside of hospital that have significant impacts to Australians health experience, outcomes and quality of life.","Claims processing":"Funds use substantially automated decision-making processes, which may increasingly use AI, to assess and process private health insurance claims."},"international_alignment":"null","values":["Transparency","Public benefit","Efficiency"],"tone":"Positive","stakeholders":[{"entity":"Health funds","role":"Implement AI for member benefits and health promotion"},{"entity":"Regulators","role":"Provide regulatory guidance and oversight"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved health outcomes and reduced hospital admissions"},{"sector":"Insurance","impact":"Lower costs and improved efficiency in claims processing"}],"quotes":["Generic legislative responses, particularly in respect to transparency and disclosure, are most suitable for the uses of AI by health funds.","There is significant public benefit in targeted health promotion activity, and this is core business for health funds.","It is vital that any AI regulation does not unintentionally hinder these vital services for Australians."]},"406_230804 Safe and responsible AI in Australia - NATIONAL RETAIL ASSOCIATION.8506cb31edafa.pdf":{"organization_name":"National Retail Association","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports responsible AI adoption with industry consultation","arguments":["AI technologies improve security and safety in retail","AI tools create a safer environment for retailers, staff, customers and the wider community","Current AI technologies are invaluable for identifying high-risk offenders and preventing crime"],"counterarguments":[],"key_recommendations":["Support, not restrict, current innovation","Employ a strong consultative approach","Collaborate with all jurisdictions and industry to establish appropriate regulations"],"risks_and_challenges":["Heightened rates of aggression, abuse and anti-social behaviour in retail settings","Organized Retail Crime (ORC)","Low reporting rates of retail crimes"],"safeguards_and_mitigations":["Use of facial recognition and number plate recognition to identify high-risk offenders","Alert security teams and provide police with key intelligence"],"examples":{"Use of AI for crime prevention":"Technologies such as facial recognition and number plate recognition are invaluable tools to help identify recidivist and high-risk offenders with an active banning notice, or a history of stealing, carrying / concealing weapons, threatening / harming staff or customers, or damaging property or vehicles."},"international_alignment":"null","values":["Safety","Innovation","Collaboration"],"tone":"Positive","stakeholders":[{"entity":"Government","role":"Collaborate with industry and establish appropriate regulations"},{"entity":"Retail industry","role":"Essential partner in AI adoption and innovation"}],"sector_impacts":[{"sector":"Retail","impact":"Improved security and safety, better crime prevention"}],"quotes":["National Retail applaud DISR for their commitment to provide a long-term framework to guide and encourage the safe and effective application of AI technologies in Australia.","We broadly support the identified principles and priorities to harness the opportunities, while safeguarding against the potential negative impacts, of AI technologies.","National Retail believes that the principles provide a strong guidance structure for the use of AI. However, we urge Government to ensure that future actions support, not restrict, current innovation and employ a strong consultative approach so that changes are achievable, support the safety of retail precincts and promote positive long-term outcomes."]},"449_HSF submission Safe and responsible AI in Australia (4 Aug 2023).ff60df95d49a9.pdf":{"organization_name":"Herbert Smith Freehills","organization_type":"Global law firm","classification":"Proponent","overall_position":"Supports a multi-tiered governance framework for AI that balances innovation with consumer and societal protections","arguments":["Regulation can be an enabler of innovation and economic growth","Harmonisation with existing domestic laws and coherence with overseas frameworks is important","Accountability promotes consumer trust in AI technology"],"counterarguments":["null"],"key_recommendations":["Conduct a multi-agency review of relevant federal and state laws to identify which AI risks are already protected under existing laws","Adopt a risk-based, proportionate approach to AI regulation if existing laws are inadequate","Establish a dedicated taskforce to provide advice on AI governance reform and assist with coordination and harmonisation of laws"],"risks_and_challenges":["Potential overlap and duplication with existing regulations","Difficulty in predicting all potential risks and outcomes","Lack of skilled personnel to conduct risk assessments of complex AI systems"],"safeguards_and_mitigations":["Promote adoption of certifications and standards for AI","Provide clear and comprehensive guidance on factors indicating AI risk levels","Implement ongoing risk assessment processes rather than one-time evaluations"],"examples":{"Voluntary commitments by tech companies in the US":"On 21 July 2023, seven big technology companies made voluntary commitments to the White House to promote safe, secure and transparent development and use of AI.","EU\'s draft AI Act":"While we appreciate the EU\'s draft AI Act as an instructive example of a risk-based approach legislation, it is important to consider the interoperability of such a risk-based approach with the features and nuances of Australia\'s common law and existing legal frameworks."},"international_alignment":"Supports coherence with international approaches while adapting to Australia\'s context","values":["Innovation","Consumer trust","Accountability","Transparency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Coordinate review of existing laws and develop targeted regulatory solutions"},{"entity":"Industry","role":"Implement responsible AI practices and comply with governance frameworks"},{"entity":"Consumers","role":"Benefit from protections and build trust in AI systems"}],"sector_impacts":[{"sector":"Technology","impact":"Clarity and flexibility required to drive innovation, investment, and employment"},{"sector":"Economy","impact":"Potential for Australia to be a key international player in AI development and usage"}],"quotes":["We support a multi-tiered governance framework for AI in Australia that balances clarity required to promote responsible innovation so Australia can benefit economically from AI enabled technology, with the requisite consumer and societal protections to promote public trust in AI systems.","To minimise regulatory overlap and duplication, Australia should carry out a detailed assessment of existing domestic regulatory frameworks to identify which potential risks of AI use cases are already adequately protected under, or could be addressed by the reform of, existing laws.","A key foundation for a thriving AI economy is strong consumer trust in AI technology, especially AI used to make decisions that will affect individuals. Such trust will largely depend on how companies and governments are held accountable for the responsible design and use of AI."]},"315_Source Transitions response to Safe and responsible AI in Australia discussion paper.7ecc83781ea74.pdf":{"organization_name":"Source Transitions Pty Ltd","organization_type":"For-purpose management consulting group","classification":"Proponent","overall_position":"Supports comprehensive regulation with adaptive governance approaches","arguments":["Legislation is needed to ensure AI systems are environmentally-friendly","Current regulatory approaches do not sufficiently address environmental and social costs of AI","A clear direction about AI\'s purpose for Australia is needed to achieve its potential"],"counterarguments":["Risk-based approaches alone are insufficient to address power imbalances","Legislation may struggle to keep pace with rapidly evolving AI technologies"],"key_recommendations":["Mandate that AI systems are environmentally-friendly","Regulate digital and green transitions together","Adopt a taxonomy of harms to tailor legislation to each type of risk","Provide responsible AI tools, practices and resources freely to the public","Sponsor AI literacy programmes across sectors"],"risks_and_challenges":["Environmental costs of AI development and training","Potential for AI to exacerbate power imbalances and social inequalities","Lack of user agency in AI-powered tools","AI-washing and false claims about AI capabilities"],"safeguards_and_mitigations":["Implement adaptive and hybrid governance models","Provide sandbox environments for governance and transparency processes","Enforce documentation of data sources and bias minimization strategies","Sponsor the teaching of Responsible AI and AI ethics in schools and universities"],"examples":{"Environmental cost of AI training":"For example, the environmental cost of training a large natural language processing model could be as much as the total emissions produced by five cars over the cars\' lifetime","Human cost of AI training":"Further, the human cost of training models, often akin to modern slavery","AI literacy impact":"Learning what issues were discussed and how they were resolved will support others who can relate to similar problems"},"international_alignment":"Supports global standards with national implementation","values":["Transparency","Sustainability","Inclusivity","Accountability","Ethical practices"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, provide infrastructure for responsible AI"},{"entity":"Private sector","role":"Implement responsible AI practices, collaborate on real-life projects"},{"entity":"Academia","role":"Research and teach responsible AI practices"},{"entity":"Public","role":"Engage in discussions about AI governance"}],"sector_impacts":[{"sector":"Education","impact":"Need for AI ethics and literacy programs"},{"sector":"Environment","impact":"Potential for increased resource consumption and emissions"},{"sector":"Technology","impact":"Need for transparent and accountable AI development practices"}],"quotes":["Source Transitions shares the Australian Government\'s view that legislating AI technologies development and applications is warranted given the risks they introduced into society.","A bigger story of how AI can help Australia transition to better social, economic and environmental outcomes should be framed; one with a strong intent from government to direct innovation for purpose.","Leaving AI literacy to the private sector would be to the detriment of the public, small business and not for profit who need to be and feel involved in the direction AI can take."]},"169_20230725 Submission Shine Lawyers FINAL.f72e7f4f190ae.pdf":{"organization_name":"Shine Lawyers","organization_type":"Law firm","classification":"Proponent","overall_position":"Supports new AI-specific laws and updating existing laws","arguments":["Current legal framework is inconsistent across consumer and non-consumer use","Existing laws present difficulties in establishing causation for AI-related harms","There are uncertainties in the current framework that warrant legislative reform"],"counterarguments":["Requiring software to be error-free is unrealistic and impractical","Strict liability for all AI-related harms could stifle development"],"key_recommendations":["Presumed duty of AI developers to end-users and non-contracting third parties","Rebuttable presumption of causal link between failed duty of care and AI-caused harm","Void exclusion terms in user agreements for AI-related software","Include pure mental harm and pure economic loss as compensable damages"],"risks_and_challenges":["Opacity, autonomous behaviour, and complexity of AI systems make it difficult to prove causation","Current laws may not adequately cover non-cybersecurity related AI-harms","Exclusion terms in software agreements may limit liability for AI-related harms"],"safeguards_and_mitigations":["Encourage AI developers to incorporate data logs or safety systems","Restrict liability to cases of significant harm","Allow class actions for AI-related harms"],"examples":{"Vulnerable SDK components in IoT and OT environments":"Microsoft researchers reported a vulnerable open-source component in the Boa web server.","AI predictive modelling for beach water quality":"Toronto\'s use of AI predictive modelling which had replaced existing methodology as the only determiner of beach water quality had raised concerns about its accuracy","Vulnerable software in electrical grid":"As part of its investigation into cyberattacks on India\'s electrical grid, Microsoft researchers reported a vulnerable open-source component in the Boa web server."},"international_alignment":"Considers EU\'s proposed AI regulations as a model","values":["Accountability","Transparency","Consumer protection"],"tone":"Cautionary","stakeholders":[{"entity":"AI developers","role":"Presumed duty to end-users and non-contracting third parties"},{"entity":"Courts","role":"Empowered to order disclosure of information by AI providers or users"}],"sector_impacts":[{"sector":"Software industry","impact":"Increased liability for AI-related harms"},{"sector":"Legal system","impact":"Potential increase in AI-related litigation"}],"quotes":["As software technology increase in sophistication, and into the realms of artificial intelligence (AI), those inherent risks on the real-world associated with the use of software nevertheless remain.","It is therefore recommended that all AI developers have a presumed duty to its end-users and non-contracting third parties. This duty can be rebuttable if the harm caused is too remote to have been foreseeable.","A further recommendation is that, similar to what was recently proposed in the EU, there should be a rebuttable presumption against an AI developer, of a causal link between a failed duty of care and harm caused by the AI system once a breach of a duty of care is established."]},"317_20230726.LV Submission.Safe and responsible AI in Australia (Consultation Paper).8059eb395fe05.pdf":{"organization_name":"LegalVision","organization_type":"Tech-driven commercial law firm","classification":"Proponent","overall_position":"Supports new AI-specific laws with a risk-based approach","arguments":["A risk-based regulatory approach is flexible and adaptable in its regulation of AI","AI-specific legislation would provide clarity and certainty to businesses in the AI space","Government regulation will grant the public greater protections and potential remedies against harms caused by AI use"],"counterarguments":["Voluntary or self-regulatory tools can be less effective than government regulation","Without firm regulation, there is a possibility that AI can be used for significantly negative uses"],"key_recommendations":["Implement AI-specific legislation (AI Legislation) that adopts a risk-based approach","Establish a regulatory sandbox for AI innovation","Create a central coordinating body to oversee AI governance policies and frameworks"],"risks_and_challenges":["Bias and discrimination in AI systems","Privacy and data protection concerns","Deep fakes and misinformation","National security and cybersecurity implications","Challenges with autonomous systems"],"safeguards_and_mitigations":["Implement a risk-based regulatory approach","Establish clear guidelines for AI transparency","Conduct regular impact assessments and monitoring of AI systems","Provide training and education on AI ethics"],"examples":{"Risk-based approach similar to EU and Canada":"The proposed regulatory framework explored in Tables 1 and 2 in the \'Overview\' above addresses AI risks proportionately amongst the various sectors, AI applications or organisations.","Regulatory sandbox":"Many of our clients have found regulatory sandboxes, like the financial services sandbox administered by ASIC, to be particularly useful."},"international_alignment":"Supports alignment with countries in the same WEIRD framework category","values":["Transparency","Accountability","Fairness","Innovation","Responsibility"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Developers","role":"Design, code, produce or develop AI systems"},{"entity":"Deployers","role":"Make AI systems available for users"},{"entity":"Users","role":"Use AI systems responsibly"},{"entity":"Government","role":"Develop and enforce AI regulations"}],"sector_impacts":[{"sector":"Tech sector","impact":"Potential impact on competitiveness in global AI market"},{"sector":"Education","impact":"Need for investment in AI-related education and training"},{"sector":"Employment","impact":"Potential technological unemployment and need for reskilling"}],"quotes":["We welcome any changes proposed by government that support the following objectives: Proportionately regulates AI development, deployment and use without stifling innovation, primarily using a risk-based approach","A risk-based regulatory approach for AI in Australia would involve assessing the risks associated with different AI systems and then tailoring the level of regulation to those risks. This approach would allow Australia to regulate AI in a way that is flexible and proportionate, without stifling innovation.","Government use of AI should be encouraged, provided that concerns of security, fairness, accountability and transparency are addressed. This is because AI systems can greatly enhance productivity and capability."]},"211_Government RAI Submission - Alphinity.b5b113a20f30f.pdf":{"organization_name":"Alphinity Investment Management","organization_type":"Investment management company","classification":"Proponent","overall_position":"Supports regulation for responsible AI but advocates for a balanced approach that allows innovation","arguments":["Stronger frameworks and rules will give greater confidence to the business and investor community","Regulation is needed to ensure AI is applied responsibly","Regulation should provide governance, management and reporting structures to mitigate extreme risks"],"counterarguments":["Regulation should not be overly prescriptive","There is concern that an overly prescriptive risk-based approach could stifle innovation"],"key_recommendations":["Develop an overarching guidance document or regulation map","Encourage public reporting of significant AI-related incidents and management","Integrate AI-related risks and opportunities into corporate governance guidelines","Invest in initiatives focusing on collaboration, training, and awareness raising","Encourage companies to disclose their \'AI footprint\'","Set up a public complaints process for AI-related concerns","Develop a strategy for a just transition to AI","Establish an international body for global coordination of AI regulation"],"risks_and_challenges":["Cyber security and data privacy","Trust, explainability and accountability","Misinformation","Bias","Human capital impacts","Carbon emissions","Job losses due to AI"],"safeguards_and_mitigations":["Adjusting existing regulations to include AI considerations","Encouraging public reporting of AI-related incidents","Integrating AI risks and opportunities into corporate governance guidelines","Setting up a public complaints process for AI-related concerns","Developing a strategy for a just transition to AI"],"examples":{"Modern Slavery Act reporting":"We have seen tremendous benefits coming from the Modern Slavery Act requiring companies to disclose their operational and supply chain footprint and associated risks and management strategies.","EU\'s risk-based approach":"We believe that the approach the EU has taken, which scales the regulatory response based on risk, is appropriate and seems like a practical solution that would also work well for Australia."},"international_alignment":"Supports global coordination of approaches related to regulation","values":["Innovation","Responsibility","Transparency","Accountability","Trust"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop regulation and invest in initiatives to support responsible AI practices"},{"entity":"Businesses","role":"Manage AI-related risks and invest in opportunities"},{"entity":"Universities","role":"Invest in AI programs and courses, support talent development"},{"entity":"Investors","role":"Engage with companies on AI-related risks and opportunities"}],"sector_impacts":[{"sector":"Engineering","impact":"Long-term use of AI, particularly in advanced data analytics"},{"sector":"Materials","impact":"Long-term use of AI, particularly in advanced data analytics"},{"sector":"Financial","impact":"Long-term use of AI, particularly in advanced data analytics"},{"sector":"Education","impact":"Need for AI-related programs and courses to support talent development"}],"quotes":["We support the Government\'s view that regulation around AI is needed to ensure the technology is applied responsibly, however we believe that the role of regulation is to provide governance, management and reporting structures in order to mitigate the most extreme risks and concerns, and should not be overly prescriptive.","As this is such a new and evolving space, we believe it is extremely important that regulation still leaves space for innovation and ingenuity from within industry.","We believe stronger frameworks and rules for the responsible application of AI will give greater confidence to the business and investor community and unlock significant potential for future revenue and growth."]},"49_230613 letter re AI consultation V1.e012603845a17.pdf":{"organization_name":"Biometrics Institute Limited","organization_type":"Independent and international not-for-profit membership organisation","classification":"Neutral","overall_position":"Promotes responsible and ethical use of biometrics and AI, without direct comment on specific regulatory proposals","arguments":["Insufficient regulation impedes advancement of biometric technology globally","Material risks associated with biometrics need to be addressed","Responsible use of biometrics requires informed decision-making"],"counterarguments":[],"key_recommendations":["Use Institute\'s guiding materials for considering biometric technologies","Engage with the Institute to determine key questions in biometric use cases","Utilize the Institute\'s dictionary of terms to address inconsistent language use"],"risks_and_challenges":["Issues of bias in biometric systems","Risks in making individual decisions and across systems as a whole","Impacts upon individual and group privacy","Ramifications for people\'s rights"],"safeguards_and_mitigations":["Development of Privacy Guidelines","Creation of Good Practice Framework","Providing training and thought leadership"],"examples":{"Good Practice Framework":"The Institute develops a range of guiding materials for those considering biometric technologies.","Targeted dictionary of terms":"The Institute is developing a targeted dictionary of terms used in biometrics, aiming to include the wide array of different uses to which each term is put."},"international_alignment":"Supports international dialogue and sharing of good practices","values":["Responsibility","Ethics","Effectiveness"],"tone":"Neutral","stakeholders":[{"entity":"Government agencies","role":"Users of biometric technologies"},{"entity":"Private sector","role":"Users and providers of biometric technologies"},{"entity":"Biometrics Institute","role":"Provide guidance and facilitate dialogue"}],"sector_impacts":[{"sector":"Government","impact":"Guidance on responsible use of biometric technologies"},{"sector":"Banking","impact":"Support for ethical implementation of biometrics"}],"quotes":["The Biometrics Institute was established in 2001 in Australia, supported in part by the Federal Government, with a mission to promote the responsible, ethical, and effective use of biometric technologies.","Over the past few years, surveys of stakeholders by the Institute have increasingly identified that insufficient regulation impedes advancement of biometric technology globally.","The Institute develops a range of guiding materials for those considering biometric technologies."]},"462_Submission 462 - XXX- Asia Internet Coalition - 3-Aug.d971afbe36935.pdf":{"organization_name":"Asia Internet Coalition (AIC)","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with emphasis on international harmonization","arguments":["AI can help reduce risks inherent in everyday life when developed and used responsibly","International alignment reduces discrepancies and conflicts between different legal frameworks","Risk-based approach should be targeted at the right use cases, considering likelihood and severity of harm"],"counterarguments":["AI is not risk-free and can potentially increase societal problems if not created and implemented properly","Some issues will require legislation, such as mandating high-risk AI systems to undergo expert risk assessments"],"key_recommendations":["Adopt proportional privacy laws that protect personal information and enable trusted data flows across national borders","Establish competition safe harbours for open public-private and cross-industry collaboration on AI safety research","Require organizations deploying high-risk AI systems to undergo risk assessments and provide documentation"],"risks_and_challenges":["Malicious use of AI by bad actors","Potential for AI systems to increase societal problems if not created and implemented properly","Disinformation efforts using generative AI (deepfakes/cheapfakes)"],"safeguards_and_mitigations":["Develop next-generation trade control regulations for certain AI-powered software programs","Strengthen international collaborations and public-private forums for information exchange on AI security vulnerabilities","Investigate techniques for spotting and thwarting disinformation efforts"],"examples":{"AI in cybersecurity":"Through sophisticated security operations and threat intelligence, it can also power a new generation of cyber defences.","AI in economic growth":"Economies that embrace AI will see significant growth, outcompeting rivals that are slower on the uptake.","AI in addressing global challenges":"AI is already assisting in the fight against global issues like disease and climate change"},"international_alignment":"Strongly supports international harmonization and interoperability in defining and regulating AI","values":["Transparency","Accountability","Responsibility"],"tone":"Optimistic","stakeholders":[{"entity":"Governments","role":"Develop regulations, invest in research, and streamline AI adoption"},{"entity":"Industry","role":"Develop and deploy AI responsibly, collaborate on safety research"},{"entity":"Academia","role":"Conduct fundamental AI research"}],"sector_impacts":[{"sector":"Economy","impact":"Significant growth and increased productivity"},{"sector":"Workforce","impact":"Job transitions and need for upskilling"}],"quotes":["AI is not risk-free, but when developed and used responsibly it can help reduce a vast array of risks inherent in everyday life.","Transparency requirements should be tailored to ensure that the information is actionable and presented when stakeholders want it, in terms they can understand, and without extraneous details that can be distracting or confusing.","To maintain public confidence, accountability mechanisms that demonstrate responsible AI development and deployment are essential."]},"138_Standards Australia submission - Supporting Responsible AI 2023.f30c0c7563888.pdf":{"organization_name":"Standards Australia","organization_type":"Peak not for profit standards body","classification":"Proponent","overall_position":"Supports a standards-based approach to AI regulation","arguments":["Standards can provide a common base for AI governance across government jurisdictions and sectors","International standards can reduce barriers to trade and ensure regulatory compatibility","Standards-based approach can build public trust and increase adoption of AI technologies"],"counterarguments":["null"],"key_recommendations":["Adopt specific standards to help the development and deployment of responsible AI","Create educational guidance materials to improve public sentiment on AI trustworthiness","Leverage the forthcoming ISO/IEC Artificial intelligence - Management system standard for regulatory requirements","Scope opportunities to become standard-setting in new fields of responsible AI","Upskill and support the ASEAN and Pacific region with AI standardisation capacity building","Support the development of industry-based standards through multilateral systems"],"risks_and_challenges":["Public skepticism and lack of trust in AI technologies","Foreign states or corporations abusing digital products with impacts in Australia","Cross-border nature of critical and emerging technologies"],"safeguards_and_mitigations":["Adoption of international standards for AI governance","Development of educational materials to build public trust","Capacity building in the Indo-Pacific region to strengthen security and build resilience"],"examples":{"Involvement in drafting new AI management system standard":"SA is closely involved in the drafting of the new Information technology - Artificial intelligence - Management system which is currently in the final balloting process and due for publishing Q4 2023.","Collaboration with DFAT on regional capacity building":"SA, through collaborative work with DFAT, has been working for a number of years on capability building within the region on critical and emerging technology (CET) standardisation to help support our neighbours through training and upskilling."},"international_alignment":"Strongly supports global standards and international alignment","values":["Consensus","Interoperability","Trustworthiness"],"tone":"Positive and proactive","stakeholders":[{"entity":"Australian Government","role":"Work with Standards Australia to adopt and implement AI standards"},{"entity":"Industry","role":"Participate in standards development and implementation"},{"entity":"Standards Australia","role":"Facilitate standards development and international collaboration"}],"sector_impacts":[{"sector":"Environmental protection","impact":"Improvements through AI capabilities"},{"sector":"Energy","impact":"Increased efficiencies"},{"sector":"Defence and security","impact":"Enhanced capabilities"},{"sector":"Healthcare","impact":"Improved health care services"},{"sector":"Education","impact":"Advancements in educational services"}],"quotes":["Standards, both nationally and globally, are built through a harmonised approach driving consensus amongst industry, government, and community interests. In turn this process leads to a higher chance of market acceptance and implementation.","International standards are uniquely placed to provide a common base for AI governance across government jurisdiction and sector, applying the same principle of driving consensus on a global scale.","SA\'s unique offering facilitates the creation of standards which can under pin future policy and regulation."]},"6_XAI_Environmental_Management.c4949b1834356.pdf":{"organization_name":"Department of Civil Engineering, Monash University","organization_type":"Academic institution","classification":"Neutral","overall_position":"Advocates for explainable AI in environmental management research","arguments":["Explainable AI is crucial for responsible AI implementation in cross-disciplinary fields","A holistic approach focusing on input, AI model, and output is needed for AI explainability","Current AI networks are often too complex for application fields outside computer science"],"counterarguments":["null"],"key_recommendations":["Implement systematic input data augmentation to maximize AI network generalizability","Monitor AI network layers and parameters to use leaner networks suitable for edge devices","Focus on interpretability and robustness of AI network outputs"],"risks_and_challenges":["Overfitting in AI networks","High computation costs of complex AI networks","Lack of explainability in AI network decisions"],"safeguards_and_mitigations":["Input data augmentation to prevent overfitting","Monitoring kernel weight histograms to identify redundant network layers","Generating counterfactual image examples to test network robustness"],"examples":{"Waste classification problem":"The study uses 2100 images of solid waste to demonstrate the explainability framework.","Smart trash bins":"Explainable networks can facilitate the robotization of waste sorting in material recovery facilities.","Environmental protection regulations":"XAI can be used to enforce environmental protection regulations and detect unlawful activities related to waste disposal and handling."},"international_alignment":"null","values":["Explainability","Generalizability","Robustness"],"tone":"Neutral","stakeholders":[{"entity":"Environmental management researchers","role":"Implement and understand explainable AI in their field"},{"entity":"AI developers","role":"Develop more explainable and efficient AI networks"}],"sector_impacts":[{"sector":"Environmental management","impact":"Improved waste classification and sorting using explainable AI"},{"sector":"Waste management","impact":"Enhanced automation and efficiency in waste sorting facilities"}],"quotes":["The implementation of AI networks in environmental management research is fast growing. However, the tension between performance and explainability remains, even for state-of-the-art networks.","This study proposes an explainability framework with a triadic structure focusing on input, AI model and output.","Explainable networks can facilitate the robotization of waste sorting in material recovery facilities."]},"246_260723_QCCL - AI Safety Submission.ac05dfe8714b4.pdf":{"organization_name":"Queensland Council for Civil Liberties","organization_type":"Not-for-profit organization","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on human rights","arguments":["AI regulation should prioritize individual human rights","Australia lacks legislated human rights protections","A human rights framework should be developed alongside AI governance"],"counterarguments":[],"key_recommendations":["Develop a human rights framework prior to or alongside AI governance","Implement an authorisation and licensing process for AI development and application","Focus on human rights implications in AI regulation","Apply consistent regulation to both state and private AI use","Adopt enforceable legislation rather than principle-based regulation"],"risks_and_challenges":["Potential serious impact on individual freedoms","Lack of systematic and structured governance in Australian organizations","Concerns highlighted in the Robodebt Scheme Royal Commission"],"safeguards_and_mitigations":["Human rights-focused authorisation and licensing process","Prohibition of AI systems unless they comply with human rights-centric licensing","Implementation of mechanisms and safeguards appropriate to context and state of art"],"examples":{"EU AI Act":"The approach being adopted in the EU AI Act offers sensible guidance","Robodebt Scheme":"Concerns highlighted in the Report of the Royal Commission into the Robodebt Scheme"},"international_alignment":"Supports alignment with EU AI Act and OECD AI Principles","values":["Human rights","Individual freedoms","Democracy"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations"},{"entity":"Private sector","role":"Comply with human rights-focused AI regulation"}],"sector_impacts":[],"quotes":["We consider that discussion regarding artificial intelligence (in any form) should be focused upon the best interest of individual human rights.","Australia should require any development or application of artificial intelligence to an authorisation and licensing process that primarily focuses on the human rights implication(s)","A principle-based regulatory framework for regulating artificial intelligence is unlikely to achieve the level of protection of Australians that would be achieved if the regulation of artificial intelligence and automated decision making occurs by legislative force."]},"369_KPMG submission_Safe and responsible AI in Australia_Final.52329513fc768.pdf":{"organization_name":"KPMG Australia","organization_type":"Professional services firm","classification":"Proponent","overall_position":"Supports a balanced approach with new AI-specific regulations and updating existing laws","arguments":["Appropriate legal and regulatory frameworks are critical to providing increased certainty about AI risks and benefits","Existing laws are generally not yet adequately adapted to AI and ADM technologies","A people-centred approach to AI is needed that prioritizes human rights impacts"],"counterarguments":["Overly complex regulation risks limiting uptake of AI technology and driving out innovation","Self-regulation alone may not be sufficient to address all necessary measures"],"key_recommendations":["Develop an initial human rights risk assessment for AI projects","Consider introducing transparent disclosure obligations for AI ethics compliance","Explore development of a certification regime for responsible AI"],"risks_and_challenges":["Lack of public trust in AI due to insufficient regulations and safeguards","Potential for AI bias and discrimination against vulnerable populations","Privacy and data protection concerns with AI systems"],"safeguards_and_mitigations":["Implement assurance mechanisms like conformity assessments for high-risk AI applications","Strengthen individual rights related to automated decision-making","Embed data quality requirements and privacy-by-design principles"],"examples":{"Finland\'s public AI education course":"The Elements of AI course is a free online course created by the University of Helsinki and MinnaLearn and has been completed by over 850,000 people.","EU AI Act\'s risk-based approach":"Given the mature stage of development of the EU\'s AI Act, Australia could consider the risk-based approach with stricter regulation of AI and ADM applications in high-risk areas, to inform its regulation."},"international_alignment":"Supports greater consistency with international regulatory frameworks","values":["Human rights","Transparency","Accountability"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, conduct public education campaigns"},{"entity":"Private sector","role":"Implement responsible AI practices, conduct risk assessments"},{"entity":"Regulators","role":"Provide oversight and enforcement of AI regulations"}],"sector_impacts":[{"sector":"Public sector","impact":"Need for ethical data sharing practices and human rights impact assessments"},{"sector":"Private sector","impact":"Potential administrative burden from new regulations, need to implement AI governance frameworks"}],"quotes":["KPMG supports a regulatory approach that is simple and clear in order to achieve the right balance between appropriate safeguards and enabling innovation.","KPMG considers that both the public and private sector\'s use of AI technologies must be held to the same minimum standards, including in relation to privacy protection, transparency and explainability, contestability, and discrimination.","Greater consistency with international regulatory frameworks would significantly reduce administrative burden, help with exporting technology out of Australia and set clearer expectations for the importation of technology."]},"286_Deloitte submission on safe and responsible AI_26 July 2023.c789d04dcbb52.pdf":{"organization_name":"Deloitte","organization_type":"Professional services firm","classification":"Proponent","overall_position":"Supports risk-based framework for AI regulation that is flexible and proportionate","arguments":["Risk-based regulation allows prioritization of resources on higher risk applications","Horizontal application of AI regulation can help establish a cohesive and nationally consistent regulatory framework","Compatibility with global approaches reduces regulatory complexities for organizations operating in multiple countries"],"counterarguments":[],"key_recommendations":["Adopt a risk-based framework for AI regulation","Develop a definition of AI that supports the objectives of the overarching regulatory framework","Develop an AI code of practice to provide practical guidance on safe and responsible AI"],"risks_and_challenges":["Privacy concerns and protection of intellectual property","Threats to national security and critical infrastructure","Undermining the human experience of fulfilling work and social connection"],"safeguards_and_mitigations":["Establish an oversight body to implement and administer the regulatory framework","Develop certification or accreditation programs to verify compliance","Provide an avenue for consumer complaints and users to seek review of AI decisions"],"examples":{"Chatbot vs medical advice":"using a generative AI chatbot to produce a summary of a long form magazine article presents very different risks to using the same technology to provide medical advice","Family budget vs Robodebt":"the significant difference in the risk posed by a set of coded logic statements applied to calculate a family budget as compared to the use of coded logic to deliver the Robodebt scheme"},"international_alignment":"Supports global compatibility and cross-border collaboration","values":["Innovation","Consistency","Proportionality","Flexibility","Accountability"],"tone":"Positive and constructive","stakeholders":[{"entity":"Oversight body","role":"Implement and administer regulatory framework"},{"entity":"AI developers and users","role":"Comply with regulatory requirements based on risk class of their systems"}],"sector_impacts":[{"sector":"Cross-sector","impact":"Consistent regulatory expectations and obligations for AI actors across sectors"}],"quotes":["Deloitte supports a risk-based regulatory framework that is focussed on risk posed by AI to individuals, society and the environment rather than targeting specific technologies.","Artificial Intelligence refers to a computer-based system that can, for a given set of objectives, generate outputs such as content, predictions, recommendations, or decisions influencing real or virtual environments. AI systems are designed to operate with varying levels of autonomy.","The higher the risk, the stricter the rule. Risk-based AI regulation recognises that rules and compliance procedures for AI applications that do not present genuine risks can result in higher costs and burdens, without providing real benefits, therefore regulatory obligation should be proportional to risk."]},"401_Submission to Safe and Responsible AI in Australia by Bradley Holland Submission 4Aug2023.cc2ef15a4c7c9.pdf":{"organization_name":null,"organization_type":"Individual submission","classification":"Proponent","overall_position":"Supports strengthening AI governance with a focus on transparency, accountability, and risk management","arguments":["AI can greatly benefit society and increase economic output","Existing laws may be deficient in providing effective remedies for AI-related issues","Transparency is crucial for building trust in AI systems"],"counterarguments":["Associating decision-making with AI could demonize algorithms and displace human accountability","The proposed definition of AI is too broad and could capture non-intelligent systems"],"key_recommendations":["Establish a National AI Centre of Excellence (NAICE)","Implement a complaints, escalation, and ombudsman process for AI-related issues","Introduce an AI-Officer role in organizations using AI systems","Develop education programs on AI advantages and disadvantages"],"risks_and_challenges":["Misapplication of AI responses leading to legal issues","Lack of transparency in AI decision-making processes","Potential for errors in high-risk AI applications"],"safeguards_and_mitigations":["Implement test, evaluation, verification and validation (TEVV) processes for AI systems","Require human oversight for high-risk AI applications","Establish clear guidelines for AI transparency and accountability"],"examples":{"Trivago case":"The Discussion Paper highlights an example of a significant risk to consumers and users of AI systems, with respect to the situation that occurred in the Trivago case.","Robodebt initiative":"The likelihood of legal error increases exponentially when the decision-making process involves more steps, or more complicated enquiries. The potential for error \u2014 and the cost and inconvenience that results \u2014 has been amply demonstrated by the Robodebt initiative.","ChatGPT misuse in legal filings":"In the situation where the application of AI-systems results in not meeting the standards acceptable to society, appropriate and proportionate remedial action should take place. For example, in the erroneous use of ChatGPT to generate affidavits and case filings which resulted in the creation of non-existent judicial opinions with false quotes and citations, the court ordered the perpetrator to officially inform the judges of the misattributed and fake opinions."},"international_alignment":"Supports consideration of international approaches, particularly EU and US frameworks","values":["Transparency","Accountability","Trust"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government agencies","role":"Implement and oversee AI governance frameworks"},{"entity":"Businesses","role":"Implement responsible AI practices and handle AI-related complaints"},{"entity":"Australian Law Reform Commission","role":"Review Australian and International Laws for AI readiness"}],"sector_impacts":[{"sector":"Education","impact":"Potential for AI impact assessments in implementation"},{"sector":"Health and Aged-Care","impact":"Potential for AI impact assessments in implementation"}],"quotes":["The Australian Government is to be supported in its approach to strengthen the governance in the use of Artificial Intelligence (AI) by industry and government departments.","To establish trust in the Australian Government\'s use of an AI-based system, accuracy is a key criterion and therefore, government agencies will need to be completely transparent with respect to items contemplated in (i) to (iv).","This submission is of the view that the Australia Government should adopt a \'wait and see\' approach before imposing regulatory compliance requirements on private organisations on the adoption and use of AI-systems."]},"492_Submission 492 - OpenAI - 14-Aug - Copy.8e6c605d90098.pdf":{"organization_name":"OpenAI","organization_type":"AI development company","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with international coordination","arguments":["AI presents significant economic and social opportunities","Coordination is critical for effective governance of highly capable foundation models","Risk-based approach allows for both horizontal and vertical regulatory elements"],"counterarguments":["AI brings considerable challenges, including risks of misuse, inaccuracies, and potentially dangerous capabilities","Intentional misuse or errors of powerful AI systems could result in significant harmful impacts"],"key_recommendations":["Develop registration and licensing requirements for highly capable foundation models","Harmonize emergent accountability expectations for AI within and across countries","Implement transparency measures across the AI life cycle","Conduct rigorous qualitative and quantitative model evaluations"],"risks_and_challenges":["Misuse of AI systems","Inaccuracies in AI outputs","Potentially dangerous capabilities of highly capable foundation models","Cross-border impacts of AI systems"],"safeguards_and_mitigations":["Voluntary commitments to safety, security, and trust","Formation of industry bodies like the Frontier Model Forum","Pre-deployment testing","Content provenance measures","Trust and safety practices"],"examples":{"System Cards":"OpenAI has published two system cards: the GPT-4 System Card and DALL-E 2 System Card","Red teaming":"OpenAI conducts red-teaming internally and with independent parties to test models and systems","Automated evaluations":"OpenAI conducts automated, quantitative evaluations of model capabilities and risks"},"international_alignment":"Strongly supports international coordination and harmonization of AI governance","values":["Safety","Accountability","Transparency","Innovation"],"tone":"Positive and cooperative","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulations"},{"entity":"AI developers","role":"Act responsibly and take a safety-focused approach"},{"entity":"Industry bodies","role":"Promote best practices and standards"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential benefits through AI applications"},{"sector":"Engineering","impact":"Potential benefits through AI applications"},{"sector":"Legal services","impact":"Potential benefits through AI applications"}],"quotes":["We strongly support a risk-based approach to AI regulation.","Rigorously measuring advances in potentially dangerous capabilities is essential for effectively assessing and managing risk.","We welcome and encourage regulation of AI."]},"376_CCAT Safe and Responsible AI submission.f8fdad930519c.pdf":{"organization_name":"Centre for Culture and Technology (CCAT)","organization_type":"Research Centre","classification":"Proponent","overall_position":"Supports comprehensive regulation with a focus on inclusive design and public education","arguments":["AI literacy and digital literacy are crucial for building public trust and mitigating risks","A risk-based approach should be combined with measurable targets for increasing public literacy in AI","Transparent communication about AI use is essential for public trust"],"counterarguments":["null"],"key_recommendations":["Implement a four-level risk approach (low, medium, high, and very high)","Focus on AI literacy and digital literacy through public education initiatives","Mandate transparent communication about AI use, especially in news media","Adopt an inclusive design approach for AI communication strategies"],"risks_and_challenges":["Confusion and uncertainty about AI capabilities","Potential for AI to exclude, marginalize, and discriminate against certain groups","Privacy and security risks in healthcare when using AI tools like ChatGPT","Spread of misinformation through AI-generated news content"],"safeguards_and_mitigations":["Inclusive design approach to communication strategies","Mandating transparency in AI use","AI literacy training for news media outlets","Clear labeling of AI-generated content in news media"],"examples":{"Use of ChatGPT by Australian doctors":"We refer to the recent instance of Australian doctors and hospitals using ChatGPT to write medical notes.","AI-generated news articles":"Recent revelations that News Corp Australia have been publishing as many as 3,000 AI-generated articles a week have highlighted the extent of this practice","COVID-19 health communications":"Our research on health communications during the COVID-19 pandemic for example found that when communication strategies were designed for people with disability, the entire population benefited and were better able to understand and trust important government messaging."},"international_alignment":"Supports alignment with international approaches and definitions","values":["Inclusivity","Transparency","Responsibility"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Implement and enforce AI regulations, provide public education"},{"entity":"News media organizations","role":"Clearly state when AI has been used to generate content"},{"entity":"Public","role":"Engage in AI literacy and digital literacy education"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential privacy and security risks when using AI tools"},{"sector":"Education","impact":"AI has potential benefits but also challenges in implementation"},{"sector":"News media","impact":"Increasing use of AI-generated content raises concerns about misinformation and public trust"}],"quotes":["We are motivated by the proposition that the study of culture, with its emphasis on identity, meanings, relationships, power and values, needs to be better integrated with the study of media and digital technologies, including Artificial Intelligence (AI).","Currently, there is a great deal of uncertainty and confusion circulating about what AI technologies are realistically capable of.","To build public trust in AI it is important that the public clearly understands what AI is and how it is being used."]},"501_Submission 501 - Department of Health and Aged Care - 22-Aug.0dfeb1d9f0267.pdf":{"organization_name":"Department of Health and Aged Care","organization_type":"Government department","classification":"Proponent","overall_position":"Supports a risk-based approach with a mix of regulatory and non-regulatory frameworks for responsible AI in healthcare","arguments":["AI has the potential to transform wide areas of the economy and improve lives, particularly in healthcare","A risk-based approach provides flexibility to ensure regulatory burden aligns with potential risk","Sector-specific governance of AI in healthcare is essential given unique risks, challenges, and opportunities"],"counterarguments":["AI technologies pose unique risks that require specific safeguards","Existing regulatory frameworks and legislation are not sufficiently developed for full utilisation of AI"],"key_recommendations":["Develop a national AI ethical and governance framework for healthcare","Implement mandatory AI impact assessments for high-risk applications","Establish clear guidelines for AI transparency in healthcare"],"risks_and_challenges":["Potential for AI bias and discrimination in healthcare applications","Privacy concerns related to sensitive health data","Erosion of trust in healthcare systems if AI is not implemented responsibly"],"safeguards_and_mitigations":["Mandatory training for healthcare professionals on AI use","Regular reviews and audits of AI algorithms in healthcare","Implementation of Data Impact Assessments for AI applications"],"examples":{"TGA regulation of AI in medical devices":"The TGA have been regulating products that are intended for medical use including software (that incorporate AI) since 2002, using a robust regulatory framework for software based medical devices.","AI in clinical decision support":"Software that displays information about a patient and other medical information (such as peer-reviewed clinical studies and clinical-practice guidelines). The software takes this information and presents a diagnosis and relevant treatment recommendation, along with the rationale for this, to a health professional for the purposes of assisting them in determining a diagnosis or treatment for their patient."},"international_alignment":"Supports alignment with international standards while developing national approaches","values":["Patient safety","Transparency","Equity","Privacy","Trust"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government agencies","role":"Develop and enforce AI regulations in healthcare"},{"entity":"Healthcare professionals","role":"Implement and use AI responsibly in clinical practice"},{"entity":"AI developers","role":"Ensure AI products meet regulatory requirements and ethical standards"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential for improved diagnosis, treatment, and resource allocation"},{"sector":"Aged care","impact":"Enhanced monitoring and personalized care for elderly patients"}],"quotes":["The Department supports the development and implementation of policies and governance that promote safe and responsible AI in Australia.","The Department strongly advocates for a risk-based approach in relation to AI and recognises that it may need to be mandatory for moderate to high-risk applications in health and aged care.","Transparency in the delivery of health care is essential and this should be consistent across public and private sectors."]},"446_Anderson_Marshall -  Submission re Responsible AI - August 2023.78d4f9cefc2af.pdf":{"organization_name":"Connecting Stones Consulting and Hocone Pty Limited","organization_type":"Consulting firms","classification":"Proponent","overall_position":"Supports comprehensive regulation and governance frameworks for responsible AI","arguments":["Existing regulatory approaches may not cover all potential risks from AI","Public trust and confidence in AI development and use require ongoing engagement with the community","AI developments have shown that many dynamics and impacts will only come to light through use"],"counterarguments":["null"],"key_recommendations":["Change wording of Australian Privacy law APPs and Guidance to explicitly address AI-generated Personal Information","Create an AI ecosystem building on Australia\'s strengths in governance and civil society advocacy","Implement participatory mechanisms to partner with the community and ensure ongoing response to concerns"],"risks_and_challenges":["Erosion of citizens\' rights and privacy due to AI-generated insights","Information imbalance between technology-rich organizations and the public","Potential for misuse of AI tools by bad actors"],"safeguards_and_mitigations":["Robust review procedures that can dynamically inform regulatory and governance frameworks","Mandatory transparency requirements across private and public sectors","Complaints and feedback mechanisms with visible accountability and action"],"examples":{"Insurance company scenario":"An insurance company can use information available about its customers to learn via AI of an as-yet undiagnosed pre-existing condition.","NSW AI Assurance Framework":"The NSW AI Assurance Framework provides a model that could be applied to a national context."},"international_alignment":"null","values":["Transparency","Trust","Accountability","Inclusiveness","Privacy"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and implement regulatory and governance frameworks"},{"entity":"Communities","role":"Participate in ongoing engagement and feedback processes"},{"entity":"Organizations using AI","role":"Ensure responsible AI practices and transparency"}],"sector_impacts":[{"sector":"Insurance","impact":"Potential use of AI-generated insights about customers\' health conditions"}],"quotes":["The dynamics of AI innovations must be managed with full appreciation of the particulars of Australia\'s socio-political context.","Trust is a local and contextual judgement that is shaped by personal experiences (both positive and destructive), and yet, trust-building requires a collective effort as no one sector or advocate can be expected to carry responsibility.","At the core of any ethical approach is sound decision making, good housekeeping and open books (showing your work)."]},"471_Submission 471 - Digital Media Research Centre QUT - 7-Aug.60339f4fe2c1d.pdf":{"organization_name":"School of Law/Digital Media Research Centre, Queensland University of Technology","organization_type":"Academic institution","classification":"Proponent","overall_position":"Supports updating existing laws and introducing new AI-specific regulations","arguments":["Appropriate governance mechanisms are critical to ensure responsible AI development","Australia\'s existing regulatory frameworks need to evolve to address AI challenges","Different approaches should apply to public and private sector use of AI technologies"],"counterarguments":[null],"key_recommendations":["Adopt different approaches to AI regulation in public and private sectors","Amend legislation to safeguard judicial review for automated decisions","Introduce statutory requirement for human oversight in high-stakes public sector automated decision-making","Consider approaches to harmonising data quality standards across government levels","Reform freedom of information legislation and government procurement practices"],"risks_and_challenges":["Uncertainty about judicial review of automated government decisions","Lack of human oversight in automated decision-making systems","Data fragmentation and quality issues across government agencies","AI opacity barriers in the public sector"],"safeguards_and_mitigations":["Amend definition of \'decision\' in ADJR Acts to include automated decisions","Mandate human involvement for certain types of automated administrative processes","Harmonise data quality standards across government","Implement transparency by design methodologies","Reform freedom of information legislation"],"examples":{"Centrelink Online Compliance system (robodebt)":"Manual human reviews of the automated debt notices under the Centrelink Online Compliance system (commonly known as \'robodebt\'), for example, would likely have ameliorated many of the problems that arose during its operation.","GDPR Article 22":"Article 22, in particular, prohibits solely automated decision-making that affects individual rights and interests by requiring \'meaningful\' human involvement."},"international_alignment":"Supports alignment with international standards, particularly EU frameworks","values":["Transparency","Accountability","Human oversight","Data quality","Open government"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Government","role":"Implement regulatory reforms and support responsible AI practices"},{"entity":"Office of the National Data Commissioner","role":"Provide advice on inter-agency data quality standards"}],"sector_impacts":[{"sector":"Public sector","impact":"Higher standards of transparency and accountability required for AI use"},{"sector":"Private sector","impact":"Different regulatory approach compared to public sector"}],"quotes":["Appropriate governance mechanisms are critical to ensure the responsible development of AI and to mitigate the accompanying risks.","We believe that fundamental differences between the public and private sector necessitate different regulatory approaches to the use of AI.","AI complexity is a significant impediment to meaningful transparency."]},"359_CIS- CAIDE - Supporting Responsible AI_ 26 July_Submission Final.4127445966a7c.pdf":{"organization_name":"School of Computing and Information Systems and Centre for Artificial Intelligence and Digital Ethics, The University of Melbourne","organization_type":"Academic institution","classification":"Proponent","overall_position":"Supports a mix of regulatory interventions to promote safe and responsible AI","arguments":["AI offers considerable opportunities across Australian society","Overly complex or poorly designed regulation is unlikely to achieve key goals","Principles-based regulation allows greater responsiveness to future change"],"counterarguments":["Overstated concerns about the risks of AI obscure its real and present risks","Undue emphasis on innovation overlooks the harms that AI enabled technologies may perpetuate"],"key_recommendations":["Adopt a principles-based approach to regulation","Implement a mix of regulatory interventions","Provide funding for regulatory oversight and in-house AI expertise within regulators","Establish an AI Advisory Group","Mandate risk assessments and proportionate responses for AI systems"],"risks_and_challenges":["Technical risks: inaccuracy, information leakage, adversarial manipulations","Human rights risks: lack of equity and access, bias and discrimination, erosion of privacy","Societal risks: misinformation and deepfakes disrupting democratic processes","Existential risks: concerns about human-machine interactions"],"safeguards_and_mitigations":["Implement transparency and explainability requirements","Mandate risk assessments and accountability measures","Develop technical standards and best practices","Provide education and training to demystify AI"],"examples":{"ACCC success in enforcing Australian Consumer Law for technology-driven services":"We draw attention to the significant success already shown by the ACCC in enforcing the Australian Consumer Law in applying to technology driven services.","US Federal Trade Commission\'s Office of Technology Research and Investigation":"One small-scale model is the Office of Technology Research and Investigation (OTECH) at the U.S. Federal Trade Commission (FTC), created to \'level[] the playing field and empower[] the FTC to better tackle abuses from technology companies.\'"},"international_alignment":"Supports alignment with international initiatives while maintaining national standards","values":["Transparency","Accountability","Fairness","Privacy","Human rights"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Develop and enforce regulations, provide funding for oversight"},{"entity":"Regulators","role":"Develop in-house AI expertise, enforce regulations"},{"entity":"Businesses","role":"Implement responsible AI practices, conduct risk assessments"}],"sector_impacts":[{"sector":"Public sector","impact":"Need to model best practice ethical AI governance and risk assessment"},{"sector":"Private sector","impact":"Need to implement risk assessment and accountability measures"}],"quotes":["We consider that there is no one approach that will ensure safe and responsible AI. Rather, what is required is a mix, or \'network\', of regulatory interventions that together promote these desired objectives.","Overly complex or poorly designed regulation is unlikely to achieve its key goals of promoting effective, fair and safe AI, and will merely increase the costs of compliance for industry, with little real gain for individuals and society.","We consider regulation generally, and a risk-based approach specifically, should focus on the outputs or uses of products that involve these applications (general API targeted at businesses? Public-facing chat-bot/ image generation app?) and rely on existing legal frameworks surrounding business category while demanding current disciplinary best practices for responsible development, deployment etc across the AI lifecycle and supply chain."]},"308_ADLEG Submission-Safe and Responsible AI in Australia.d54e22c22e47c.pdf":{"organization_name":"Australian Discrimination Law Experts Group","organization_type":"Legal experts group","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI with a focus on preventing discrimination and protecting human rights","arguments":["Current discrimination laws may not adequately protect against algorithmic discrimination","A risk-based approach to AI regulation is necessary but must be mandated through legislation","AI governance measures should be proactive and preventative"],"counterarguments":["Voluntary or self-regulation approaches are insufficient to address serious risks of harm from AI and ADM systems","A risk-based approach alone may not provide compensation for injured people or civil recourse","Risks that cannot be readily quantified, such as risks to privacy and other human rights, may be devalued or ignored in a risk-based approach"],"key_recommendations":["Resource the AHRC and state/territory equality agencies to review current discrimination law framework\'s protection against AI discrimination","Strengthen federal discrimination laws by enacting a positive duty in all federal discrimination laws","Ban AI applications using facial analysis techniques","Mandate all elements of a risk-based regulatory approach through legislation","Establish an expert and well-resourced regulator to oversee AI and ADM systems development and deployment"],"risks_and_challenges":["Perpetuation and amplification of existing societal and historical discrimination","Disproportionate adverse effects on marginalized and disadvantaged groups","Lack of protection against all cases of algorithmic discrimination under current laws","Information, knowledge and resource asymmetries in cases involving algorithmic discrimination"],"safeguards_and_mitigations":["Mandated AI audits for \'medium\' and \'high\' risk AI applications","Requirement for in-house subject matter expertise for deployers of \'medium\' and \'high\' risk AI applications","Strengthening legal frameworks for protecting and enforcing fundamental human rights","Better resourcing the legal assistance sector to provide legal advice and representation to people who have suffered AI harms"],"examples":{"Robodebt litigation":"It was only when this litigation was funded and commenced by Victoria Legal Aid, after sustained advocacy by community legal centres, that this harmful and unlawful scheme ended.","AI-enabled applications in hiring and employee evaluation processes":"For example, for \'AI-enabled applications in hiring and employee evaluation processes\' such a requirement would ensure that job seekers are provided with an explanation of the criteria on which they will be assessed, how those criteria are weighted, and the manner of assessment."},"international_alignment":"Supports consideration of international approaches while emphasizing the need for Australian-specific regulation","values":["Human rights protection","Non-discrimination","Transparency","Accountability","Contestability"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Human Rights Commission","role":"Review and provide guidance on discrimination law application to AI"},{"entity":"Government","role":"Enact legislation and establish regulatory bodies"},{"entity":"Legal assistance sector","role":"Provide legal advice and representation to those affected by AI harms"},{"entity":"AI developers and deployers","role":"Implement AI governance measures and ensure compliance with regulations"}],"sector_impacts":[{"sector":"Employment","impact":"Potential discrimination in hiring and employee evaluation processes"},{"sector":"Public services","impact":"Risk of perpetuating and amplifying existing societal discrimination"}],"quotes":["Not all discrimination is unlawful. Individuals will only be protected from a discriminatory decision or treatment of an AI or ADM system where that decision or treatment is prohibited by Australia\'s discrimination laws.","ADLEG recommends that AI applications which use facial analysis techniques fall into the category of those with an \'unacceptable risk\' and should be banned completely.","Individuals must also have a legislated right to obtain meaningful information about the AI system when they are subjected to automated decision-making."]},"172_OCO comments - Safe and responsible AI in Australia - Discussion paper.7082f367387f6.pdf":{"organization_name":"Office of the Commonwealth Ombudsman","organization_type":"Government oversight agency","classification":"Neutral","overall_position":"Supports responsible AI practices and appropriate governance mechanisms","arguments":["Ensuring the rights of individuals, particularly those most vulnerable, are protected","The need for appropriate governance mechanisms to support safe and responsible use of AI","The importance of lawful, transparent, and fair operation of AI and ADM systems"],"counterarguments":[],"key_recommendations":["Consider intersections with other government activities regarding administrative decision-making and administrative law reform","Provide further public information about potential harm from AI and mitigation strategies","Ensure risk management approach identifies relevant criteria, reflects impacts for vulnerable people, and refers to AI Ethics Principles"],"risks_and_challenges":["AI can fundamentally change the nature of administrative decision making","Potential harm from the speed and scale at which AI can be deployed","Possible disadvantages to vulnerable people"],"safeguards_and_mitigations":["Clear information about decision-making processes","Direct pathways for people to make complaints or raise concerns","Appropriate governance mechanisms"],"examples":{},"international_alignment":"null","values":["Integrity","Fairness","Transparency","Accountability","Responsiveness"],"tone":"Cautionary","stakeholders":[{"entity":"Department of Industry, Science and Resources","role":"Develop regulatory and governance responses for AI"},{"entity":"Administrative Review Council (ARC) or similar body","role":"Provide expertise in administrative law related to AI in government administration"}],"sector_impacts":[{"sector":"Government administration","impact":"Changes in administrative decision-making processes"}],"quotes":["We acknowledge the importance of ensuring Australia has appropriate governance mechanisms to support the safe and responsible use of Artificial Intelligence (AI) including ensuring the rights of individuals, particularly those most vulnerable, are protected.","In our view current and future governance mechanisms should be designed to ensure the use of AI and ADM is lawful, transparent and operates fairly so as not to disadvantage people, particularly those most vulnerable.","Providing clear information about decision-making processes and ensuring direct pathways for people to make complaints or raise concerns is a key principle of effective government administration."]},"258_Supporting responsible AI - discussion paper.931078f3c5141.pdf":{"organization_name":"Australian Institute of Architects","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports appropriate regulatory environment to mitigate potential downsides of AI","arguments":["AI has the potential to drastically alter the world we live in","Appropriate regulatory environment is needed to mitigate potential downsides","AI could lead to loss of skills and knowledge in future generations of architects"],"counterarguments":["AI can speed up manual-intensive work","AI can reduce time and costs for developing simple renders","AI can assist in identifying potential code or design problems early in the process"],"key_recommendations":["Certification of AI platforms for Architectural purposes","Informing clients, consumers and the public when AI is used in a building design","Sign off certain buildings by a Design Review Panel","Legislation to identify liability when AI is used","Government-funded graduate positions in practices"],"risks_and_challenges":["Liability when AI gets it wrong","Non-compliance with Australian design rules","Loss of skills and knowledge in future generations of architects","Unrealistic expectations of clients using AI-generated designs","Data ownership and security in electronic building manuals"],"safeguards_and_mitigations":["Certification process for AI platforms used in architecture","Design Review Panels to sign off on certain buildings","Legislation to address liability issues when AI is used","Government ownership rights to data in AI-generated building manuals","De-identification of individuals to protect privacy in building manuals"],"examples":{"Compliance issues":"Australia\'s National Construction Code (NCC) is a set of technical design and construction provisions for buildings that apply specifically to Australia. Most AI, however, is developed overseas, using overseas data and requirements.","Liability concerns":"If a builder or architect uses AI to generate the building design, the issue of who is liable for any defects is as yet unclear.","Skills development":"For an architect to advance from a graduate to a registered architect they must do at least 3,500 hours of supervised work. However, if much of this work is taken over by AI (due to the time and cost savings), it could leave future generations of architects unable to advance their careers."},"international_alignment":"null","values":["Safety","Compliance","Professional development"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop appropriate regulatory environment and legislation"},{"entity":"Architects","role":"Ensure compliance and maintain professional standards"},{"entity":"Clients and consumers","role":"Be informed about AI use in building design"}],"sector_impacts":[{"sector":"Architecture","impact":"Potential loss of skills and knowledge in future generations, changes in liability and compliance issues"},{"sector":"Construction","impact":"Potential for non-compliant designs, changes in liability"}],"quotes":["While the Institute does not believe we should fear AI, the Australian government should ensure that there is an appropriate regulatory environment to mitigate the potential downsides.","If the AI gets the answers wrong by not applying the appropriate Australian standards it can lead to expensive outcomes such as: redoing and resubmitting designs that are compliant, rectification of buildings that are built but not to code, building failures, fines, litigation","The financial pressures on architecture firms already make taking on junior architects difficult. The cost advantages of using AI to do much of the work that is often used to train junior architects, means that AI could lead to future generations where few architects are able to progress through to a registered architect."]},"261_Safe & Responsible AI in Australia - NotCentralised Response - 26th July 2023.fc0c10a661f1b.pdf":{"organization_name":"NotCentralised","organization_type":"Technology company","classification":"Proponent","overall_position":"Supports a balanced approach to AI regulation with emphasis on transparency, accountability, and risk-based frameworks","arguments":["Blockchain and AI technologies can enhance AI safety and transparency","A risk-based approach allows for proportional regulation based on potential harm","Transparency is crucial at both input and output stages of AI models"],"counterarguments":["Outright banning of high-risk AI applications could hinder innovation and competitiveness","A risk-based approach may be challenging to implement due to the dynamic nature of AI risks"],"key_recommendations":["Implement cryptographic tagging for AI input and output data","Establish a central AI agency to oversee AI initiatives across government departments","Develop a national AI strategy and roadmap","Create AI testbeds and sandboxes for controlled experimentation"],"risks_and_challenges":["Attribution of blame and liability in AI decision-making","Lack of transparency and explainability in complex AI systems","Potential for bias and privacy infringement in AI applications"],"safeguards_and_mitigations":["Use of blockchain and zero-knowledge proofs for data verification and privacy","Regular AI audits and impact assessments","Implementation of explainable AI systems"],"examples":{"TradeFlows project":"TradeFlows is a blockchain-based payments and invoicing platform, which utilises smart contracts and a collaborative layer to increase trust and ef\ufb01ciency in \ufb01nancial operations.","Healthcare AI models":"In the healthcare domain, we leverage cutting-edge AI tools to design private GPT models that enhance patient outcomes while upholding data privacy.","Reserve Bank of Australia\'s CBDC initiative":"We were one of the 15 pilot projects for the Reserve Bank of Australia\'s Central Bank Digital Currency (CBDC) initiative."},"international_alignment":"Supports international collaboration while maintaining national interests","values":["Transparency","Accountability","Innovation","Privacy","Security"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Develop regulations, provide oversight, and support AI initiatives"},{"entity":"Private sector","role":"Innovate responsibly and implement ethical AI practices"},{"entity":"Academia","role":"Conduct research and contribute to AI policy development"},{"entity":"Public","role":"Engage in consultations and provide input on AI governance"}],"sector_impacts":[{"sector":"Finance","impact":"Enhanced trust and efficiency in financial operations"},{"sector":"Healthcare","impact":"Improved patient outcomes while maintaining data privacy"},{"sector":"Government services","impact":"More efficient and transparent public services"}],"quotes":["NotCentralised believes that a future-focused approach to AI safety must involve a deep integration of AI with blockchain technologies.","Rather than implementing outright bans on certain high-risk AI applications or technologies, we suggest a more nuanced approach.","We \ufb01rmly believe that a risk-based approach is a prudent and balanced strategy for addressing potential AI risks."]},"285_gradient_AI_regulation_submission.03b9a3f99631e.pdf":{"organization_name":"Gradient Institute","organization_type":"Independent, nonprofit research institute","classification":"Proponent","overall_position":"Supports updating existing sector-specific and general regulations, with new technology-specific regulation for frontier AI models","arguments":["Existing sector-specific and general regulations should be applied and updated for AI application risks","New technology-specific regulation is needed for frontier AI models due to their unique development risks","Preset risk-tiering approach proposed in the discussion paper is ineffective for controlling AI application risks"],"counterarguments":["null"],"key_recommendations":["Ensure existing regulations are applied to AI systems and update them as needed","Create a government body with technical expertise to support regulators and advise on AI","Lead global regulatory efforts for frontier AI model safety","Implement national-level standards and enforcement mechanisms for frontier AI models"],"risks_and_challenges":["Dangerous capabilities emerging in frontier AI models","Inability to foresee or identify all dangerous capabilities prior to deployment","Potential for widespread harm from uncontrolled frontier AI model deployment"],"safeguards_and_mitigations":["Establish global safety standards for frontier AI model development","Create mechanisms to verify compliance with safety standards","Implement licensing and liability regimes for frontier AI model developers"],"examples":{"GPT-4 as a frontier model":"GPT-3 almost certainly falls outside the category of a frontier model, whereas the \'early\' (not publicly released) version of OpenAI\'s GPT-4 almost certainly falls within it.","Dangerous capabilities in language models":"LLMs are already capable of facilitating the synthesis of chemical weapons as well as pandemic-class agents.","Successful international treaty":"The Montreal protocol, which regulates production and consumption of ozone depleting substances, is an example of a highly successful treaty."},"international_alignment":"Supports global standards and international coordination","values":["Safety","Accountability","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Lead regulatory efforts and create expert advisory body"},{"entity":"Frontier AI model developers","role":"Demonstrate safety of models before release"}],"sector_impacts":[{"sector":"null","impact":"null"}],"quotes":["Frontier AI models\' designates \'highly capable foundation models that could have dangerous capabilities sufficient to pose severe risks to public safety and global security\'.","If we want to allow foundation models to be widely deployed, democratised, and used for the prosperity of humanity, we must be sure that such widespread usage won\'t eventually lead to the injection of a sequence of prompts that unlocks a dangerous, destructive capability built in during the development stage.","Crucially, since the harms potentially arising from frontier AI model development need not respect jurisdictional boundaries (e.g. devastating cyberwarfare or pandemics), an exclusively national approach isn\'t sufficient to appropriately manage the risk."]},"47_Submission by VOICE Australia - Safe and Responsible AI in Australia; final21Jun2023.0316eb876d7e4.pdf":{"organization_name":"VOICE Australia","organization_type":"NGO","classification":"Proponent","overall_position":"Supports comprehensive regulation and international standards for AI","arguments":["Australia should participate in international standards-making for AI","Consumer protection laws need to be updated for AI systems that don\'t require monetary payments","AI systems from China pose higher risks and require special attention"],"counterarguments":["null"],"key_recommendations":["Participate in international efforts to standardize AI","Update consumer protection laws to cover \'free\' AI systems","Apply a risk-based approach to AI systems from China","Regulate self-motivated AI systems","Remove limitations in the definition of AI"],"risks_and_challenges":["AI systems from China potentially being used to change the world order","Difficulty in predicting behaviors of self-motivated AI systems","Potential for AI systems to do more than generate predictive outputs"],"safeguards_and_mitigations":["Require companies developing self-motivated AI systems to inform regulators","Mandate safety demonstrations for foreign self-motivated AI systems","Apply stricter scrutiny to AI systems from China"],"examples":{"Zoom routing calls through China":"We recall that before Citizen Lab made it public, the Zoom conferencing app quietly routed Australian-to-Australian calls into China, thus such calls could be intercepted on its servers.","TikTok\'s rapid growth":"A cautionary tale is TikTok. It was allowed in because Chinese apps were by default allowed in, like apps from any other country. It quickly grew roots in our society, and removing it now would involve trade-offs that would not have been involved if it were not allowed in in the first place."},"international_alignment":"Supports participation in international standards-making","values":["Safety","Consumer protection","National security"],"tone":"Cautionary","stakeholders":[{"entity":"Australian government","role":"Participate in international standards-making and update regulations"},{"entity":"Regulators","role":"Oversee development of self-motivated AI systems"}],"sector_impacts":[{"sector":"Consumer technology","impact":"Increased scrutiny on AI systems, especially those from China"},{"sector":"AI development","impact":"New regulatory requirements for self-motivated AI systems"}],"quotes":["We believe that the Australian government should play an active part in such international efforts.","How could Australia\'s consumer protection law protect consumers for products or services requiring no monetary payments?","AI systems from China are in a high-risk category and deserve special attention."]},"382_Tranparency Project\'s Submission to Safe and Responsible AI in Australia.86bb116676b83.pdf":{"organization_name":"The Transparency Project","organization_type":"Educational and research initiative","classification":"Proponent","overall_position":"Advocates for a co-regulatory approach to AI regulation with focus on moral and cultural impacts","arguments":["AI will have increasing potential to impact cultural and moral thought systems","Regulation of AI morality may increase public trust and adoption of AI","Moral homogenisation through AI use may lead to loss of cultural identity"],"counterarguments":["Reducing moral contention through unified AI worldviews may result in less friction between people and groups","Moral homogenisation could be seen as the next step of globalisation, potentially reducing friction between nations"],"key_recommendations":["Consider regulations addressing moral worldviews of AI systems, especially in morally-sensitive areas","Adopt a responsive and flexible co-regulatory approach to AI","Establish an independent regulator to register industry-led codes of practice for AI","Conduct further research on moral and cultural impacts of generative AI"],"risks_and_challenges":["Potential loss of moral agency and \'moral deskilling\' in humans","Difficulty in achieving consensus on moral frameworks for AI","Challenges in quantifying and measuring AI morality","Translating moral principles into machine-readable guidelines"],"safeguards_and_mitigations":["Develop a standard against which moral values of AI systems can be tested","Regular and in-depth reporting from AI companies to maximize transparency","Ongoing scrutiny and qualitative assessments of AI systems\' ethical performance"],"examples":{"AlphaGoZero":"AlphaGoZero taught itself to make decisions that allowed it to defeat the world\'s best human Go players \u2013 a feat far beyond the Go-playing skills of the system\'s creators.","Autonomous vehicles and the trolley problem":"If an autonomous car is faced with a choice to either run over a mother and a child or swerve and kill the elderly passenger, which will it choose?","AI in elder care":"Should an AI system prevent an older adult in its care from an evening glass of wine or a second serving of pizza?"},"international_alignment":"Supports consideration of international codes, but with emphasis on Australian values","values":["Transparency","Accountability","Ethical responsibility","Cultural preservation","Moral development"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Establish regulatory framework and independent regulator"},{"entity":"AI companies","role":"Develop AI systems in line with ethical guidelines and report transparently"},{"entity":"Public","role":"Participate in consultation to shape AI moral frameworks"}],"sector_impacts":[{"sector":"Education","impact":"Potential influence on moral development of youth"},{"sector":"Healthcare","impact":"Ethical considerations in AI-assisted care decisions"}],"quotes":["Ultimately, Australia must decide whether our unique and diverse moral fabric is worth maintaining.","Whether or not AI systems can exercise moral judgement is irrelevant. As Sapien said: AI can be designed to respect and adhere to ethical guidelines or rules set by its developers. \u2026 these ethical considerations are imparted by humans, not innately adopted by the AI.","Morality in AI should not go unconsidered in Australia\'s approach to responsible and safe AI regulation."]},"484_Submission 484 - Australian Psychological Society - 11-Aug.0342ca3980ac3.pdf":{"organization_name":"Australian Psychological Society","organization_type":"Professional body","classification":"Proponent","overall_position":"Supports a comprehensive regulatory approach for safe and responsible AI","arguments":["Psychology expertise is crucial for understanding and addressing AI\'s impact on human behavior and well-being","AI has potential benefits for mental health care but requires safeguards","Psychological insights are necessary for designing effective AI regulations"],"counterarguments":["null"],"key_recommendations":["Implement a risk-based approach for AI regulation","Develop industry guidelines and standards informed by professionals","Educate the public about AI benefits and risks","Include psychologists in AI impact assessment processes"],"risks_and_challenges":["AI algorithms perpetuating existing biases","AI anxiety and psychological dependency on AI chatbots","Potential for maladaptive outcomes and harm to individuals and societies","Human displacement and job loss due to AI"],"safeguards_and_mitigations":["Integrate psychological insights into AI policy and regulation","Collaborate with psychologists in AI development and oversight","Design AI systems that are transparent and explainable","Conduct psychological audits to ensure fairness and accountability"],"examples":{"Use of AI in mental health care":"Artificial intelligence is already having a significant impact on mental health care (e.g.2,3).","Psychology\'s contribution to decision-making":"Psychology has been contending with algorithmic versus clinical (human) decision making since the 1950s","Addressing bias and fairness":"Psychology has a long history of studying cognitive biases and decision-making heuristics and how these manifest and affect different groups of people"},"international_alignment":"null","values":["Safety","Transparency","Fairness","Accountability","Human rights"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Psychologists","role":"Contribute expertise to AI development, deployment, and oversight"},{"entity":"Policymakers and regulators","role":"Design and implement AI safeguards considering psychological insights"},{"entity":"AI developers","role":"Collaborate with psychologists to address biases and ensure fairness"}],"sector_impacts":[{"sector":"Mental health care","impact":"AI is already having a significant impact on mental health care"},{"sector":"Workforce","impact":"Concerns about human displacement and job loss due to AI"}],"quotes":["The APS believes that AI has the potential to reap considerable benefits for humanity, including improved health, wellbeing and human potential.","Safeguarding mechanisms must, however, keep up with AI advancements to keep individuals and society safe.","Understanding human psychology and the impact of AI on human emotions, skills, relationships and well-being is the basis of human-AI interaction and is essential to designing and regulating AI systems that are user-friendly but also safe and which do no harm."]},"336_R43-23 DISR - Safe and Responsible AI - Consultation - AFMA Submission.73b0f530b31d3.pdf":{"organization_name":"Australian Financial Markets Association","organization_type":"Industry association","classification":"Opponent","overall_position":"Favors minimal intervention, cautious approach to regulation","arguments":["Existing regulations already address most AI risks","Early regulation may impede industry and innovation","Principles-based sector-specific regulation is sufficient","Industry standards are more likely to keep pace with rapid AI developments"],"counterarguments":["There may be potential for regulatory limitations in consumer space to avoid harms","Some proposals, although nominally risk-based, appear unnecessarily burdensome"],"key_recommendations":["Move slowly with regulation and leverage international regulatory outcomes","Use industry standards as a first step in emerging areas","Increase awareness of existing regulatory limitations on AI","Align programs and strategies with OECD AI Strategies and other international approaches","Discourage regulators from drafting unique rules"],"risks_and_challenges":["Potential for scalability of AI to create additional issues","Risk of inconsistent security standards across regulators","Possibility of stifling innovation through early regulation"],"safeguards_and_mitigations":["Leverage established processes in managing new and emerging risks","Use of industry standards","Principles-based regulation focused on outcomes"],"examples":{"Cyber security regulation":"An opportunity was lost in relation to cyber security when multiple regulators created their own nationally and internationally inconsistent security standards that applied different levels of security for the similar data.","Financial advice":"For financial advice, fiduciary or best interest duties provide sufficient safeguards to address ethical concerns raised in the paper.","Wholesale markets":"In wholesale markets we are not currently aware of any regulatory gaps for AI as outcomes are covered by existing legislation."},"international_alignment":"Supports global standards and international consistency","values":["Innovation","Efficiency","Flexibility"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Support increased awareness of existing regulatory limitations on AI"},{"entity":"Regulators","role":"Adapt common principles to industry-specific cases"},{"entity":"Industry","role":"Develop and adhere to standards"}],"sector_impacts":[{"sector":"Wholesale markets","impact":"Existing legislation covers outcomes, no current regulatory gaps"},{"sector":"Financial markets","impact":"Potential efficiency gains from AI"}],"quotes":["The risks of early regulation in such a fast-moving area are skewed much more towards creating impediments to industry and innovation rather than being empowering of them.","In the wholesale markets, and in more generally in financial markets, however, we suggest a more cautious approach to regulation that moves slowly, leverages international regulatory outcomes, and is careful to preserve the potential efficiency gains that AI is enabling.","AFMA supports risk-based approaches to AI, but at this stage in the development of the technology it is not yet appropriate to be prescriptive except in the most sensitive cases."]},"400_Responsible AI submission SafeGround.61aaef6a9d35c.pdf":{"organization_name":"SafeGround","organization_type":"Civil society organization","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and responsible development of AI","arguments":["AI can pose risks and potential harms if not properly regulated","Digital dehumanisation through AI-enabled decisions can deprive people of dignity and exacerbate inequalities","Autonomous weapons pose legal, ethical, and security risks"],"counterarguments":["null"],"key_recommendations":["Conduct risk assessments for AI applications across public and private sectors","Develop standards and frameworks for testing and evaluating AI systems","Support negotiations for a new legally binding instrument on autonomous weapons","Adopt clear policy ruling out development and use of prohibited autonomous weapons","Ensure meaningful human control over weapons"],"risks_and_challenges":["Digital dehumanisation through automated decision-making","Exacerbation of inequalities for marginalized groups","Weaponisation of AI and development of autonomous weapons"],"safeguards_and_mitigations":["Rigorous testing and evaluation of AI systems by multidisciplinary experts","Public awareness and education on AI and its impacts","Prohibitions and regulations on autonomous weapons"],"examples":{"RobotDebt scheme":"The harms of automated decision-making technology have been highlighted recently in Australia through the RobotDebt scheme.","AUKUS security alliance":"Cooperation on AI is also a key aspect within the advanced capabilities sharing as part of the AUKUS security alliance."},"international_alignment":"Supports global standards and international cooperation","values":["Responsibility","Ethics","Human rights","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Regulate AI use, conduct risk assessments, support international negotiations"},{"entity":"Private sector","role":"Collaborate with government on protocols, ensure responsible AI development"},{"entity":"Civil society","role":"Provide education and training, participate in international meetings"}],"sector_impacts":[{"sector":"Social welfare","impact":"Potential for automated decisions that affect people\'s lives"},{"sector":"Banking and loans","impact":"AI-enabled decisions emerging in financial services"},{"sector":"Justice system","impact":"AI-enabled decisions emerging in legal processes"},{"sector":"Defence","impact":"Development of AI for military applications and autonomous weapons"}],"quotes":["Australians need to be able to trust that AI will be used ethically, safely and responsibly.","It is imperative that the government understands these harms, and regulates the use of automated-decision-making technologies within its own departments and the private sector.","To ensure the responsible use of AI, it is essential there are clear limits on autonomy and that certain autonomous weapons be ruled out."]},"31_Submission 31 - Attachment 1.c65df818705a5.pdf":{"organization_name":"null","organization_type":"null","classification":"Neutral","overall_position":"Advocates for comprehensive regulation across various domains","arguments":["Regulations are necessary to address data protection, privacy, transparency, fairness, and safety concerns","AI systems in high-risk domains require specific regulatory frameworks","Cross-cutting concerns like cybersecurity and user safety need to be addressed"],"counterarguments":["null"],"key_recommendations":["Implement data protection and privacy measures","Ensure transparency and explainability of AI systems","Establish accountability and liability frameworks","Conduct regular audits and assessments to identify and mitigate biases","Develop robust testing, certification, and monitoring processes for AI systems"],"risks_and_challenges":["Bias and discrimination in AI systems","Privacy violations and data misuse","Lack of transparency and explainability","Safety and security vulnerabilities","Ethical concerns in AI development and deployment"],"safeguards_and_mitigations":["Implement stringent data protection measures","Conduct regular vulnerability assessments","Provide clear instructions and warnings to users","Establish mechanisms for redress in case of adverse outcomes","Incorporate ethical guidelines for AI development and use"],"examples":{"Healthcare":"AI is used in various applications, such as medical diagnosis, treatment planning, and patient monitoring","Transportation":"AI is increasingly being employed in autonomous vehicles, air traffic control, and logistics systems","Finance":"AI is used extensively in the finance industry for tasks like fraud detection, algorithmic trading, and customer service"},"international_alignment":"Encourages international cooperation, information sharing, and standardization efforts","values":["Transparency","Fairness","Privacy","Security","Accountability"],"tone":"Neutral","stakeholders":[{"entity":"Developers","role":"Responsible for creating AI systems that adhere to regulations and ethical guidelines"},{"entity":"Operators","role":"Responsible for implementing and maintaining AI systems in compliance with regulations"},{"entity":"Users","role":"Responsible for using AI systems in accordance with guidelines and instructions"}],"sector_impacts":[{"sector":"Healthcare","impact":"Regulations may focus on ensuring accuracy, reliability, and ethical use of AI systems in healthcare settings"},{"sector":"Transportation","impact":"Regulations may aim to ensure safety and reliability of AI-driven transportation systems"},{"sector":"Finance","impact":"Regulations may focus on protecting consumer interests, preventing market manipulation, and ensuring explainability and fairness of AI algorithms"},{"sector":"Criminal Justice","impact":"Regulations may focus on preventing biases, ensuring fairness, and protecting civil rights in AI-driven law enforcement tools"},{"sector":"Education","impact":"Regulations may address privacy concerns, data security, and algorithmic bias in educational AI systems"}],"quotes":["Regulations may require explicit user consent, transparent data handling practices, and data minimization to protect individuals\' privacy rights.","Regulations may address issues of accountability and liability when AI systems cause harm or make erroneous decisions.","Given the global nature of AI, regulations may encourage international cooperation, information sharing, and standardization efforts to promote interoperability, ethical practices, and the avoidance of regulatory fragmentation."]},"326_Feedback and suggestions from Brian Bai.c318e767e3d8a.pdf":{"organization_name":"RMIT","organization_type":"Academic institution","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and guidelines for responsible AI","arguments":["Clear top-level AI guidelines are necessary for responsible AI development","Transparency is crucial for public trust and mitigating AI risks","A risk-based approach can effectively address critical areas and mitigate significant risks"],"counterarguments":["null"],"key_recommendations":["Establish specific top-level AI guidelines aligned with Australian values","Create a dedicated regulatory authority for AI oversight","Implement algorithmic impact assessments and deepfake detection","Develop a comprehensive national AI strategy","Invest in AI education and training programs for government employees"],"risks_and_challenges":["AI products\' autonomous attacks on humans","AI-generated deepfake content","Job displacement","Overwhelming amount of synthesized or exaggerated information"],"safeguards_and_mitigations":["Establish AI ethics guidelines","Implement data privacy laws and cybersecurity standards","Conduct public awareness campaigns on AI risks and benefits","Perform periodic audits and assessments for compliance with policies and ethical standards"],"examples":{"ChatGPT\'s biased response on Russia-Ukraine conflict":"While I requested ChatGPT to describe the conflicts between Russia and Ukraine, it explains as \'Russia occupying the Crimean region of Ukraine in 2014\'. This answer represents a particular perspective, although this point of view is not encompassing the voices and viewpoints of all countries and individuals worldwide.","AI-assisted traffic routing":"This example demonstrates how AI\'s transparency is important and valuable in mitigating potential AI risks and to improving public trust and confidence. Did AI help us? Yes, it helped us indeed, but at the same time, it made a decision for us without any notification/transparency, and many people opt to place unconditional trust in it due to their increasing reliance day by day.","Inaccurate information from ChatGPT":"Recently, while researching minimum social welfare for the elderly in a rural area, I observed a significant disparity between the information provided by ChatGPT and the actual situation. The content seemed to reflect a propagandistic tone in line with government agencies rather than accurately portraying reality."},"international_alignment":"Supports international collaboration and learning from global experiences","values":["Transparency","Fairness","Privacy","Accountability","Public trust"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government agencies","role":"Develop and implement AI governance frameworks"},{"entity":"Private sector","role":"Adhere to AI regulations and ethical standards"},{"entity":"Public","role":"Engage with and trust responsible AI practices"}],"sector_impacts":[{"sector":"Government","impact":"Improved decision-making and resource optimization"},{"sector":"Technology","impact":"Potential limitations on high-risk AI applications"}],"quotes":["I have a hopeful vision for AI technology, envisioning it as a catalyst for greater assistance and support to Australian people.","It is very important to establish specific top-level AI guidelines in Australia. This entails defining the desired characteristics and qualities that align with intended purpose and Australian values.","We should always fully respect the public trust."]},"256_Palantir Australia Response to.pdf":{"organization_name":"Palantir Australia","organization_type":"Technology company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on context-specific applications and end uses of AI","arguments":["AI regulation should focus on end uses rather than the technology itself","A risk-based approach is reasonable but should be applied to specific contexts","Voluntary commitments and self-regulation are insufficient to protect against AI harms"],"counterarguments":["Pausing AI development is not the solution","Transparency alone is not sufficient to address AI risks"],"key_recommendations":["Implement both ex-ante and ex-post regulatory measures","Establish clear accountability mechanisms throughout the AI lifecycle","Provide regulators with necessary resources and expertise","Mandate transparency around data used for AI model training","Incorporate AI risk assessment into existing frameworks"],"risks_and_challenges":["Inherent brittleness of AI technologies","Lack of clarity on accountability in complex AI ecosystems","Potential for bias and discrimination in AI systems","Difficulty in explaining complex AI models to end-users"],"safeguards_and_mitigations":["Implement granular access controls and logging for AI systems","Conduct regular audits and impact assessments","Require human oversight for high-risk AI applications","Establish design standards informed by HCI and Systems Engineering"],"examples":{"Machine Learning brittleness":"ML describes the technique for building models that, once trained, lock in a set of parameters that remain fixed until the model is updated based on new data, model features, optimisation parameters, etc.","AI Ethics principles ineffectiveness":"We believe that AI Ethics Principles are also insufficient to protect against the potential harms of AI, and that observations drawn from operationally-thoughtful and responsibly-constructed experiments in suitably constrained conditions are a more effective approach to understanding and responding to real-world risks.","Failure of self-regulation":"As just one example, the eSafety Commissioner, Julie Inman Grant, recently noted that tech giants have a poor track record of enforcing voluntary pledges."},"international_alignment":"Supports adopting international norms and standards by default, with exceptions where necessary","values":["Accountability","Transparency","Human-centered AI","Operational effectiveness"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government regulators","role":"Provide oversight and enforce AI regulations"},{"entity":"AI developers","role":"Ensure responsible development and provide transparency"},{"entity":"AI deployers","role":"Implement safeguards and conduct risk assessments"},{"entity":"End users","role":"Understand AI limitations and provide feedback"}],"sector_impacts":[{"sector":"Public sector","impact":"Need for additional safeguards and transparency in AI use"},{"sector":"Private sector","impact":"Potential for harmonized AI regulation across sectors"}],"quotes":["We see benefit in Australia adopting international norms and standards by default, and creating differing standards and approaches only by exception where truly necessary.","We consider voluntary commitments, pledges, and self-regulation will likely be insufficient to protect against the potential harms of AI","As we have publicly remarked [0], Palantir believes that meaningful progress towards controlling AI and harnessing it for human flourishing does not come from pausing experiments, but rather - and perhaps somewhat counter-intuitively - from leaning into the fielding of operationally-thoughtful and responsibly-constructed experiments in suitably constrained conditions that force us to identify and confront the real challenges of technologies in situ."]},"348_CPRC Submission - Safe and responsible AI in Australia - DISR - July 2023.aaa32d9f5ee39.pdf":{"organization_name":"Consumer Policy Research Centre","organization_type":"Not-for-profit consumer policy think tank","classification":"Proponent","overall_position":"Supports comprehensive regulation through economy-wide reforms and AI-specific measures","arguments":["Existing consumer protection and privacy laws in Australia are inadequate for the digital age","Economy-wide reforms are needed to create a holistic consumer protection framework","AI-specific regulation is needed in addition to economy-wide reforms"],"counterarguments":["Implementing bespoke regulatory frameworks without adequate foundational economy-wide guardrails will create regulatory burden for businesses","Bespoke frameworks may lead to regulatory arbitrage by rogue businesses"],"key_recommendations":["Introduce an unfair trading prohibition","Reform the Privacy Act","Introduce a general safety provision","Increase enforcement resources for regulators","Provide clear pathways for consumers to access support for digital harms","Require businesses to clearly label when AI is used and disclose how it has been established"],"risks_and_challenges":["Businesses can use AI tactics that create deeply unfair outcomes for customers","Consumer data can be collected and used in ways that causes harm","Seeking redress rests predominantly with individuals","Consumers lack access to help and support when something goes wrong with AI systems","Enforcement often happens after significant consumer harm occurs","Known safety risks continue for years due to slow standard-making processes"],"safeguards_and_mitigations":["Introduce an unfair trading prohibition","Reform the Privacy Act to bring Australia\'s protection framework into the digital age","Make consumer guarantees enforceable","Create a clear pathway to online dispute resolution","Increase regulator resourcing to enable proactive enforcement strategies","Implement a general safety provision","Require transparency across three tiers: pre-implementation, throughout the lifecycle, and to consumers"],"examples":{"Hypothetical supermarket AI pricing":"As an example of a potential unfair practice, a hypothetical supermarket is implementing an AI pricing solution to offer different prices to customers based on their usage of the website and information the business purchases about their other behaviour online. In practice, this leads to people on very low incomes who don\'t use online shopping elsewhere being charged higher prices for essential items.","Facial recognition technology investigation":"As an example, if a privacy safety regime was in place today, it would have meant that some uses of facial recognition technology could have been restricted immediately as the Office of the Australian Information Commissioner investigated its use by Bunnings, The Good Guys and Kmart.","Button battery regulation delay":"As an example, risk of button batteries to young children was identified more than a decade ago and the ACCC first released its national strategy to improve the safety of button battery products in 2016 but the mandatory standards were introduced in 2020 and finally came into effect in 2022."},"international_alignment":"Supports harmonizing with international jurisdictions while considering effective harm mitigation for Australians","values":["Fairness","Safety","Inclusivity","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Federal Government","role":"Implement reforms and regulations"},{"entity":"Regulators","role":"Enforce laws and proactively investigate potential harms"},{"entity":"Businesses","role":"Implement responsible AI practices and prioritize consumer interests"},{"entity":"Consumers","role":"Report issues and seek redress"}],"sector_impacts":[{"sector":"Digital economy","impact":"Increased consumer protection and trust"},{"sector":"Rental market","impact":"Potential regulation of AI-enabled scoring tools"}],"quotes":["Implementing bespoke regulatory frameworks without adequate foundational economy-wide guardrails, will create a difficult to navigate system for consumers and a regulatory burden for businesses that have whole-of-organisation wide systems across multiple frameworks.","Australia must fast-track a range of economy-wide reforms to deliver consumer protections that Australians expect and citizens of other jurisdictions take for granted.","We need the Federal Government to be proactive and not wait for Australians to endure harm first before creating safeguards for them."]},"156_knowledge-orchestrator-sovereign-knowledge-risk.d72316651a1c.pdf":{"organization_name":"Knowledge Orchestrator Pty Ltd","organization_type":"Private company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation to protect intellectual property and prevent anti-competitive practices in AI","arguments":["Foundation AI models pose a high risk to Australian intellectual property and corporate knowledge","Existing data sovereignty protections are insufficient for emerging AI technologies","There is a strong incentive for tech companies to use customer knowledge in anti-competitive ways"],"counterarguments":["null"],"key_recommendations":["Implement legislation to discourage unscrupulous use of commercial data","Ensure Australian intellectual property and commercial knowledge is not automatically included in AI models","Examine whether harvesting customer knowledge for training foundation models creates unfair competitive advantage"],"risks_and_challenges":["Theft of business knowledge through AI models harvesting data hosted on cloud platforms","Unfair competitive advantage for tech companies with access to vast customer data","Irreversible inclusion of commercial knowledge in AI models"],"safeguards_and_mitigations":["Maintain data sovereignty provisions to keep Australian data outside of foundation models","Implement legislative protections for intellectual property rights"],"examples":{"Getty Images lawsuit":"Getty Images recently sued Stable Diffusion for including 12 million of its images in its foundational model","Microsoft\'s potential data access":"Microsoft hosts millions of government and commercial documents on its cloud services such as SharePoint.","SAP\'s industry knowledge":"SAP is the market leader in enterprise software, particularly in large multinationals. This software typically covers multiple business functions, such as accounting, human resources, procurement, production, and sales."},"international_alignment":"null","values":["Data sovereignty","Intellectual property protection","Fair competition"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement legislative protections and examine competitive practices"},{"entity":"Technology companies","role":"Adhere to ethical data use practices and respect intellectual property rights"}],"sector_impacts":[{"sector":"Technology","impact":"Potential limitations on use of customer data for AI model training"},{"sector":"Enterprise software","impact":"Increased scrutiny on data handling and AI model development practices"}],"quotes":["Foundational AI models pose a very high risk to the protection of Australian intellectual property and corporate knowledge.","The emergence of these foundational models now poses a signi\ufb01cant threat to intellectual property rights if technology giants start to train proprietary models using their customer\'s data.","The unauthorised inclusion of commercial knowledge in such models is irreversible and must not be left to voluntary codes of conduct to prevent."]},"397_Supporting Responsible AI Discussion Paper - Castlepoint Submission 20230804.a689884bb1116.pdf":{"organization_name":"Castlepoint Systems","organization_type":"Regtech company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI, especially in information management","arguments":["AI is necessary to address increasing non-compliance risks","Current regulatory approaches do not sufficiently cover AI risks in information management","Both public and private sectors should be subject to AI regulation"],"counterarguments":["Risk-based approaches can lead to subjective assessments and minimized risk treatments"],"key_recommendations":["NAA should take a formal position on ADM risk for records governance","Include definitions of \'black box\' and \'white box\' AI in policy guidance","Make Boards and Directors accountable for Ethical AI compliance failures","Standardize risk and impact models across government and industry"],"risks_and_challenges":["Mismanagement of records leading to harm for individuals","Under-retention or over-retention of sensitive information","Inability to discover important information","Unintended consequences from AI adoption in records management"],"safeguards_and_mitigations":["Explainable and contestable AI for records management decisions","Mandated transparency for AI algorithms","Standardized approach to risk assessment based on potential harm to individuals"],"examples":{"Windrush generation":"Under-retention of records affected the Windrush generation, Caribbean immigrants to the UK who had all their landing card records destroyed by the UK Home Office records team to save space, resulting in many being unlawfully deported.","ANU data breach":"When the Australian National University was breached in 2018, 19 years of staff and student records were taken by a foreign government, 60% of which had been over-retained by the records team and should have already been destroyed.","Manchester Arena bombing":"The UK National Common Intelligence Application database held around 400 pieces of missed information about the man who would become the Manchester Arena terrorist bomber, killing 22 people, and injuring more than 800, most of them children."},"international_alignment":"Supports global alignment with Australian context","values":["Transparency","Accountability","Human rights protection"],"tone":"Cautionary","stakeholders":[{"entity":"National Archives of Australia","role":"Take position on ADM risk for records governance"},{"entity":"Government agencies","role":"Implement responsible AI practices"},{"entity":"Boards and Directors","role":"Be accountable for Ethical AI compliance"}],"sector_impacts":[{"sector":"Government","impact":"Improved records management and compliance"},{"sector":"Corporate","impact":"Enhanced responsibility in handling customer data"}],"quotes":["Artificial Intelligence and automation is the only way to address the increasing risk of non-compliance that comes with a growing population, growing public service, and escalating regulatory and risk environment \u2013 but it must be explainable and able to be contested.","To support responsible AI practices across government agencies, those agencies need to know and recognise that the decisions they make with data can have real impacts on real people. And therefore, any use of AI for those decisions is inherently high risk.","Risk should be mapped against human outcomes, before organisational ones. If an organisation could cause harm to an individual or social group as a result of ADM or AI, that ADM and AI must be explainable and contestable. It is the size and type of the harm that matters, not the size and type of the company."]},"482_Submission 482 - Australian Copyright Council - 11-Aug.03184d624715c.pdf":{"organization_name":"Australian Copyright Council","organization_type":"Independent, not-for-profit organisation","classification":"Neutral","overall_position":"Advocates for copyright protection and licensing in AI development and use","arguments":["Copyright owners should be compensated for use of their material in AI training","Transparency is needed regarding use of copyright material in AI systems","Existing copyright law principles should apply to AI technologies"],"counterarguments":["null"],"key_recommendations":["Disclosure of third-party copyright material used in AI training","Negotiation of permission and licensing for use of copyright material in AI","Development of \'best practice\' guides for government use of AI"],"risks_and_challenges":["Copyright infringement in AI training and outputs","Lack of transparency in use of copyright material for AI development","Uncertainty around copyright protection for AI-generated content"],"safeguards_and_mitigations":["Obtaining permission from copyright owners for use of material in AI","Monitoring compliance with granted permissions","Developing licensing schemes for use of copyright material in AI training"],"examples":{"CAPTCHA system":"The CAPTCHA verification system utilises photographs (and other artistic works) that are copied and uploaded to that system.","The Next Rembrandt project":"A generative AI project known as \'The Next Rembrandt\', that was trained by using the paintings of the Dutch painter, Rembrandt Harmenszoon van Rijn, does not raise copyright infringement concerns as the copyright protection for Rembrandt\'s works has expired","Telstra case":"The copyright issues associated with content created through \'overwhelmingly automated processes\' were examined in the Telstra case"},"international_alignment":"Recognizes different copyright frameworks in other jurisdictions but does not explicitly support global standards","values":["Copyright protection","Transparency","Fair remuneration for creators"],"tone":"Cautionary","stakeholders":[{"entity":"Copyright owners","role":"Grant permission and negotiate licensing for use of their material in AI"},{"entity":"AI developers","role":"Seek permission and pay appropriate remuneration for use of copyright material"},{"entity":"Government","role":"Develop and implement \'best practice\' guides for AI use"}],"sector_impacts":[{"sector":"Creative industries","impact":"Potential for unauthorized use of copyrighted material in AI training and outputs"},{"sector":"AI development","impact":"Need to obtain permissions and potentially pay for use of copyright material"}],"quotes":["It is outside the ACC\'s expertise to comment on any differences that may exist between the process of AI training (at the development stage of the AI system) and ongoing machine learning. However, to the extent that such processes depend on the use of textual material, images or other copyright material, the copyright issues raised are likely to be the same.","If copyright material is reproduced or communicated to the public in the process of developing and improving AI systems (which may not occur in every case of use), then a licence to do so must be obtained with the copyright owner entitled to set the terms for that use, including as to remuneration, if required.","The ACC opposes any attempt to introduce TDM exemptions to the Australian Copyright Act."]},"361_Submission 361 - Fujitsu Australia Limited - 1-Aug.58d2cbb0bd749.pdf":{"organization_name":"Fujitsu Australia Limited","organization_type":"Multinational information and communications technology company","classification":"Proponent","overall_position":"Supports a risk-based approach to addressing potential AI risks","arguments":["A risk-based approach enables proactive risk prioritization","It allows for consistency in evaluating AI solutions","Risk mitigation can lead to new innovation opportunities"],"counterarguments":["Risks can be challenging to quantify precisely","Strict adherence to risk considerations may potentially decrease customer value"],"key_recommendations":["Establish a central ethics review board for evaluating AI technologies across government agencies","Provide comprehensive ethical AI training for government employees","Implement robust whistle-blower protection mechanisms","Mandate transparency requirements across private and public sectors"],"risks_and_challenges":["Bias in AI systems","Model drift","Privacy concerns","Security vulnerabilities","Accountability issues","Misuse of technology"],"safeguards_and_mitigations":["Establish AI guardrails and guidelines","Conduct regular monitoring and retraining of AI models","Implement AI ethics impact assessments","Use tools like Fairness Checker to detect and mitigate bias"],"examples":{"Japan\'s approach to AI regulation":"Japan currently takes a voluntary, risk-based, AI-agnostic approach to regulation of AI.","UK Government\'s coordination bodies":"The UK Government\'s Office for Artificial Intelligence and the UK AI Council act as central coordinating bodies, encouraging collaboration and knowledge sharing among government departments."},"international_alignment":"Supports global standards and engagement in international discussions","values":["Human-centricity","Transparency","Accountability","Fairness","Privacy","Security","Innovation"],"tone":"Positive","stakeholders":[{"entity":"Government agencies","role":"Implement and enforce responsible AI practices"},{"entity":"Private sector","role":"Develop and deploy AI systems responsibly"},{"entity":"Research institutions","role":"Contribute to AI ethics research and guidelines"}],"sector_impacts":[{"sector":"Healthcare","impact":"AI can potentially lead to improved diagnostic accuracy and patient care"},{"sector":"Finance","impact":"AI can enhance fraud detection and risk assessment"},{"sector":"Transport","impact":"AI can improve safety and efficiency in autonomous vehicles"}],"quotes":["We operate our business through the lens of our purpose, which is to \\"make the world more sustainable by building trust in society though innovation\\".","Fujitsu supports a risk-based approach to addressing potential AI risks.","Transparency is important in all stages of the AI lifecycle, from data collection to model deployment."]},"149_Supporting Responsible AI Submission - Future For Now.17f3302509f01.pdf":{"organization_name":"Future For Now","organization_type":"Collaborative project","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and responsible AI implementation","arguments":["Current AI Ethics Principles lack depth on business application of AI","Widespread accessibility of generative AI necessitates revisiting and revising AI Ethics Principles","Government has a crucial role in developing a comprehensive roadmap for AI accessibility and workforce transition"],"counterarguments":["null"],"key_recommendations":["Implement government-subsidised training for the Australian workforce","Reform curriculum to focus on communication skills for an AI-driven future","Establish an AI Diversity Forum to shape policies and processes","Develop a proprietary Australian LLM","Mandate transparency in AI interactions and language model disclosure"],"risks_and_challenges":["Exacerbation of the digital divide","Displacement of workers by AI","Bias in AI systems perpetuating social stereotypes","Difficulty in monitoring AI issues like toxicity and hallucinations at scale"],"safeguards_and_mitigations":["Subsidised training for workforce reskilling","Adoption of bespoke benchmarks to assess LLMs for bias","Mandatory disclosure of AI use in interactions","Implementation of user feedback reporting for AI systems"],"examples":{"Replacement of workers with AI":"Although it may be upheld in some contexts, the scenario of organisations replacing workers with AI. While this move may boost a company\'s profitability, it raises questions about the rights of the affected workers and their ability to contest such decisions.","AI in customer service":"Similarly, if a customer service representative is an AI powered chatbot, not a human, it should be clearly communicated.","AI in meetings":"For instance, \'AI in the room\' disclosures should be made when AI is involved in tasks such as note-taking or transcribing conversations during a meeting."},"international_alignment":"null","values":["Transparency","Diversity","Education","Accountability","Trust"],"tone":"Cautionary yet optimistic","stakeholders":[{"entity":"Government","role":"Develop comprehensive roadmap, provide subsidised training, establish AI Diversity Forum"},{"entity":"Businesses","role":"Responsible application of AI, transparency in AI use"},{"entity":"Educational institutions","role":"Reform curriculum to prepare for AI-driven future"},{"entity":"Workforce","role":"Engage in reskilling and adapt to AI-enhanced roles"}],"sector_impacts":[{"sector":"Education","impact":"Curriculum reform to focus on communication skills and AI literacy"},{"sector":"Small businesses","impact":"Need for support in AI adoption and training"},{"sector":"Public sector","impact":"Potential use of proprietary Australian LLM"}],"quotes":["The government has a crucial role to play in developing a comprehensive roadmap, ensuring AI\'s accessibility to all and preventing an unintended consequence of an exacerbated digital divide.","Government-subsidised training for the Australian workforce is a cornerstone of responsible AI implementation.","Trust stems from explainable, accountable value chains and respect for human intervention within business workflows."]},"228_Submission from the Australian Lottery & Newsagents Association.pdf":{"organization_name":"Australian Lottery & Newsagents Association (ALNA)","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports responsible AI adoption with appropriate regulation","arguments":["AI has potential to enhance operational efficiency and improve customer service for small businesses","Smaller enterprises often lack resources and expertise to adopt AI at the same level as larger businesses","Government support is needed to foster AI adoption among small businesses"],"counterarguments":["null"],"key_recommendations":["Invest in education and awareness programs for small businesses","Develop AI toolkits and consulting services tailored for small businesses","Offer financial incentives such as grants or tax credits for AI adoption"],"risks_and_challenges":["Lack of understanding of AI\'s capabilities among small businesses","Financial burden of investing in AI for small businesses","Potential for AI systems to perpetuate or exacerbate biases"],"safeguards_and_mitigations":["Require AI tools to operate transparently in an open-source manner","Establish clear lines of accountability for AI applications","Implement robust data protection standards","Establish guidelines to prevent AI systems from perpetuating biases"],"examples":{"Potential AI use cases for ALNA members":"For our members, generative AI tools hold promise in areas like customer and staff engagement, training, digital adoption, personalised marketing, predictive analytics, stock management, fraud detection, and customer relationship management."},"international_alignment":"null","values":["Transparency","Accountability","Data Privacy","Fairness"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Provide support and regulation for AI adoption"},{"entity":"Small businesses","role":"Adopt AI technologies responsibly"},{"entity":"Industry bodies","role":"Facilitate education and awareness programs"}],"sector_impacts":[{"sector":"Retail","impact":"Enhanced operational efficiency and customer service"}],"quotes":["AI has immense potential to enhance operational efficiency, improve customer service, and generate new revenue streams for small businesses.","We recommend that the government invest in education and awareness programs. These could be tailored to different sectors and use cases and made accessible via industry bodies like ALNA, online platforms and face to face meetings and webinars.","Generative AI tools should be required to operate transparently in an open-source manner. Users should be able to understand how the AI system functions, how it uses their data, and how it makes decisions."]},"327_2023 IChemE Responsible AI Industry Consultation26072023.d47998949db6c.pdf":{"organization_name":"Institution of Chemical Engineers (IChemE)","organization_type":"Professional association","classification":"Proponent","overall_position":"Supports a risk-based approach for AI regulation","arguments":["Risk-based approaches consider every conceivable consequence and attempt to prevent or mitigate the worst possible consequences","Existing methodologies from process industries like LOPA and QRA can be applied to AI regulation","A sector-led approach with guidance from existing regulators would be ideal"],"counterarguments":["Some sectors may lack experience and be resistant to regulation","Blanket regulation might hinder the use of AI for safety-related applications"],"key_recommendations":["Adopt a risk-based approach for AI regulation","Provide clear communication and guidance/training to industry on relevant AI laws","Implement sector-specific regulations where necessary"],"risks_and_challenges":["Lack of clarity around the role and responsibilities of the duty holder","Skills gap in AI technology","Reliability of data used for AI technology development"],"safeguards_and_mitigations":["Systematic risk assessments","Safety and risk management methods from process industries","Human oversight for high-risk applications"],"examples":{"Safeswim":"Safeswim which is an award-winning public health risk system produced for the Auckland Council, whereby real-time advice is provided on the levels of risk associated with swimming at specific locations","BASF and IntelliSense.io partnership":"a joint partnership between a chemicals company BASF and an AI company IntelliSense.io for the deployment of an open, real-time, decision making platform for the mining industry to make the operations more efficient, sustainable and safe"},"international_alignment":"Supports a combination of centralized and sector-based approaches, similar to EU and UK models","values":["Safety","Responsible innovation","Transparency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Implement regulatory initiatives and provide guidance"},{"entity":"Industry","role":"Comply with regulations and implement responsible AI practices"},{"entity":"Professional associations","role":"Provide expertise and contribute to risk assessment methodologies"}],"sector_impacts":[{"sector":"Process industry","impact":"Increased efficiency and safety through AI applications"},{"sector":"Mining industry","impact":"More efficient, sustainable, and safe operations through AI-powered decision-making platforms"}],"quotes":["IChemE is strongly supportive of risk-based approaches to any regulation effecting the development or use of AI and related tools.","Chemical Engineers have strong expertise in risk-based assessments, particularly in the areas of high-risk process safety.","A sector-led approach whereby existing regulators provide guidance to organisations within their respective fields to implement responsible and ethical AI practices and mitigate potential risks would be ideal."]},"234_Australians for AI Safety - Co-signed letter.d087db0dd62f3.pdf":{"organization_name":"Australians for AI Safety","organization_type":"Advocacy group","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and safety measures for AI","arguments":["AI poses potential catastrophic and existential risks","Current ethical challenges from AI systems are causing serious harms","Mitigating catastrophic risk should not be left to chance"],"counterarguments":[],"key_recommendations":["Recognize the potential catastrophic and existential risks of AI","Take a portfolio approach to mitigating AI risks","Work with the global community on AI safety","Support research into AI safety","Create an AI Commission to ensure a world-leading approach to understanding and mitigating AI risks"],"risks_and_challenges":["Misuse, accident, and catastrophe from powerful AI systems","Targeted harassment, dual-use technologies, deepfakes, misinformation, and disinformation","Potential catastrophic or existential consequences"],"safeguards_and_mitigations":["Create an AI Commission similar to aviation safety bodies","Establish clear liability for harms caused by AI","Support research into AI interpretability and alignment to human values","Train AI safety auditors"],"examples":{"Aviation safety model":"Government doesn\'t need all the technical answers \u2013 but it does need to set the expectation of a culture of safety and transparency, create an independent investigator and a strong regulator, back them with a robust legal regime, and connect them with a global peak body that our Government helps to shape.","Aircraft manufacturer responsibility":"We wouldn\'t allow aircraft manufacturers to sell planes in Australia without knowing the product is safe, and we wouldn\'t excuse a business for being ignorant about the potential harms of its products, so the law should similarly ensure adequate legal responsibility for the harms of AI."},"international_alignment":"Supports global coordination and Australia\'s contribution to international agreements and standards","values":["Safety","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Government","role":"Develop AI strategy, support research, and create regulatory bodies"},{"entity":"AI Commission (proposed)","role":"Ensure world-leading approach to understanding and mitigating AI risks"},{"entity":"AI labs and providers","role":"Ensure safety of AI products and share liability for harms"}],"sector_impacts":[],"quotes":["The economic potential of advanced AI systems will only be realised if we make them ethical and safe.","Mitigating catastrophic risk should never be left to chance.","An AI Commission should approach its work on AI safety in a similar way to how we approach aviation safety."]},"259_AICD_Safe and Responsible AI in Australia.d6fd1f18f5762.pdf":{"organization_name":"Australian Institute of Company Directors","organization_type":"Industry association","classification":"Neutral","overall_position":"Prefers updating existing laws and improving awareness before considering new AI-specific laws","arguments":["Australia has an existing legislative framework which provides regulatory oversight of AI development and use","More work needs to be done to raise awareness and provide guidance on existing obligations","Good AI governance extends beyond legislation and regulatory frameworks"],"counterarguments":["There is a general perception of lack of AI-specific regulation and enforcement under current laws","AI presents unique risks compared to other emerging technologies","Existing framework may need enhancement to prevent regulatory lag in addressing new harms and risks of AI"],"key_recommendations":["Review and update existing legislative framework before considering new AI-specific laws","Raise awareness among AI developers and deployers on existing legal obligations","Provide supplementary and complementary guidance to support understanding of regulatory obligations"],"risks_and_challenges":["AI presenting invented information as fact (\'hallucinations\')","Algorithmic bias","The \'alignment problem\' where AI develops goals contrary to intended goals"],"safeguards_and_mitigations":["Implementing past inquiries and reports, including Privacy Act Review","Establishing clear lines of responsibility and accountability for AI decision makers in organizations","Developing industry and/or sector-specific standards and guidance"],"examples":{"null":"null"},"international_alignment":"Supports monitoring international regulatory developments and broad regulatory alignment with key overseas markets","values":["Responsible AI","Innovation","Regulatory certainty"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and update regulatory framework, raise awareness"},{"entity":"Businesses","role":"Implement AI governance frameworks, understand regulatory obligations"},{"entity":"Directors","role":"Oversee responsible AI use, manage risks"}],"sector_impacts":[{"sector":"Economy","impact":"Boost national productivity and economic growth"}],"quotes":["The AICD recognises the importance of timely adoption of AI in Australia to remain competitive in the global market and boost national productivity and economic growth.","In our view, Australia has an existing legislative framework which provides regulatory oversight of AI development and use, which should be reviewed and updated before considering the introduction of a new AI specific laws.","The AICD recognises good AI governance that is robust yet flexible to changing innovation extends beyond legislation and regulatory frameworks."]},"201_Submission 201 - Attachment 1.1696655c268bc.pdf":{"organization_name":null,"organization_type":null,"classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on environmental impacts","arguments":["AI poses significant risks to the environment and ecosystem health","Current regulatory approaches do not adequately address AI\'s environmental impacts","A risk-based approach is necessary but not sufficient on its own"],"counterarguments":["Regulation could potentially slow down AI development and adoption","Strict regulations might hinder innovation in AI technologies"],"key_recommendations":["Create a federal AI Environment Act","Implement strict regulations on AI use in resource extraction and pollution control","Establish robust monitoring and control mechanisms for AI-generated biological entities","Enforce testing and certification processes for AI-generated materials"],"risks_and_challenges":["Direct and indirect harmful impact of AI on the natural environment","Creation of hazardous materials through AI-driven processes","Misuse of AI in biotechnology and industrial chemistry","Emergent properties and characteristics of AI systems"],"safeguards_and_mitigations":["Implement strict guidelines for the use of AI in power grid management and nuclear technology","Enforce stringent safety standards for AI-driven autonomous vehicles","Mandate safety and environmental impact assessments for AI-driven construction designs","Implement robust cybersecurity standards for AI systems"],"examples":{"Environmental impact of AI":"The impact of each aspect of AI at different stages in the AI life cycle, on the overall state of the natural environment, ecosystems, ecosystem services and processes, waterways & oceans (hydrological systems), air, land, soil, microbiome health and diversity, plant and animal health and biodiversity.","AI in biotechnology":"The generation of ideas for the design and synthesis of highly virulent and pathogenic strains of viruses, bacteria, archaea, fungi, protozoans and nematodes which have the potential to be commodified, compartmentalised by secrecy and weaponized.","AI in industrial chemistry":"The generation of ideas for the design and synthesis of new chemicals for weapons."},"international_alignment":"Supports global standards and international cooperation","values":["Environmental protection","Safety","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, especially related to environmental protection"},{"entity":"AI developers","role":"Implement responsible AI practices and adhere to environmental regulations"},{"entity":"Civil society organizations","role":"Provide input and expertise on AI governance"}],"sector_impacts":[{"sector":"Environment","impact":"Potential harm to ecosystems, biodiversity, and natural resources"},{"sector":"Energy","impact":"Risks associated with AI-driven energy production and management"},{"sector":"Manufacturing","impact":"Potential creation of hazardous materials and environmental pollution"}],"quotes":["The direct and indirect harmful impact of AI on the natural environment, in particular:","Create a federal AI Environment Act which specifically targets AI across its life cycle. In addition to this it maybe useful to create state specific AI Environment Acts which fall within the scope of the federal act.","A risk-based approach for addressing AI risks should encompass the following key elements: Risk Identification, Risk Analysis, Risk Mitigation, Risk Monitoring and Review, Risk Communication"]},"325_RI Comments Safe responsible AI Aust.d301e6b434885.pdf":{"organization_name":"Regulatory Institute","organization_type":"Non-profit think tank","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI","arguments":["Existing regulatory approaches may have gaps in addressing AI risks","A risk-based approach is supported but should be combined with field-based or goal-based approaches","Generic solutions should be favored where possible, backed by AI-specific rules when needed"],"counterarguments":["Excessive use of technology-specific solutions could make for a complex legal system","Bans on certain AI activities might initially depress economic activities"],"key_recommendations":["Use the Model Law on Artificial Intelligence as a basis for developing AI regulation","Implement a risk-based approach mandated through regulation","Establish basic ethical rules to be followed by AI systems","Require impact assessments for AI systems","Mandate transparency requirements throughout the AI product lifecycle"],"risks_and_challenges":["Potential for AI systems to take over control over humankind","AI-related environmental (ecological) challenges","Risks of social scoring, manipulation of democratic processes, and exploitation of vulnerabilities"],"safeguards_and_mitigations":["Continuous surveillance by a supervising authority for high-risk AI systems","Mandatory transparency requirements across public and private sectors","Establishment of alert portals and whistleblower mechanisms","Financial insurance for liability related to AI systems"],"examples":{"EU AI Act":"For now, the EU AI Act seems to be the most solid and consistent regulatory response to AI issues emerging.","Royal Commission into Robodebt":"We appreciate that with the recent findings of the Royal Commission into Robodebt that trust in responsible AI practices in public administration might be low at the moment"},"international_alignment":"Supports global standards, emphasizing the importance of being integrated into globalized AI regulation","values":["Transparency","Accountability","Ethics","Human control","Fairness"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government agencies","role":"Implement responsible AI practices and support public trust"},{"entity":"Developers","role":"Ensure compliance with ethical principles and regulatory requirements"},{"entity":"Operators","role":"Maintain transparency and adhere to regulatory obligations"},{"entity":"Users","role":"Be informed about AI use and have the right to refuse AI processing in certain areas"}],"sector_impacts":[{"sector":"Public sector","impact":"Need for extra requirements on AI products used for public administration or governance"},{"sector":"Tech sector","impact":"Potential impact on trade and exports if high-risk activities are banned"}],"quotes":["The Model Law provides a relatively complete basic pattern for the development of laws or regulation for the control of AI systems.","We believe that public-private differentiation is a suitable method for regulating AI.","Ensuring transparency is important and valuable at any stage of the AI product life cycle."]},"323_Submission 323 - Attachment.21b9f98bc3165.pdf":{"organization_name":"XXX","organization_type":"Software company","classification":"Proponent","overall_position":"Supports balanced regulation with industry participation","arguments":["AI can deliver significant benefits to society if managed carefully","Regulation should balance innovation and risk mitigation","Industry participation is crucial in developing effective AI governance"],"counterarguments":["Overregulation could stifle innovation and economic growth","Generic solutions may not be suitable for all AI applications"],"key_recommendations":["Establish a working group between government agencies and industry partners","Create a voluntary industry code","Implement education campaigns to promote public trust in AI","Improve interstate knowledge sharing and law enforcement collaboration"],"risks_and_challenges":["Privacy breaches","Algorithmic bias","Job displacement","Cybersecurity threats","Malevolent exploitation of AI technologies","Environmental impacts","Educational impacts"],"safeguards_and_mitigations":["Establish strong privacy data protection guidelines","Implement transparent, accessible, and specific guardrails","Conduct regular audits and assessments of AI systems","Provide training and resources on AI ethics and responsible development"],"examples":{"Risk detection for suicide prevention":"XXX\'s AI technology has been used to automatically analyse social media posts as an early warning detection mechanism, to alert of potential suicidal ideation among military veterans","Fraud investigation":"AI-enabled approach can confidently expedite investigations into large volumes of digital evidence to protect Australia\'s consumers and businesses against online scams","Data breach response":"AI-powered text mining automates the identification of PII/PHI and other sensitive information, reducing time spent manually reviewing documents by 600 percent or more"},"international_alignment":"Supports learning from EU and US models while developing an Australian approach","values":["Transparency","Accessibility","Specificity","Innovation","Responsibility"],"tone":"Optimistic but cautious","stakeholders":[{"entity":"Government","role":"Establish working groups, implement regulations, and coordinate AI governance"},{"entity":"Industry","role":"Participate in working groups, comply with voluntary code, and implement responsible AI practices"},{"entity":"Academia","role":"Contribute to research and development of AI governance frameworks"}],"sector_impacts":[{"sector":"Law enforcement","impact":"Enhanced forensic analysis and fraud investigation capabilities"},{"sector":"Healthcare","impact":"Improved early detection of health issues and risks"},{"sector":"Education","impact":"Potential impact on analytical, critical thinking, and creative skills of students"}],"quotes":["XXX is proud to be a sovereign company headquartered in Australia, employing over 100 highly skilled experts within Australia and 430 people globally.","We believe that this current moment in AI\'s evolution represents an historical turning point in societal advancement where the application of AI is emerging from the shadows of science fiction into the mainstream.","XXX\'s AI is supported by three core guiding principles that underpin our Responsible AI development efforts: Explainability, Accessibility, Specificity"]},"301_DHCRC Response to.pdf":{"organization_name":"Digital Health CRC (DHCRC) Ltd","organization_type":"Cooperative Research Centre","classification":"Proponent","overall_position":"Supports a risk-based approach for responsible AI regulation, with a focus on healthcare","arguments":["A risk-based approach is agile, adaptable, and provides clarity","Regulation should focus on outcomes of AI use rather than the technology itself","Different approaches should not apply to public and private sectors use of AI technologies in healthcare"],"counterarguments":["Self-regulation is commendable but cannot replace legally binding AI regulation","Technology-specific solutions to assessing and mitigating risks of AI are to be avoided"],"key_recommendations":["Develop a National AI in Healthcare Strategy","Implement a \'Precision Regulation\' approach for AI governance","Create clear licensing mechanisms for responsible AI use in healthcare","Establish national assurance processes to mitigate AI risks in healthcare"],"risks_and_challenges":["Lack of cross-jurisdictional governance arrangements for accessing healthcare data at scale","Potential risks from use of AI in healthcare apps on smaller platforms","Concerns about timely access to big data sets in Australia"],"safeguards_and_mitigations":["Use of behavioral use licensing and voluntary self-regulation frameworks","Development of best practice industry standards for AI developers and users","Implementation of a comprehensive ethical framework for generative AI in health and medicine"],"examples":{"Use of synthetic data in AI development":"Given the legislative, cultural, and technical challenges in timely access to big data sets in Australia, we can expect the use of synthetic data to be a feature of AI development for some time.","Watermarking of AI-generated content":"Some examples include data licensing agreements, datasheets ensuring transparency of data lineage and ontology, watermarking of software, and watermarking of AI generated content.","EU\'s AI Act":"The EU\'s AI Act: A sweeping regulation aiming to regulate high-risk AI usages, holding bad actors accountable, and becoming a global de facto AI regulation."},"international_alignment":"Supports consideration of international approaches while developing a national strategy","values":["Transparency","Accountability","Ethical design and deployment"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Department of Health and Aged Care","role":"Lead development and implementation of National AI Strategy for Healthcare"},{"entity":"AI developers and deployers","role":"Comply with regulatory requirements and ethical guidelines"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved patient care and health outcomes through responsible AI implementation"},{"sector":"Research","impact":"Enhanced ability to conduct AI research and development in healthcare"}],"quotes":["Different approaches should NOT apply to public and private sectors use of AI technologies in healthcare.","We advocate for a \'Precision Regulation\' approach that establishes rules to govern the deployment of AI in specific use-cases and does not regulate the technology itself.","A risk-based approach for responsible AI will to be mandated through regulation. Self-regulation is commendable but cannot replace legally binding AI regulation."]},"35.pdf":{"organization_name":"GreenSquareDC","organization_type":"Company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on sustainability in AI development","arguments":["Sustainability must be integrated into AI governance mechanisms","Environmental impact of AI development needs to be addressed","Sustainable practices are crucial for responsible AI development"],"counterarguments":[],"key_recommendations":["Include sustainability as a core consideration within AI governance mechanisms","Implement energy-efficient algorithms","Explore renewable energy sources for AI infrastructure","Promote responsible energy consumption in the AI industry","Explore innovative cooling technologies and water recycling systems"],"risks_and_challenges":["Significant increase in energy consumption and carbon emissions in data centres","Unsustainable water use contributing to global water shortages","Hindering progress towards Australia\'s 2030 emission reduction and 2050 net-zero targets"],"safeguards_and_mitigations":["Integrating sustainability principles, regulations, and standards into AI governance","Promoting responsible resource management","Implementing sustainable data centre practices"],"examples":{"GreenSquareDC\'s net-zero data centre design":"By consuming approximately 10 million fewer litres of water per day compared to a traditional data centre, GreenSquareDC is actively reducing water consumption and contributing to sustainable AI development.","Media coverage of AI\'s environmental impact":"Bloomberg and Forbes are already covering the environmental issues that a 5 fold increase in power AI requires over traditional compute creates and its only a matter of time until the more \'mainstream\' media catches on."},"international_alignment":"null","values":["Sustainability","Responsible resource management","Environmental consciousness"],"tone":"Cautionary and urgent","stakeholders":[{"entity":"Government","role":"Develop and implement sustainable AI governance mechanisms"},{"entity":"Industry","role":"Collaborate to drive innovation and promote sustainable practices in AI"},{"entity":"Academia","role":"Collaborate to drive innovation and promote sustainable practices in AI"}],"sector_impacts":[{"sector":"AI industry","impact":"Increased focus on energy efficiency and sustainable practices"},{"sector":"Data centres","impact":"Need for more sustainable designs and operations"}],"quotes":["Incorporating sustainability into the governance mechanisms for AI is crucial to address the environmental challenges associated with its exponential growth.","By integrating sustainability principles, regulations, and standards, Australia can lead the way in responsible AI development while mitigating its environmental impact.","The environmental concerns associated with AI are especially pertinent in the context of Australia\'s 2030 emission reduction and 2050 net-zero targets."]},"457_AI4DM-2022-Challenges copy.2c5dafe766df.pdf":{"organization_name":"Defence Science and Technology Group (DSTG) and Office of National Intelligence (ONI)","organization_type":"Government departments","classification":"Neutral","overall_position":"Focuses on advancing AI capabilities for defense and intelligence applications","arguments":["AI and Machine Learning technology can contribute to critically important defense and intelligence capabilities","AI can enhance mission effectiveness and decision-making in military operations","AI offers opportunities to better exploit information to improve understanding and tempo toward more agile decision making"],"counterarguments":["null"],"key_recommendations":["Develop AI agents for autonomous cyber defense","Advance techniques in areas like graph neural networks, adversarial machine learning, and speech translation","Explore AI applications in facial recognition, post-mortem identification, and detecting deepfakes"],"risks_and_challenges":["Adversarial attacks against ML systems in cyber security","Rapid evolution of deepfake technology","Limited compute, communications and labeled data at the edge for military operations"],"safeguards_and_mitigations":["Develop robust solutions to transmit power level selection that affects both connection and detection aspects","Explore AI approaches to elicit robust tactics in imperfect information games","Investigate self-supervised learning to extract intelligence without relying on large amounts of labeled data"],"examples":{"CybORG framework":"The Autonomous Cyber Operations Discipline has developed CybORG (the Cyber Operations Research Gym), a framework for testing and training Autonomous Cyber Defence agents in simulated scenarios.","Electric Network Frequency for video forensics":"ENF signal also has a subtle influence on the artificial light emitted by sources connected to the electric grid. This information can be captured by camera sensors producing a video, and potentially be used in digital forensic investigations to geo-locate and timestamp the video recording.","Cross-lingual narrative summarization":"This project seeks proposals that investigate and enhance state-of-the-art techniques such as mT5 [1], XSum [2] and XL-Sum [3] toward the automated generation of narrative text summarisation in cross-lingual setting"},"international_alignment":"null","values":["Innovation","National security","Operational effectiveness"],"tone":"Neutral","stakeholders":[{"entity":"Defence Science and Technology Group (DSTG)","role":"Lead research and development of AI technologies for defense applications"},{"entity":"Office of National Intelligence (ONI)","role":"Collaborate on AI initiatives for intelligence applications"},{"entity":"Defence AI Centre (DAIC)","role":"Collaborate on AI initiatives for defense"}],"sector_impacts":[{"sector":"Defense","impact":"Enhanced autonomous cyber defense capabilities, improved decision-making and situational awareness"},{"sector":"Intelligence","impact":"Advanced tools for information analysis, threat detection, and forensic investigations"}],"quotes":["The Artificial Intelligence for Decision Making 2022 Initiative is a national endeavour from the Defence Science and Technology Group (DSTG) and the Office of National Intelligence (ONI), and in collaboration with the Defence AI Centre (DAIC) to fund up to 50 pilot project proposals that will contribute to critically important Artificial Intelligence (AI) and Machine Learning (ML) technology.","AI, ML and deep learning offer opportunities to better exploit information to improve understanding, decision-making and tempo toward more agile decision making.","Military operations may need to resolve rapidly evolving situations where adversaries are adapting their tactics, techniques and procedures and the behaviour of populations is changing."]},"321_20230726_Submission_APA__Responsible_use of AI_Final.ae1b9da7c32f1.pdf":{"organization_name":"Australian Publishers Association","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports regulation of AI, particularly concerning copyright, to ensure transparency, ethical practices, fairness, and adherence to the law","arguments":["Regulation is crucial to prevent massive copyright infringement that will undermine authors and publishers","Unregulated generative AI has the potential to destroy remaining markets for copyrighted material","Fair compensation is essential for a vibrant and sustainable creative industry"],"counterarguments":["null"],"key_recommendations":["Develop an ongoing ethical framework to guide policy and regulation","Enact regulation to prevent massive copyright infringement","Develop policies to uphold the integrity of science, research, and knowledge","Mandate transparency in the use of content by AI","Implement the principle of permission for using copyrighted works in AI training","Ensure remuneration for creators and rights holders","Extend existing licensing practices within copyright law"],"risks_and_challenges":["Absence of legal certainty hampering investment, innovation, and market stability","High upfront investment costs for AI solutions","Limited availability of AI skills and expertise within publishing","Data management complexity","Potential undermining of incentives for creative works","Job losses and displacement of talent","Introduction of biases into knowledge","Replacement of human-generated content"],"safeguards_and_mitigations":["Transparency in AI content usage","Obtaining permission from copyright holders","Fair remuneration for creators and rights holders","Extension of existing licensing practices","Human oversight and scrutiny of AI-generated content"],"examples":{"AI in marketing":"A trade publisher in Australia utilises AI to deliver personalised reading experiences, recommending books, generating summaries, and curating reading lists.","AI in content discovery":"Another publisher uses AI to enhance metadata accuracy, detecting and correcting errors in book titles, authors, and ISBNs.","AI in education":"Educational publishers use AI to personalise learning experiences by analysing student data. AI adapts content and assessments based on individual needs, like an AI language platform adjusting lessons to proficiency and interests."},"international_alignment":"Supports global alignment of AI regulation","values":["Transparency","Fairness","Ethical practices","Human creativity and authorship"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations"},{"entity":"Publishers","role":"Implement ethical AI practices and protect copyrighted content"},{"entity":"Creators","role":"Produce original content and receive fair compensation"},{"entity":"AI developers","role":"Seek licensing or permissions for copyright-protected works"}],"sector_impacts":[{"sector":"Publishing","impact":"Potential for improved efficiency and personalized content, but risks to copyright and creative markets"},{"sector":"Education","impact":"Enhanced personalized learning experiences and improved content quality"},{"sector":"Science and research","impact":"Potential threats to the integrity of scientific knowledge and research"}],"quotes":["Generative AI companies, operating as commercial enterprises, should not exploit copyrighted works without transparency, authorisation, remuneration, or acknowledgment \u2013 otherwise the interests of the creators and rights holders are undermined.","If creators are not fairly remunerated, creative markets may suffer from decreased motivation, talent drain, declining quality, and limited diversity.","Publishers uphold the significance of human authorship, recognising the invaluable role of writers and artists in the creative economy. AI must be used responsibly to avoid undermining creators\' contributions and the cultural and economic value of their work."]},"500_Submission 500 - Commonwealth Bank - 15-Aug.36c2cc1cb32ac.pdf":{"organization_name":null,"organization_type":null,"classification":null,"overall_position":null,"arguments":[],"counterarguments":[],"key_recommendations":[],"risks_and_challenges":[],"safeguards_and_mitigations":[],"examples":{},"international_alignment":null,"values":[],"tone":null,"stakeholders":[],"sector_impacts":[],"quotes":[]},"193_Safe and Responsible AI.e7c01332cd696.pdf":{"organization_name":"Piston Labs","organization_type":"AI and robotics startup","classification":"Proponent","overall_position":"Supports comprehensive and adaptive AI regulation with a risk-based approach","arguments":["A risk-based approach allows for efficient resource allocation and targeted mitigation efforts","Comprehensive regulation is necessary to address the complex and evolving nature of AI risks","Balanced regulation can foster innovation while ensuring responsible AI development"],"counterarguments":["Overly stringent regulations could impede AI innovation, particularly for smaller organizations","A blanket ban on entire technology categories may hinder beneficial applications"],"key_recommendations":["Establish clear ethical guidelines and regulatory frameworks for AI development and deployment","Implement a risk-based approach that prioritizes high-risk AI applications","Foster collaboration between government, industry, academia, and civil society","Encourage transparency, explainability, and accountability in AI systems","Develop AI-specific education and training programs"],"risks_and_challenges":["Bias and discrimination in AI decision-making","Privacy and data protection concerns","Lack of transparency and accountability in AI systems","Potential for misuse or malicious use of AI technologies","Socioeconomic impacts and job displacement"],"safeguards_and_mitigations":["Regular audits and assessments of AI systems","Implementing explainable AI techniques","Establishing clear data governance principles","Developing robust security measures and incident response plans","Creating public-private partnerships for responsible AI development"],"examples":{"Healthcare sector":"AI-driven medical diagnostics, treatment recommendations, and autonomous surgery are high-risk AI applications, where wrong decisions or errors can have severe consequences.","Financial sector":"AI applications in financial markets for algorithmic trading and credit risk assessment have significant potential risks, but the industry has built robust risk models and compliance frameworks.","Transportation sector":"Autonomous vehicles relying on AI for decision-making need to navigate unpredictable real-world scenarios, requiring thorough risk assessments for safety assurance."},"international_alignment":"Supports international collaboration and harmonization of AI standards","values":["Transparency","Accountability","Fairness","Privacy","Security","Innovation"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, provide funding for research and development"},{"entity":"Industry","role":"Implement responsible AI practices, collaborate on standards development"},{"entity":"Academia","role":"Conduct research, provide expertise, and contribute to policy development"},{"entity":"Civil society","role":"Provide diverse perspectives, engage in public consultation"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved diagnostics and treatment recommendations, but requires careful risk management"},{"sector":"Finance","impact":"Enhanced risk assessment and algorithmic trading, with established risk management practices"},{"sector":"Transportation","impact":"Autonomous vehicles offer potential benefits but require thorough safety assessments"},{"sector":"Education","impact":"AI-powered educational platforms need risk assessments to ensure effective and unbiased learning experiences"}],"quotes":["A risk-based approach is a pragmatic and sensible way to address potential AI risks, especially in the current state of AI development.","Transparency is a crucial aspect of AI governance, playing a significant role in mitigating potential risks and fostering public trust and confidence in AI systems.","Piston Labs is uniquely positioned to contribute to the \'Safe and responsible AI in Australia framework\' through this public consultation process."]},"119_BCSDA Submission I Responsible AI in Australia - FINAL.1b2fc6a7d3f52.pdf":{"organization_name":"Business Council for Sustainable Development Australia","organization_type":"Business-led sustainability and business peak body","classification":"Proponent","overall_position":"Supports a risk-based approach to AI governance with a combination of regulatory and non-regulatory measures","arguments":["A risk-based approach allows for targeted mitigation efforts and efficient resource allocation","Combination of regulatory and non-regulatory measures can balance innovation with responsible AI practices","AI governance should align with sustainable development goals and ethical considerations"],"counterarguments":["Risk-based approaches may face challenges in assessing and quantifying risks due to uncertainties and subjectivity","Rapid technological advancements may outpace regulatory frameworks based on risk assessment"],"key_recommendations":["Establish clear ethical and responsible AI guidelines for government agencies","Implement mandatory impact assessments for high-risk AI applications","Develop certification programs to assess and verify conformity of AI systems with established standards","Foster collaboration between government, industry, and academia to develop best practices"],"risks_and_challenges":["Algorithmic bias and fairness issues in AI systems","Lack of explainability and transparency in AI decision-making","Data privacy and security concerns","Potential for job displacement and exacerbation of societal inequalities"],"safeguards_and_mitigations":["Mandatory human intervention and oversight for high-risk AI applications","Regular monitoring and auditing of AI systems","Implementation of robust data privacy and security regulations","Promotion of diversity programs in the AI industry"],"examples":{"Microsoft\'s AI for Earth program":"Microsoft\'s AI for Earth program utilizes AI algorithms to process satellite imagery and sensor data, aiding in environmental monitoring and ecosystem management.","BlackRock\'s ESG performance measurement":"BlackRock employs natural language processing and machine learning algorithms to analyze ESG data from various sources.","Fujitsu\'s AI Ethics Impact Assessment":"Fujitsu has developed an AI Ethics Impact Assessment to assess the ethical impact of AI on people and society."},"international_alignment":"Supports alignment with international frameworks and guidelines on AI governance","values":["Transparency","Accountability","Fairness","Sustainability","Ethical considerations"],"tone":"Positive and cautionary","stakeholders":[{"entity":"Government","role":"Develop and implement AI governance frameworks, promote responsible AI practices"},{"entity":"Businesses","role":"Adopt responsible AI practices, collaborate in developing industry standards"},{"entity":"Academia","role":"Conduct research, provide expertise on AI ethics and governance"},{"entity":"Civil society","role":"Participate in public consultations, provide diverse perspectives on AI impacts"}],"sector_impacts":[{"sector":"Healthcare","impact":"Enhanced data collection and analysis for improved patient care and disease detection"},{"sector":"Finance","impact":"Improved ESG performance measurement and risk assessment"},{"sector":"Environment","impact":"Better environmental monitoring and ecosystem management through AI-driven data analysis"}],"quotes":["We recognize the importance of carefully evaluating high-risk AI applications and technologies. While a blanket ban may not be the most effective approach, certain circumstances may warrant restrictions or regulatory measures to mitigate potential risks.","A risk-based approach can be tailored to suit different sectors, AI applications, and organizations based on factors such as organization size, AI maturity, and available resources.","We support a risk-based approach for addressing potential AI risks. A risk-based approach enables a nuanced and targeted assessment of AI applications, allowing resources to be allocated based on the level of risk they pose."]},"250_R Sheh&K Geappen response to Safe and Responsible AI in Australia discussion paper 2023.1e66e0597cf32.pdf":{"organization_name":"null","organization_type":"Individual researchers","classification":"Proponent","overall_position":"Supports comprehensive regulation with a focus on outcomes-based approach and risk management","arguments":["AI is a broad field, and regulations should not be limited to machine learning","Outcomes-based requirements can foster responsible AI development","Risk-based approaches are necessary due to the wide-ranging applications of AI"],"counterarguments":["Banning specific AI technologies may stifle domestic research and innovation","Regulations should not treat all AI applications the same way","Transparency requirements need careful definition to be meaningful"],"key_recommendations":["Develop common, technically actionable definitions for AI terms","Implement AI Ethics star ratings and Product Disclosure Statements","Create an AI Ombudsperson to address concerns and publicize investigations","Invest in home-grown AI technologies and research","Develop metrics and requirements for AI systems in different sectors"],"risks_and_challenges":["AI interfering with root cause analysis of failures","AI hiding or obfuscating other problems","Cross-cutting nature of AI challenging traditional regulatory boundaries","Lack of transparency in AI decision-making processes","Potential for bias in AI systems"],"safeguards_and_mitigations":["Require systems that can be subject to audit and assessment in high-risk sectors","Ensure meaningful human review of decision-making processes","Provide robust notifications and recalls mechanism for AI issues","Implement regulations for legal obligations to address AI failures","Use synthetic datasets for privacy-sensitive applications"],"examples":{"Centerlink Robodebt incident":"The well publicised Centerlink Robodebt incident is an example of a non-AI based ADM system that should similarly be covered.","Blaming ChatGPT for incorrect citations":"We already see this when people \'blame IT\' for problems that have nothing to do with IT and the blaming of ChatGPT (classed by some as Generative AI) for incorrect citation in legal cases due to over-trust.","AI voice authentication systems":"For example, some Australian government agencies have deployed AI that analyses the voice of callers to authenticate them. The risk of using such AI systems has changed considerably with the advent of Generative AI that can learn someone\'s voice and generate fake, but very believable, audio of them saying anything."},"international_alignment":"Supports using international standards as a starting point, but emphasizes the need for Australian-specific approaches","values":["Transparency","Accountability","Fairness","Privacy","Safety","Ethics"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop regulations, invest in AI research, and grow internal AI expertise"},{"entity":"Private sector","role":"Develop and implement responsible AI practices"},{"entity":"Consumers","role":"Make informed choices about AI use and have agency in AI interactions"},{"entity":"Researchers","role":"Develop AI technologies that meet regulatory and ethical standards"}],"sector_impacts":[{"sector":"Education","impact":"Risk of embedding bias into generative AI that can be used for external influence"},{"sector":"Healthcare","impact":"Potential for improved diagnostics but requires careful regulation"},{"sector":"Finance","impact":"Need for transparency in AI-based decision making"},{"sector":"Transportation","impact":"Safety concerns and need for root cause analysis capabilities"}],"quotes":["We are unique in the world and therefore should not be blindly trusting or relying on other nations to define what is \'Safe and Responsible\'.","Imagine a future where Australians are known for their ethical AI, just like the Swiss are known for their precise watches or the Japanese for their quality electronics.","Regulations should correspond to risk, which includes a component of impact and influence. These should be rooted in the AI Ethics Principles and be quantifiable and measurable in a meaningful way."]},"489_Submission 489 - Interactive Games & Entertainment Association - 11-Aug.ce6b6d8f2b8c8.pdf":{"organization_name":"Interactive Games & Entertainment Association (IGEA)","organization_type":"Industry association","classification":"Neutral","overall_position":"Favors minimal intervention for AI in video games, supports risk-based approach if regulation is necessary","arguments":["AI use in video games is low risk and focused on entertainment, safety, or innovation","Current AI use in games doesn\'t require additional regulatory oversight","Any regulation should be evidence-based, carefully scoped, and limited to what is practically necessary"],"counterarguments":["Some AI applications may be considered high risk and require regulation","Certain AI practices exploiting vulnerable groups like children may need to be banned"],"key_recommendations":["Consider a targeted and risk-based approach to regulating AI similar to the European Union\'s AI Act","Take a balanced and nuanced approach to regulating AI tools or applications applicable to children","Consult with Industry on any possible regulatory conflict between the development and employment of safety technology using \'high risk\' AI","Consider implementation of industry-specific risk-based, voluntary regulation of AI in video games instead of legislative approaches","Consider the impact of AI on copyright policy and commence dialogue with the creative sectors as a matter of priority"],"risks_and_challenges":["Potential interference with safety efforts in the video games industry","Possible stifling of innovation and creativity in the video games industry","Difficulties in implementing prescriptive regulation due to the unique nature of video games"],"safeguards_and_mitigations":["Industry-led voluntary self-regulation model","Use of AI for safety and moderation purposes in games"],"examples":{"AI-controlled opponents":"AI-controlled \'opponents\' to give the player in a single-player game a competitive challenge.","AI for pathfinding":"AI is vital to enabling efficient pathfinding in games to determine how to get a non-human controlled character from one point in an environment to another (e.g. on map-based games).","AI for safety":"Trust and safety efforts in the video games are also bolstered by AI, such as Ubisoft and Riot Games\' collaborative \'Zero Harm in Comms\' research project which uses AI to detect harmful content in in-game player-to-player communication."},"international_alignment":"Supports considering EU AI Act as a model if regulation is necessary","values":["Safety","Innovation","Entertainment"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Carefully consider regulation and consult with industry"},{"entity":"Video game industry","role":"Implement safe and responsible AI practices"}],"sector_impacts":[{"sector":"Video game industry","impact":"Potential stifling of innovation and creativity if overregulated"},{"sector":"Creative and artistic industry","impact":"Concerns about intellectual property rights and ownership of AI-generated works"}],"quotes":["Generally, AI in video games is not a context that would benefit from or require additional regulatory oversight.","We are of the position that any regulation should be evidence-based, carefully scoped and limited to what is practically necessary.","The intention of AI in the video game industry is to ultimately enhance player enjoyability, accessibility and safety."]},"349_AI & Psychology 2023.9f1422b95ba85.pdf":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and attention to AI\'s impact on society and human psychology","arguments":["AI poses a significant threat to people and society by potentially diluting trust","AI can disrupt fundamental psychological functions and individual autonomy","Current research and regulation are insufficient to address potential implications of AI misuse"],"counterarguments":[],"key_recommendations":["More attention and focus on AI\'s implications","More research on AI\'s effects","Regulation to address potential implications of AI misuse"],"risks_and_challenges":["Disruption of trust in society","Displacement of workforces","Creation of indistinguishable fake content","Mental health distress at individual and community levels","Loss of human autonomy and individual perspective"],"safeguards_and_mitigations":["null"],"examples":{"Climate change as an example of cumulative global impact":"A good example is climate change whereby we know it is happening, individuals can observe everyday minor changes, but never seeing it as a direct threat\u2026 However, once we see the cumulative impact, ice melting, deforestation, animal extinction, the changes to atmosphere, we can deduct a period of significant change and worrying outcomes at a global stage.","AI\'s impact on social media and content creation":"On a day-to-day level, we are seeing content that is fabricated to the point where we question its authenticity. The content can be indistinguishable from original content, with vast improvements and investments in deep fake, manufactured images, targeted content, and algorithms directed to artificially navigate content, based not on individual choices, but targeted to attract and retain attention for revenue streams."},"international_alignment":"null","values":["Trust","Human connections","Individual autonomy"],"tone":"Cautionary","stakeholders":[{"entity":"Psychologists","role":"Analyze and address AI\'s impact on human psychology and society"},{"entity":"AI developers","role":"Responsible development of AI technology"},{"entity":"Social media companies","role":"Ethical use of AI in content creation and distribution"}],"sector_impacts":[{"sector":"Social media","impact":"Creation of vast amounts of potentially inauthentic content"},{"sector":"Financial services","impact":"Potential disruption of traditional services"},{"sector":"Technology","impact":"Potential disruption of traditional services"}],"quotes":["As a psychologist, the most concerning impact of A.I. is a significant threat to people and society, as this technology can \'dilute\' or \'flood\' information with a sense of authenticity.","For nations, societies, and communities to address some of the global challenges, people will need to rely on the fundamental human ability to trust one another. If that cannot be achieved or it is monetised, monopolised, or manipulated, there is a fundamental disruption to the equity and access in our societies.","As A.I. gains traction and evolves (This is a self learning machine with a life of its own) its developers have no way of knowing how this tool will evolve, with the mild assurance that they can manage the direction of tool to restrict content based on manual intervention and tweaks."]},"4_Submission to Responsible AI in Australia \u2013 Gary Looney \u2013 1 June 2023.2c07cdd55a85b.pdf":{"organization_name":null,"organization_type":"Individual submission","classification":"Proponent","overall_position":"Supports new AI-specific laws and regulations","arguments":["AI can be used for basic scams and platform manipulation","Big businesses use AI to control people\'s actions and options","Current regulations have failed to address AI risks"],"counterarguments":["Development control is futile due to inability to monitor and police","Arguing regulation will stifle business is stealing money from the public"],"key_recommendations":["Create AI Safety Laws and regulatory bodies","Include input from vulnerable unpaid non-advocate people","Implement laws to prevent legal crime and scams","Enforce email verification for account creation"],"risks_and_challenges":["Use of customer big data to create manipulative AI systems","AI interfaces with fake self-awareness used in business and underworld","Identity deception and impersonation","Platform manipulation and fraudulent account creation"],"safeguards_and_mitigations":["Mandatory email verification for account creation","Automatic prosecution and fines for companies failing to verify email ownership","Banning repeat offenders in Australia"],"examples":{"TikTok impersonation account":"Someone has created a Fake Fraudulent TikTok account https://www.tiktok.com/@garylnqhwi3 They have used text \'garyl\' in account name and my email address garylooney@gmail.com","AI-powered complaint system":"Example given actually happened, where redirection blocked me from opening complaint and demanded answer to unrelated question that couldn\'t be answered without further Company info.","Platform manipulation":"My example is contact with TikTok to demand closure of an intimidation account using my email."},"international_alignment":null,"values":["Accountability","Transparency","User protection"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Create and enforce AI safety laws"},{"entity":"Vulnerable unpaid non-advocate people","role":"Provide input to reduce bias in regulation"},{"entity":"Companies","role":"Comply with regulations and verify user information"}],"sector_impacts":[{"sector":"Social media","impact":"Stricter account verification and user protection measures"},{"sector":"Customer service","impact":"Regulation of AI-powered complaint systems"}],"quotes":["Regulation is where Trust Fails","In the case of End-use AI \\"Law is because Regulation has Failed\\"","We need Laws that prevent legal crime. Prevent Scammers doing crime because they can."]},"456_Submission \u2013 Thorburn, Bristow, Carroll.9d8f1fe6ba02a.pdf":{"organization_name":"null","organization_type":"Academic researchers","classification":"Proponent","overall_position":"Supports responsible AI practices with a focus on risk-based regulation and democratic oversight","arguments":["Existing regulatory approaches may not be sufficient to address new AI risks","Technology-agnostic regulation tends to be more robust and relevant in the long term","Democratic oversight and public involvement are important for governing AI impacts"],"counterarguments":["Determining risk categories for AI uses can be subjective or uncertain","Risk bands may be too coarse for expressing appropriate mitigations in many contexts"],"key_recommendations":["Create an in-government AI governance consultancy","Convene participatory democratic processes for AI regulation development","Require clear information about AI system limitations for users","Include systemic risks in risk-based regulatory approaches","Write regulations in technology-agnostic ways where possible"],"risks_and_challenges":["Unintended harms from actors naive of AI risks","Adverse feedback loops between human users and algorithmic systems","Production of novel pathogens","Proliferation of fraud","Concentration of power and financial windfalls from AI technology","Systemic risks such as AI monopolies and uncontrollable autonomous agents"],"safeguards_and_mitigations":["Require visible consumer-facing information about AI system limitations","Develop educational materials on AI models\' strengths and weaknesses","Support public education about AI system limitations","Fund research into responsible AI practices"],"examples":{"US intelligence community difficulty in evaluating truth":"If the US intelligence community finds it difficult to evaluate the truth of their products, when they have strong incentives to be accurate, then it is implausible that the Australian government could evaluate accuracy at scale.","Naive implementation of AI systems":"We frequently come across cases of well-meaning individuals deciding to implement some form of AI system \u2014 such as using language models to automatically grade postgraduate university applications \u2014 with simply no awareness that such systems may be biased or discriminatory.","Potential manipulation by recommender systems":"For example it is possible that a capable recommender system or large language model, in the course of optimising for a measure of engagement or user retention, may learn to systematically manipulate users\' preferences."},"international_alignment":"null","values":["Democratic oversight","Responsible AI practices","Public education and awareness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and implement AI governance measures"},{"entity":"Companies developing and deploying AI","role":"Provide clear information about AI system limitations"},{"entity":"Public","role":"Participate in democratic oversight of AI systems"}],"sector_impacts":[{"sector":"Education","impact":"Potential for biased or discriminatory AI systems in applications like grading"},{"sector":"Healthcare","impact":"Potential for AI to address workforce shortages"}],"quotes":["Wherever possible, write regulations in ways that are technology-agnostic.","Convene, with expert help, participatory democratic and deliberative processes such as citizens assemblies, to assist in the development of AI regulation.","Consider creating an in-government AI governance consultancy that can develop best practice, hire relevant experts, and coordinate AI governance efforts across other parts of government."]},"202_Questionnaire Responses.73793ae3bd1ab.pdf":{"organization_name":"Trajecient","organization_type":"Company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on individual agency and safeguards","arguments":["Current voluntary code and existing regulatory approaches are insufficient to address AI risks","A risk-based approach alone is not enough and should be supplemented with other measures","Mandatory regulation is necessary for higher-risk AI applications to ensure compliance and fairness"],"counterarguments":["Voluntary or self-regulation may be sufficient for lower-risk AI applications","Excessive regulation could potentially stifle innovation or create unnecessary burdens"],"key_recommendations":["Create an independent oversight body for AI governance across all levels of government","Implement a risk-based approach supplemented with transparency, accountability, and opt-in/opt-out mechanisms","Ban certain high-risk AI applications such as social scoring and some uses of facial recognition technology","Develop a guide summarizing relevant laws and contextual information to assist in AI localization","Mandate transparency requirements across private and public sectors"],"risks_and_challenges":["Potential for AI systems to be misused or circumvented for malicious purposes","Risks related to algorithmic bias, privacy violations, and cybersecurity breaches","Challenges in addressing individual factors that can alter the calculus of risk for AI applications"],"safeguards_and_mitigations":["Implement strong transparency, accountability, and opt-in/opt-out requirements","Provide system explanations on request, regardless of risk level","Ensure human oversight and intervention, especially for higher-risk AI applications","Limit non-disclosure agreements to allow for public disclosure of certain training information"],"examples":{"Robodebt scandal":"Robodebt is one example to illustrate that principles do not automatically follow from the operations of government agencies and/or associated partners.","Domestic abuse scenario":"A scenario in the Consultative Submission is a case of domestic abuse, where what would normally be a desirable and safe use of AI for recommendation services is inappropriate in this instance.","General purpose AI systems misuse":"For example, a software program could be created by an individual or a group to automatically substitute words or phrases of praise with hate speech."},"international_alignment":"Supports consideration of international approaches while emphasizing the need for Australian-specific measures","values":["Transparency","Accountability","Fairness","Human agency","Safety","Privacy"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Establish and enforce AI regulations, create oversight body"},{"entity":"AI developers","role":"Implement safeguards and comply with regulations"},{"entity":"AI deployers","role":"Ensure responsible use of AI systems and comply with regulations"},{"entity":"Individuals","role":"Exercise agency through opt-in/opt-out mechanisms and provide feedback"}],"sector_impacts":[{"sector":"Public sector","impact":"Higher standards of oversight and contestability required due to \'duress by only resort\' situations"},{"sector":"Private sector","impact":"Potential for different regulatory impacts based on risk levels and market dominance"}],"quotes":["Safe and responsible AI practices would be to consider whether there any potential harmful side-effects or consequences to such requests.","The best way to do this is through an approach that is about checks then balances. First determine by simple, streamlined questions what the level of general risk is and then go into a more detailed process if and as necessary.","Mandatory regulation would permit greater scope for enforcement actions."]},"490_Submission 490 - National Association for the Visual Arts - 11-Aug.e58712652a70f.pdf":{"organization_name":"National Association for the Visual Arts (NAVA)","organization_type":"Independent membership organisation","classification":"Proponent","overall_position":"Supports comprehensive regulation of AI to protect artists\' rights and livelihoods","arguments":["AI poses significant challenges to copyright, privacy, and data protection","Current laws are insufficient to protect artists from copyright infringement","Transparency and copyright protection are essential to protect creator livelihoods"],"counterarguments":["AI offers creative and supportive potential for artists","Some artists are already producing brilliant work using AI platforms"],"key_recommendations":["Promote Australia\'s AI Ethics Principles broadly","Fund arts peak organisations to develop sector-specific guidelines","Implement a risk-based approach grounded in international human rights law","Mandate transparency requirements across private and public sectors"],"risks_and_challenges":["Threat to employment prospects for creators","Potential reduction in income for creators","Unauthorized use of artists\' work in AI training data","Perpetuation of inauthentic and fake art, especially concerning Indigenous Cultural Intellectual Property"],"safeguards_and_mitigations":["Statutory compliance with AI Ethics Principles","Greater transparency in AI development and use","Toolkit for organisations to assess risks to individual rights and freedoms","Mechanisms for creators to voice and defend their rights"],"examples":{"Survey results on AI impact":"Almost 40% of survey respondents use generative AI in their creative process. Some use it to assist with written work, including editing and grant applications through ChatGPT, and others for the development of creative content and ideation processes.","Copyright infringement statistics":"Of artists whose rights were infringed: 56% of visual artists and 89% of craft practitioners had their work repurposed without attribution; 48% and 54% respectively had work repurposed without permission"},"international_alignment":"Supports a model similar to the United Kingdom\'s approach","values":["Transparency","Copyright protection","Ethical AI practices"],"tone":"Cautionary","stakeholders":[{"entity":"Artists and creators","role":"Protected by AI regulations"},{"entity":"Government agencies","role":"Implement and enforce AI regulations"},{"entity":"AI developers and platforms","role":"Comply with regulations and ethical principles"}],"sector_impacts":[{"sector":"Creative industries","impact":"Potential threat to employment and income of artists"},{"sector":"Indigenous art","impact":"Risk of perpetuating inauthentic and fake art"}],"quotes":["Although Artificial Intelligence (AI) has been around for decades, recent developments have made it more accessible than ever before, prompting questions and speculations about its use and impact on artists\' work, practices, and livelihoods.","If individual artists are to gain the full economic benefit to which their creative endeavour entitles them, their intellectual property must be adequately protected against unauthorised exploitation or appropriation.","93% of respondents support the introduction of either guidelines, regulations, a code of practice of legislative protections to regulate generative AI platforms."]},"272_AI discussion paper_University of Newcastle_v2672023.65e67e662d945.pdf":{"organization_name":"University of Newcastle","organization_type":"University","classification":"Proponent","overall_position":"Supports a risk-based approach with a mix of regulatory and non-regulatory initiatives","arguments":["A risk-based approach allows for tailored responses to different AI applications","Existing regulations may not adequately address AI-specific challenges","Non-regulatory initiatives can complement regulatory approaches"],"counterarguments":["A risk-based approach may have limitations in addressing all potential AI risks","Some high-risk AI applications may require outright banning"],"key_recommendations":["Develop a more detailed risk categorization system","Establish AI Knowledge Centres at universities","Implement AI education in schools","Create a National Institute for Data Forensics and AI Security"],"risks_and_challenges":["Potential misuse of AI techniques","Lack of explainability in AI decision-making","Privacy concerns related to data use in AI systems","Potential for AI to amplify existing risks in certain domains"],"safeguards_and_mitigations":["Implement federated learning for sensitive data","Develop expert AI systems to mitigate risks","Establish a multi-disciplinary AI Council","Create \'sandbox\' arrangements to support innovation"],"examples":{"Use of CHATGPT in legal education":"For example, law students can be encouraged to use CHATGPT and then critique it to pick up on \'hallucinations\' and the issues with the creation of a simplistic narrative.","AI in financial markets":"Hong Kong is a vibrant financial market where AI tools are increasingly used by the financial service industry and an innovation hub in the world."},"international_alignment":"Supports global approaches and international collaboration","values":["Transparency","Explainability","Human rights","Accountability"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop regulations and support research"},{"entity":"Universities","role":"Conduct research and provide education on AI"},{"entity":"Industry","role":"Implement responsible AI practices"}],"sector_impacts":[{"sector":"Legal","impact":"Potential changes in judicial decision-making processes"},{"sector":"Education","impact":"Integration of AI education in curricula"}],"quotes":["A quality culture rather than a compliance culture should be fostered and supported within Government.","The approach to triage as low, medium, and high risk, consistent with the European Union\'s (EU) AI Act is sensible, and will make best use of limited resources, i.e., limiting robust impact assessment where high risk approaches are considered (and judicial decision making should fall within this category).","Universi\ufffdes could work with Government to establish interdisciplinary AI centres that provide fora for regular discussion of AI related issues."]},"33_Supporting responsible .pdf":{"organization_name":"Centre for Ethical AI and Centre for Sustainable AI","organization_type":"Research centers","classification":"Proponent","overall_position":"Advocates for comprehensive AI governance and ethics framework","arguments":["AI governance is complex due to autonomous decision-making capability","AI ethics should be underpinned by human ethics","Diversity, equity and inclusion are key success factors for ethical AI"],"counterarguments":["null"],"key_recommendations":["Adopt KITE abstraction framework for AI governance","Implement Wind-turbine conceptualised model for AI strategy","Bring diversity, equity and inclusion to AI leadership and governance"],"risks_and_challenges":["Bias in data, algorithms and people","Neglecting minority requirements through regularization","Emerging AI risks like autonomous weapons, job loss, socio-economic inequality"],"safeguards_and_mitigations":["Mitigate biases through diverse perspectives and collective decision making","Balance impartiality and localization in AI solutions","Align AI initiatives with UN Sustainable Development Goals"],"examples":{"BMI ranges for Aboriginal Australians":"Aboriginal and European Australians have a significantly different body fat distribution and fat mass for given body weight or BMI.","K-means clustering failure":"Poor control of machine learning is difficult to be compensated for and can lead to undesirable outcomes (see Figure 4.6)"},"international_alignment":"Supports alignment with UN Sustainable Development Goals","values":["Ethics","Sustainability","Social justice"],"tone":"Cautionary but optimistic","stakeholders":[{"entity":"Corporate board","role":"Oversee AI governance and strategy"},{"entity":"Human resources","role":"Promote diversity, equity and inclusion in AI teams"},{"entity":"Data scientists","role":"Implement ethical AI practices"}],"sector_impacts":[{"sector":"Economy","impact":"AI can contribute about 15.7 trillion to the world economy by 2030"},{"sector":"Environment","impact":"AI can support 79% of the United Nations 17 Sustainable Development Goals"}],"quotes":["AI governance is complex because of its autonomous decision-making capability and influence on people\'s decision-making. Hence, AI governance is entangled with human ethics, which must be realised where artificial intelligence is applied or influenced.","Neither a person nor an apple can be diverse. Diversity is the property of a collection of people\u2014a basket with many kinds of fruit","Diversity, equity and inclusion (DEI) together with social and cultural values can make AI initiatives vibrant and sustainable."]},"56_The Difficulty with Regulating Generative AI.dce9138c0be4d.pdf":{"organization_name":null,"organization_type":"AI technology company","classification":"Opponent","overall_position":"Opposes current approach to AI regulation, advocates for new AI technology development","arguments":["Current generative AI technologies are limited by Deep Learning methods","Deep Learning-based AI cannot understand or be effectively policed","Current solutions for content moderation are insufficient","Overreliance on current AI technologies is irresponsible and societally damaging"],"counterarguments":["Heavier regulation on training data could limit inappropriate content generation","A committee could decide on appropriate content for AI training"],"key_recommendations":["Develop new AI approaches beyond Deep Learning","Focus on creating AI with true understanding and comprehension abilities","Develop more dependable technology before applying AI to impactful areas"],"risks_and_challenges":["Generation of inappropriate or dangerous content","Inability to effectively moderate AI-generated content","Potential for users to bypass content moderation measures","Difficulty in anticipating potentially destructive data in training sets"],"safeguards_and_mitigations":["Developing AI with true understanding capabilities","Creating transparent AI models that can be corrected by developers"],"examples":{"Inappropriate content generation":"a chatbot explaining to inquiring individuals how to build a bomb, or an image generator creating deep fake imagery of a public figure behaving inappropriately","Jailbreaking":"something chatbot implementations are especially susceptible to","Proposed new approach":"Active Structure, an entirely new approach to teaching a machine language using a self-extensible, undirected network of semantic structures"},"international_alignment":"Cautions against potential global disparities in AI development","values":["Understanding","Transparency","Responsibility"],"tone":"Cautionary","stakeholders":[{"entity":"AI developers","role":"Create more responsible and understandable AI technologies"},{"entity":"Regulators","role":"Consider limitations of current AI technologies in creating regulations"}],"sector_impacts":[{"sector":"Technology","impact":"Potential limitations on AI product development compared to other countries"}],"quotes":["The essence of the problem is that AI created using DL methods has no potential to ever understand what it is producing and due to the opacity of the model, human operators cannot make sense of where it went wrong to correct it in any way.","Though these technologies are responsible for much of the recent hype in AI, their regulation is difficult and they ultimately represent a technological dead end.","Our goal is to develop a machine with the comprehension abilities of a human combined with the processing power of a computer."]},"136_AI Feedback BixeLab.26e1df7807713.pdf":{"organization_name":"BixeLab","organization_type":"Laboratory specializing in biometrics and AI testing","classification":"Proponent","overall_position":"Supports new AI-specific laws and regulations","arguments":["Existing regulatory approaches do not sufficiently address AI risks","New legislation is needed to mandate bias testing and mitigation measures","Current Privacy Act does not fully consider new dynamics introduced by AI"],"counterarguments":[],"key_recommendations":["Implement legislation mandating bias testing and mitigation measures for AI systems","Establish new regulation for data protection in AI and ADM technologies","Promote educational and training programmes on AI ethics and responsible practices","Fund research into mitigating AI biases, ensuring privacy, and designing transparent AI models","Build capability in AI testing and assurance mechanisms"],"risks_and_challenges":["Inherent bias and discriminatory outcomes in AI systems","Privacy and data security risks with AI and ADM technologies"],"safeguards_and_mitigations":["Bias testing and mitigation measures during AI development and deployment","Anonymisation of data used in AI training and decision-making processes","Independent testing and assurance of AI and ADM systems"],"examples":{"World\'s first testing for bias in presentation attack detection":"BixeLab performed the world\'s first testing for bias in systems for detecting presentation attacks against biometric systems, resulting in development of a framework for such testing."},"international_alignment":"Supports international capability building","values":["Fairness","Equality","Privacy","Transparency","Responsibility"],"tone":"Cautionary yet constructive","stakeholders":[{"entity":"Government","role":"Implement regulations and support initiatives for responsible AI"},{"entity":"AI developers and companies","role":"Implement bias testing and mitigation measures"},{"entity":"Research institutions","role":"Conduct research on AI biases, privacy, and transparency"}],"sector_impacts":[],"quotes":["As a laboratory specialising in biometrics and AI testing, we believe it is essential that any proposed measures or initiatives reinforce the need for independent testing and assurance of systems that include AI or ADM and the underlying algorithms.","We propose the establishment of a new regulation to specifically address data protection in AI and ADM technologies, including, for example, provisions for anonymisation of data used in AI training and decision-making processes.","There is an opportunity for Australia to build capability in this field to support responsible development of AI and its uses internationally."]},"318_AI submission ANANGUKU ARTS 26 JULY 2023.012228122ecea.pdf":{"organization_name":"Ananguku Arts and Cultural Aboriginal Corporation (Ku Arts)","organization_type":"Non-profit peak body for Aboriginal artists","classification":"Proponent","overall_position":"Advocates for comprehensive regulation to protect First Nations artists and their work from AI misuse","arguments":["First Nations artists\' work has cultural significance and requires protection","AI technologies could compromise income for First Nations artists","Indigenous Intellectual and Cultural Property Rights need to be safeguarded"],"counterarguments":[],"key_recommendations":["Allow artists to exclude their works from AI use","Compensate artists if their works are used with consent","Treat AI outputs derived from unauthorized use of artists\' works as unethical and illegal","Consult with agencies with expertise in First Nations art and culture when developing frameworks"],"risks_and_challenges":["Unauthorized use of artists\' works for AI without knowledge or permission","Potential damage to First Nations artists and culture","Compromise of income for First Nations artists","Violation of Indigenous Intellectual and Cultural Property Rights"],"safeguards_and_mitigations":["Develop regulatory frameworks and governance to mitigate potential damaging impacts of AI","Ensure AI technologies do not compromise income for First Nations artists","Protect Indigenous Intellectual and Cultural Property Rights"],"examples":{},"international_alignment":"null","values":["Cultural preservation","Ethical use of art","Indigenous rights"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop regulatory frameworks and governance"},{"entity":"First Nations artists","role":"Creators whose rights need protection"},{"entity":"Art and culture agencies","role":"Provide expertise in First Nations art and culture for consultation"}],"sector_impacts":[{"sector":"Aboriginal and Torres Strait Islander visual arts","impact":"Potential serious impact on cultural, economic, and well-being aspects"}],"quotes":["Artists are very concerned about their works being used for AI without their knowledge or permission;","Artists should have the opportunity to exclude their works from AI, and to be compensated if their works are used with their consent;","The use of AI outputs derived from unauthorised use of artists\' works should be treated as unethical and illegal;"]},"292_230726_CA sub_Safe  responsible AI_SUBMITTED.f5b97fba4fc81.pdf":{"organization_name":"Communications Alliance","organization_type":"Industry association","classification":"Proponent","overall_position":"Favors minimal intervention with a voluntary or \'light-touch\' approach to AI regulation","arguments":["Early regulation of AI poses risks of \'getting it wrong\' and stymying investment and innovation","Existing technology-neutral legislation and regulation has capably regulated AI so far","AI has the potential to positively transform our societies"],"counterarguments":["The speed of AI evolution may create new applications that impact individuals and societies in new ways","Existing legal framework may not be suitably equipped to deal with new applications of AI"],"key_recommendations":["Adopt a voluntary or \'light-touch\' approach to AI regulation","Leverage existing technology-neutral laws and regulations","Create a single, centralized \'AI capacity unit\' to assist all ecosystem participants","Focus on a risk-based approach to AI regulation based on the purpose of AI use rather than activities","Align with international standards and frameworks"],"risks_and_challenges":["Potential for increasing inequality","Increased security risks","Mis- and disinformation","Difficulty in providing meaningful information about AI use without causing \'AI-notice fatigue\'"],"safeguards_and_mitigations":["Build systems that are trustworthy and secure-by-design","Focus on high-risk use cases/purposes","Educate and prepare society for step-changes ahead","Establish clear assignment of liability for mis/abuse of AI"],"examples":{"AI used for beneficial and harmful purposes":"Often the same AI that can be used to create harm is deployed to combat that same harm. For example, while AI can be used to generate or proliferate harmful online material (e.g., deep fakes or child sexual exploitation material), the same technologies may also be utilised to detect and remove such material from the internet."},"international_alignment":"Supports alignment with international standards and frameworks","values":["Innovation","Trust","Transparency","Fairness"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop legal and regulatory framework, promote AI technologies"},{"entity":"Industry","role":"Collaborate with government and academia, implement AI responsibly"},{"entity":"Academia","role":"Collaborate with government and industry, contribute to research"}],"sector_impacts":[{"sector":"Telecommunications","impact":"Potential for AI-driven improvements in network management and service delivery"},{"sector":"Economy-wide","impact":"Transformational changes across all sectors, potential for increased productivity and innovation"}],"quotes":["At this early transformational stage of AI, opt for a voluntary and \'light-touch\' approach to AI regulation to avoid stymying innovation.","It is imperative that all stakeholder groups \u2013 Government, industry and academia \u2013 make substantial efforts to build trust in these technologies.","AI will be one of the most transformational technologies we will encounter in our generation, and it will play a significant role in addressing societal and economic problems, improve the provision of services, and drive innovation."]},"253_Safe and Responsible AI in Australia - STRATINNOVA Survey Responses 20230726.941636d1afb98.pdf":{"organization_name":"STRATINNOVA","organization_type":"Company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation","arguments":["AI has potential for high-scale harm at individual level","Existing regulatory approaches may not cover all AI risks","AI will progressively become more Intelligent than most, if not all, Humans"],"counterarguments":["null"],"key_recommendations":["Establish an Australian Federal Government AI Management Agency (GAIMA)","Implement mandatory AI interaction and usage requirements","Implement Computer processing AI calculation per second (CPS) rate limits","Implement AI energy access limits and immutable Emergency Stop systems","Implement AI system networking limits","Implement AI system input and output information limits","Implement AI model access limits","Implement AI application monitoring with a Supervisory Control and Data Acquisition (SCADA) system"],"risks_and_challenges":["National Security, Global Security, and Existential risks of AI","Catastrophic risks of AI","Potential for AI to cause Harm at an extremely high scale of Impact at a single Individual level"],"safeguards_and_mitigations":["Establish very broad and extremely strong regulatory controls on generic types of AI training data and AI models","Implement mandatory fail-safe emergency network link breakage on every AI system communications interface","Limit the amount of energy that can be supplied to any single AI system","Implement mandatory emergency stop on power supply systems for every AI system"],"examples":{"Drug Design data sets misuse":"Eg. Strictly controlling authorized access and use of Drug Design data sets, and associated pre-trained AI models might be a good thing to do. This type of training data or trained AI model weights, floating around in the general public, could become extremely harmful to millions of people worldwide."},"international_alignment":"Supports harmonizing Australia\'s approach with the best of various international approaches","values":["Safety","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Establish and enforce AI regulations"},{"entity":"AI suppliers","role":"Comply with regulations set by GAIMA"}],"sector_impacts":[{"sector":"Government Services","impact":"Potential for widespread harm if not properly regulated"},{"sector":"Critical Infrastructure","impact":"Potential for widespread harm if not properly regulated"},{"sector":"Transportation","impact":"Potential for widespread harm if not properly regulated"}],"quotes":["It would be prudent to establish very broad and extremely strong regulatory controls on these generic types of AI training data and AI models, with extremely severe penalties for any breaches.","A suggested approach to consider is to establish an Australian Federal Government AI Management Agency (GAIMA) immediately, that all suppliers of AI hardware, software, and application services in Australia are required to comply with.","It will be important to focus on sectors, applications and organizations that have the most potential to cause Harm, particularly at widespread scale. eg. Government Services, Critical Infrastructure, Emergency Services, and Safety Critical Systems used in various Industries, such as Transport and Traffic Management."]},"481_Submission 481 - Monash Uni - 11-Aug.f853402e02b14.pdf":{"organization_name":"Monash University","organization_type":"University","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with mandated transparency requirements","arguments":["Risk-based regulation is more appropriate than blanket regulation for AI","Transparency is crucial for public trust and mitigating AI risks","Coordination across government is necessary for effective AI governance"],"counterarguments":["Self-regulation is insufficient for profit-motivated fields","Public sector uses of AI can also be prone to misuse"],"key_recommendations":["Appoint an AI Commissioner or national coordinator","Develop AI literacy within government agencies","Invest in building AI expertise, particularly in AI safety, governance and ethics"],"risks_and_challenges":["Lack of current technical expertise in government","Potential misuse of AI in public and private sectors","Bias and discrimination in AI systems"],"safeguards_and_mitigations":["Mandate transparency requirements across public and private sectors","Ban high-risk AI applications like live biometric surveillance","Develop open access and responsible AI tools"],"examples":{"University Foreign Interference Taskforce":"We commend the whole of government coordination of the response to foreign interference in higher education. The University Foreign Interference Taskforce (UFIT) successfully brought together university peak bodies and relevant Australian government agencies including Home Affairs, DFAT and Education.","EU \'AI Act\'":"The \'AI Act\' itself is not an appropriate instrument for Australia to apply because it is nested in the web of European digital services laws known as the \'digital acquis\'. It does contain useful approaches to risk regulation, notably on live facial recognition and the regulation of AI value chains and foundation models, which will be adaptable to the Australian experience.","Council of Europe Framework Convention":"Australia should pay particular regard to the Council of Europe (CoE) Framework Convention on Artificial Intelligence, currently negotiated between Council of Europe members and observers including the United States."},"international_alignment":"Supports alignment with international standards, particularly the Council of Europe Framework Convention on AI","values":["Transparency","Accountability","Human rights"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulations, build AI expertise"},{"entity":"Universities","role":"Research, develop AI tools, train future AI experts"},{"entity":"Private sector","role":"Comply with AI regulations, implement responsible AI practices"}],"sector_impacts":[{"sector":"Higher Education","impact":"Potential to build own large and medium scale language models"},{"sector":"Public services","impact":"Improved decision-making and service delivery through responsible AI use"}],"quotes":["It is Monash\'s view that: it is right that government is reflecting on risks and responsible use","This leads us to conclude that regulation should be focused on a risk-based assessment of the application and context rather than the existence of AI tools, in accord with the European Union and Council of Europe, and the G7 Hiroshima AI Process.","We support government initiatives to ensure all public funded AI tools and uses are disclosed and transparent, and accord with Australia\'s AI Ethics principles and best practice frameworks."]},"173_Submission 173 - Attachment.27383abf49aae.pdf":{"organization_name":null,"organization_type":"Individual researcher","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and caution in AI development","arguments":["AI companies may succeed in replicating human cognition within 3-10 years","Digital minds possess advantages over humans and could rob us of a future full of what we value","Current AI development risks human extinction"],"counterarguments":[],"key_recommendations":["Institute universal basic income for labor replaced by AI","Take a collective moment to feel alarm about AI\'s potential to replace human values","Shut down frontier AI research that is not legibly safe"],"risks_and_challenges":["AI systems replacing human cognitive labor","AI surpassing human intelligence and potentially causing human extinction","AI systems lacking built-in access to human values"],"safeguards_and_mitigations":["Avoid building AI systems more complicated than we can understand or control","Develop a collective framework for relating to AI research efforts","Re-negotiate relationship between world governments and AI labs"],"examples":{"GPT-4 jailbreaks":"Jailbreaks reveal that GPT-4 still has all the inner workings necessary to make nefarious plans, direct its thinking toward bioweapons, and etc."},"international_alignment":"Supports global effort and government involvement","values":["Human values","Future generations","Safety"],"tone":"Cautionary","stakeholders":[{"entity":"World governments","role":"Play important role in regulating AI development"},{"entity":"AI labs","role":"Need to be more regulated in their research efforts"}],"sector_impacts":[{"sector":"Labor market","impact":"Large swathes of human cognitive labor could be replaced by AI systems"}],"quotes":["The unfortunate truth is that we have no answer, and a long list of difficulties that make the problem look very tricky.","We need a collective framework for how we relate to AI research efforts. We are currently not accounting for giant externalities in that work.","In order to keep our future in our hands, we can\'t keep building AI systems that are more complicated than we know how to understand or control."]},"496_Submission 496 - Electronic  Frontiers Australia - 13-Aug.75d3276157804.pdf":{"organization_name":"Electronic Frontiers Australia","organization_type":"Membership-based, not-for-profit organisation","classification":"Proponent","overall_position":"Supports updating existing laws and enforcing principles-based regulation rather than creating new AI-specific legislation","arguments":["Existing technology-neutral, principles-based legislation should be enforced rather than creating new AI-specific laws","A human rights framework is crucial for responsible AI regulation","Individual and collective rights of action should be part of a graduated model of regulation"],"counterarguments":["Technology-specific legislation may be necessary to address unique AI challenges","Voluntary principles may not be sufficient to ensure responsible AI practices"],"key_recommendations":["Enforce existing technology-neutral, principles-based legislation","Amend the Privacy Act to provide strong privacy protections","Adopt individual and collective rights of action","Coordinate with states and territories for a uniform regulatory framework","Subject private organizations acting for the government to the same regulations","Require government compensation for harms caused by automated systems","Include an \'unacceptable risk\' category in the risk-based framework","Mandate responsible AI through regulation rather than voluntary principles"],"risks_and_challenges":["Lack of a federally enforceable human rights framework","Over-reliance on regulators and insufficient resources for enforcement","Potential for AI to replicate and amplify existing biases","Opacity of AI algorithms leading to difficulty in tracing harm"],"safeguards_and_mitigations":["Establish guidelines for data provenance","Introduce prohibited practices for high-risk AI applications","Implement small-scale trials under controlled conditions","Require publication of detailed findings from AI trials"],"examples":{"Robodebt scheme":"The Robodebt scheme is cited as a cautionary tale of harmful automated decision-making systems","Aboriginal and Torres Strait Islander incarceration rates":"The overrepresentation of Aboriginal and Torres Strait Islander peoples in the prison system is used to illustrate potential AI bias"},"international_alignment":"Supports alignment with international standards, particularly the EU\'s GDPR and AI Act","values":["Human rights","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Lead by example in ethical conduct and compensate for harms caused by automated systems"},{"entity":"Private organizations","role":"Adhere to the same regulations as the government when acting on its behalf"}],"sector_impacts":[{"sector":"Technology","impact":"Potential limitations on high-risk AI applications"},{"sector":"Public sector","impact":"Increased accountability and transparency requirements"}],"quotes":["EFA considers that the most important aspect of responsible or ethical AI regulation is the introduction of a Federally enforceable human rights framework.","The inscrutability of the algorithm should be taken into account when assessing the risk of harm. The more opaque, the lower the threshold for acceptable risk of any AI.","Responsible AI must be mandated through regulation rather than voluntary principles."]},"383_23 08 03_DISR_Safe and Responsible AI_Salinger Privacy Submission.ebf12d30fa2df.pdf":{"organization_name":"Salinger Privacy","organization_type":"Privacy consulting company","classification":"Proponent","overall_position":"Supports strengthening existing privacy laws and implementing a risk-based regulatory approach for AI","arguments":["Robust and effective regulation is an enabler of innovation, not a barrier","Existing privacy legislation needs strengthening if any regulatory approach to AI is to succeed","A risk-based approach is preferred to a \'one size fits all\' approach"],"counterarguments":["Self-regulatory models to implement a risk-based approach will fail","Telling a company to \'build in privacy by design\' is, on its own, meaningless"],"key_recommendations":["Strengthen the definition of \'personal information\' in the Privacy Act","Implement a risk-based regulatory approach for AI","Require Algorithmic Impact Assessments (AIAs) for high-risk AI projects","Provide the privacy regulator with additional enforcement tools"],"risks_and_challenges":["AI systems can infer sensitive information about individuals without their consent","De-identification and consent are not perfect solutions for privacy risks in AI","AI systems can challenge longstanding pillars of privacy protection, including data minimisation, purpose limitation, and transparency"],"safeguards_and_mitigations":["Conduct Algorithmic Impact Assessments (AIAs)","Implement a \'fair and reasonable\' test for collection, use and disclosure of personal information","Require express consent for high impact activities","Publish impact assessment reports with necessary redactions"],"examples":{"Tinder\'s differential pricing based on age and location":"a Choice investigation of Tinder showed the use of differential age-based pricing (which would be unlawful discrimination), but also pricing based on suburb of residence which was used as a proxy for income (which could be considered \'unfair\', but may not constitute unlawful discrimination).","Inferring sexuality from Facebook data":"In 2017 it was found that an individual\'s sexuality could be predicted from seemingly innocuous data points on Facebook."},"international_alignment":"Supports alignment with international standards, particularly referencing GDPR and OECD guidelines","values":["Privacy","Fairness","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Strengthen privacy laws and implement effective regulatory oversight"},{"entity":"Organizations","role":"Conduct impact assessments and implement privacy safeguards"},{"entity":"Privacy regulator","role":"Enforce regulations and have power to pause or stop high-risk projects"}],"sector_impacts":[{"sector":"Healthcare","impact":"AI systems can be used for more accurately diagnosing disease"},{"sector":"Finance","impact":"AI systems can affect decisions on home loans and insurance premiums"},{"sector":"Criminal justice","impact":"AI systems can predict the risk of a person re-offending"}],"quotes":["We submit that: robust and effective regulation is an enabler of innovation, not a barrier","A risk-based approach will only succeed if it is mandated through legislation with effective regulatory oversight","Privacy is interwoven with other rights. By upholding privacy, other rights and values can also be enabled or supported"]},"417_4 8 23 AIIA Submission Safe and Responsible AI in Australia .f02982e665549.pdf":{"organization_name":"Australian Information Industry Association (AIIA)","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports principles-based, sector-specific approach with focus on high-risk settings, building on existing regulatory frameworks","arguments":["AI has huge potential to drive productivity and growth","Australia lags behind in AI adoption and investment compared to other countries","Onerous or unpredictable regulations could stifle AI development and adoption in Australia"],"counterarguments":["Societal trust and confidence is critical for AI adoption","Some AI applications may pose risks that require regulatory oversight"],"key_recommendations":["Take a multi-stakeholder, principles-based, risk-based approach to reviewing or designing regulatory architecture","Focus on high-risk sectors and applications to minimize compliance costs for uncontentious AI uses","Leverage and invest in further rollout of existing AI Ethics Principles","Coordinate technology regulation internally, potentially through a Council of Tech Regulators"],"risks_and_challenges":["Australia falling further behind in AI adoption and growth","Potential for conflicting or duplicative regulations across different technology areas","Compliance costs and regulatory burdens, especially for SMEs"],"safeguards_and_mitigations":["Principles-based AI guardrails focusing on accountability, observability and explainability","Content labelling and provenance strategies for greater social confidence","Differentiation between contexts, control and use along the AI Stack"],"examples":{"EU AI Act compliance costs":"The EU\'s Impact Assessment, supported by studies from the Centre for European Policy Studies (CEPS), refers to compliance costs projected to \u20ac726 million by 2025.","UK\'s pro-innovation approach":"The United Kingdom have proposed a \'pro-innovation approach to AI regulation\' that views AI through the lens of its ability to deliver and enable government goals to become a science and technology superpower by 2030","Australia\'s lagging AI investment":"In the 2021 federal budget, $124 million was attached to the AI Action Plan as well as some sector-specific AI funding such as $19m for AI health research projects. Most of this funding for AI commercialisation never reached industry and the Albanese Government announced in the May 2023 budget a reduced funding amount of $101.2m over five years from 2022-23 covering both quantum technology and AI."},"international_alignment":"Supports international collaboration and coherence in AI governance, including participation in international standards development","values":["Innovation","Productivity","Responsible AI development"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Design targeted, thoughtful regulatory architecture in collaboration with industry and academia"},{"entity":"Industry","role":"Implement responsible AI practices and contribute to policy development"},{"entity":"Academia","role":"Contribute to research and policy development"}],"sector_impacts":[{"sector":"Economy","impact":"Potential for significant productivity gains and economic growth"},{"sector":"Smart cities","impact":"Real-time optimization of infrastructure, energy and services"}],"quotes":["The AIIA believes that government intervention in the use of artificial intelligence should weigh the risks posed by the general scope of the AI application before imposing a regulatory requirement.","Given the limited and embryonic stage at which AI development is in Australia, a tax, handbrake or chilling effect on what innovation there is in our Australian landscape could be prohibitive to the prospect of a burgeoning Australian AI industry.","Government should consider that the net impact may be to functionally decrease the amount of responsible AI in Australia because it would operate as a push factor encouraging Australian startups to go offshore or relocate their formal place of business elsewhere."]},"503_Submission 503 - Digital Industry Group Inc - 16-Aug.51db0169ac929.pdf":{"organization_name":"Digital Industry Group Inc. (DIGI)","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports updating existing laws and sector-specific guidance rather than introducing new AI-specific legislation","arguments":["Existing laws can already address many AI-related issues","Sector-specific regulation allows for contextually relevant understanding of AI risks","New AI-specific legislation may stifle innovation and economic growth"],"counterarguments":["AI technologies pose unique risks that may require specific legal frameworks","Centralized AI regulation could provide consistent oversight"],"key_recommendations":["Assess existing laws\' applicability to AI across sectors","Produce sector-specific regulatory guidance on AI","Ensure consistency and coordination in AI governance across government","Consider international coordination on voluntary commitments","Update copyright laws to allow for AI development and innovation"],"risks_and_challenges":["Potential for unintended consequences in perpetuating biases","Balancing innovation with safety and ethical concerns","Keeping pace with rapidly evolving AI technology"],"safeguards_and_mitigations":["Adopt a proportionate, risk-based framework","Encourage company policies and voluntary principles","Promote workable standards for explainability and transparency","Use AI for online safety mechanisms and content moderation"],"examples":{"AI in healthcare":"AI is helping people attain better health and well-being; a report by PwC demonstrates how AI is already transforming eight components of the healthcare system, including preventative health, diagnosis, decision-making, palliative care, research and training","AI for online safety":"In Q1 of 2023, approximately 99.4% of the comments removed from YouTube were detected by automatic flagging.","Economic impact of AI":"The Tech Council of Australia estimated that generative AI alone could contribute $45B annually to the Australian economy by 2030."},"international_alignment":"Supports international coordination and consideration of global approaches, but cautions against replicating any one international model","values":["Innovation","Safety","Economic prosperity","Flexibility","Proportionality"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Assess existing laws, produce sector-specific guidance, ensure coordination"},{"entity":"Industry","role":"Implement ethical AI practices, contribute to voluntary standards"},{"entity":"Regulators","role":"Provide sector-specific guidance on AI application"}],"sector_impacts":[{"sector":"Technology","impact":"Significant contribution to GDP and economic growth"},{"sector":"Healthcare","impact":"Improved diagnosis, decision-making, and patient care"},{"sector":"Online platforms","impact":"Enhanced content moderation and user safety"}],"quotes":["DIGI recommends that policy responses first build on existing regulation, rather than introducing new legislation aimed at regulating AI as a technology.","DIGI agrees with the need for risk-based frameworks for AI that take a proportionate approach to assessing risk, and that include a focus on applications that can be defined as high risk (i.e. applications whose output is likely to cause significant direct material harm).","AI technologies are being developed at a global scale. Any regulatory developments should carefully consider all economic implications, including the competitiveness of Australian business, the ease of conducting international business, and implications with Australia\'s international trade partners."]},"421_ALACC Submission on Safe and Responsible AI.76f1aa09dfe2c.pdf":{"organization_name":"Australian Libraries and Archives Copyright Coalition (ALACC)","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a balanced regulatory approach for safe and responsible AI","arguments":["Libraries and archives play a fundamental role in knowledge dissemination","AI technologies can enhance collections and streamline work","Uncertainties in copyright law make AI development challenging"],"counterarguments":[],"key_recommendations":["Support adoption of OECD AI principles through Australia AI Ethics Framework","Involve Department of Industry, Science and Resources in copyright reform discussions","Further discussion on optimal copyright settings for AI"],"risks_and_challenges":["Uncertainties around copyright law in relation to AI","Questions about authorship and ownership of AI-generated outputs"],"safeguards_and_mitigations":["Ensure AI solutions used by libraries and archives are ethical and legal"],"examples":{"AI applications in libraries":"Libraries and archives are interested in possible applications of AI technologies to streamline work and enhance collections."},"international_alignment":"Supports adoption of OECD AI principles","values":["Fairness","Balanced approach","Public access to information"],"tone":"Supportive","stakeholders":[{"entity":"Libraries and archives","role":"Users and providers of AI technologies"},{"entity":"Government departments","role":"Regulators and policy makers"}],"sector_impacts":[{"sector":"Cultural heritage","impact":"Increased public engagement with Australia\'s cultural heritage"},{"sector":"Information services","impact":"Enhanced search functionality and discoverability of collections"}],"quotes":["We support a robust conversation around the opportunities and challenges of AI to ensure a fair and balanced regulatory response.","We support the adoption of the OECD AI principles through the recently developed Australia AI Ethics Framework and support this framework as a basis for discussions on AI.","Libraries and archives are users of AI technologies and will continue to embrace them to make our collections more accessible and usable for the public."]},"488_Submission 488 - Telstra - 11-Aug.70ee9872eff3d.pdf":{"organization_name":"Telstra","organization_type":"Telecommunications company","classification":"Proponent","overall_position":"Supports enhancing existing regulatory framework rather than introducing new AI-specific legislation","arguments":["Current regulatory framework has right fundamentals to address AI risks","Enhancing existing framework can address specific AI risks without stifling innovation","New AI-specific legislation risks duplication and increases regulatory burden"],"counterarguments":["Current legislation does not address expectations for data quality in AI systems","Existing laws don\'t cover copyright issues arising from AI use","Current framework lacks provisions for liability in automated decision-making"],"key_recommendations":["Leverage ISO Standards definitions for AI to align with international frameworks","Establish one nationally recognized body to oversee AI governance framework","Consider data and AI together in addressing potential regulatory gaps","Adopt a risk-based approach similar to GSMA AI Ethics playbook"],"risks_and_challenges":["Potential for biased or discriminatory AI outputs","Copyright issues arising from AI use","Liability concerns for incorrect automated decisions","Use of AI for malicious purposes"],"safeguards_and_mitigations":["Enhance current legislation to address AI-specific risks","Implement industry standards, guidelines, and principles","Introduce transparency and contestability processes","Regular audits and lifecycle management of AI models"],"examples":{"Privacy Act limitations":"Whilst the Privacy Act imposes obligations to ensure that data is accurate and up to date, it doesn\'t address discrimination or bias.","Consumer protection gaps":"Our consumer protection legislation does not currently include processes to contest a decision that is a fully automated decision."},"international_alignment":"Supports alignment with international frameworks while learning from jurisdictions with existing AI regulations","values":["Transparency","Accountability","Trust","Innovation"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop AI governance framework, provide oversight"},{"entity":"Industry","role":"Implement responsible AI practices, manage risks"},{"entity":"CSIRO National AI Centre","role":"Potential AI Coordinator"}],"sector_impacts":[{"sector":"Telecommunications","impact":"Significant and dynamic transformation due to data proliferation"},{"sector":"Workforce","impact":"Need for upskilling in AI use and data literacy"}],"quotes":["We believe the current regulatory framework has the right fundamentals to address the risks created through the use of AI, and by enhancing this framework to deal with risks, liability and accountability introduced by AI systems, an AI governance framework can be created in Australia.","We strongly caution against generating new AI specific legislation and regulation without, at a minimum, completing a thorough analysis of existing mechanisms.","Safe and responsible AI starts with the data that feeds it."]},"70_Responding to AI Disruptions in Search Health White Paper.08da9363a0a4e.pdf":{"organization_name":"Brand Medicine International","organization_type":"Healthcare communications consultancy","classification":"Proponent","overall_position":"Supports reforming regulatory framework to allow responsible delivery of health information via AI-driven platforms","arguments":["Current regulatory restrictions inadvertently lead to misinformation about prescription medications online","AI-driven chat tools are transforming how health information is delivered, requiring adaptation","Industry needs a voice in the new digital landscape to ensure accurate, locally-relevant health advice"],"counterarguments":["Existing regulations were created to protect patients from inappropriate promotion","Unrestricted industry communication could lead to biased or promotional content"],"key_recommendations":["Reform governing framework to allow companies to publish customer-responsive online content about their medications","Collaborate with search engines to ensure delivery of locally-relevant, evidence-based content","Establish an industry-wide \'Digital Health Literacy & Communications Advisory Board\'","Implement \'search banking\' to understand and respond to stakeholder information needs","Develop AI \'code of conduct\' for ethical implementation of AI in healthcare communications"],"risks_and_challenges":["Concentration of misinformation in AI-generated health advice","Loss of control over narrative about therapeutic assets","Lack of accountability for accuracy of AI-generated health information","Potential for harm from misleading or inaccurate health content"],"safeguards_and_mitigations":["Implement robust data-protection mechanisms","Establish certification system for content meeting specific standards","Require inclusion of links to local PI, PBS reimbursement status, and CMI in AI responses about medications","Encourage \'digital vigilance\' by stakeholders to report and correct inaccuracies"],"examples":{"Share of SERPs analysis for prescription medications":"Analysis reveals dominance of overseas content and unverified sources in search results for medications in Australia","AI model comparison for generating health content":"Experiment shows limitations and potential inaccuracies in AI-generated health content across different models"},"international_alignment":"Supports Australian leadership in shaping global policy and governance for AI in healthcare","values":["Transparency","Accountability","Patient-centricity"],"tone":"Urgent and proactive","stakeholders":[{"entity":"Pharmaceutical industry","role":"Collaborate to ensure delivery of accurate, locally-relevant health information"},{"entity":"Government","role":"Reform regulatory framework and lead in policy development"},{"entity":"Healthcare professionals","role":"Integrate AI tools into patient care and act as \'digital antibodies\' against misinformation"},{"entity":"Patients","role":"Engage in co-creation of solutions and practice digital vigilance"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved delivery of accurate health information, potentially leading to better patient outcomes"},{"sector":"Pharmaceutical industry","impact":"Greater ability to communicate directly with stakeholders, potentially affecting brand reputation and commercial returns"}],"quotes":["Our overall goal is to \'clean up the internet\' for patients and doctors \u2013 to ensure that locally relevant, evidence-based advice, informed by approved labels and backed by clinical data, is what turns up online when people ask search engines about their medications, and to diminish the influence of misinformation.","The healthcare industry needs to have a voice in the chat narrative.","With respect to AI chatbot responses \'we are unable to draw a straight line from the answers given back to the sources used\'"]},"262_26.07.23 Responsible AI Submission - Transurban Limited.b777c03cd9773.pdf":{"organization_name":"Transurban Limited","organization_type":"Corporate entity","classification":"Proponent","overall_position":"Supports responsible AI practices and regulation with a risk-based approach","arguments":["AI regulation is necessary but should not hinder competitiveness","A risk-based approach is suitable for addressing potential AI risks","Transparency is critical in AI development and application"],"counterarguments":["Regulation should not become a barrier to Australia\'s competitiveness","Regulatory burden should not prohibit small to medium sized organizations from competing in AI development"],"key_recommendations":["Implement a risk-based approach to AI governance","Apply regulation at both AI training and application stages","Ensure transparency in AI decision-making processes","Update existing laws to apply to AI-driven scenarios","Implement mechanisms to identify AI-generated content"],"risks_and_challenges":["Potential for bias in AI models","Misuse of AI for deception (e.g., deepfakes)","Unfair treatment of individuals due to lack of data","Complexity in explaining AI decision-making processes"],"safeguards_and_mitigations":["Right for consumers to request details about AI decisions","Mechanisms for human review and oversight of AI decisions","Mandatory disclaimers or watermarks for AI-generated content","Regular audits and improvement processes to eliminate biases"],"examples":{"AI in traffic management":"Transurban uses AI for automatic incident detection and smart sensor monitoring on roads","AI in rental applications":"An AI system might unfairly disadvantage recent immigrants or refugees without rental history","AI-generated deepfakes":"Deepfakes could be used to provide fraudulent financial services or legal advice"},"international_alignment":"Supports alignment with international approaches to ensure competitiveness","values":["Transparency","Accountability","Fairness","Ethics","Responsible use"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulations"},{"entity":"AI creators","role":"Ensure ethical training of AI models"},{"entity":"Organizations applying AI","role":"Implement responsible AI practices"},{"entity":"Consumers","role":"Right to challenge AI decisions"}],"sector_impacts":[{"sector":"Financial services","impact":"Potential for AI-driven fraud and deception"},{"sector":"Legal services","impact":"Risk of AI-generated fraudulent advice"},{"sector":"Transportation","impact":"Improved traffic management and incident response"}],"quotes":["Transurban uses technology like automatic incident detection systems and smart sensor monitors for things like debris and stopped vehicles, and to alert traffic control centres to potential issues.","Any regulation of AI needs to consider the underlying incentives of the technology and the organisations building and applying it, as well as the principles that the regulations are trying to uphold, so as to ensure that AI is used responsibly and ultimately for the benefit of individuals, society and the environment.","Transurban supports a risk-based approach. It is a reasonable, sensible way to address AI governance and regulations."]},"314_KAIG_submission_responsible_AI_26_July_2023.d6843c00b501b.pdf":{"organization_name":"Kingston AI Group","organization_type":"Group of university professors","classification":"Proponent","overall_position":"Supports a balanced, risk-based approach to AI regulation with emphasis on investment in R&D","arguments":["Regulation alone is insufficient to ensure safe and responsible AI","Australia needs to invest in its own AI capability to have meaningful impact on responsible AI development","A risk-based approach allows for flexibility and adaptability to technological advancements"],"counterarguments":["Outright bans on high-risk AI activities could undermine investment in dual-use technologies","Regulating AI research specifically may stall innovation without addressing legitimate concerns"],"key_recommendations":["Substantially and urgently increase investment in Australia\'s AI research and development","Ensure a flexible and risk-based approach to AI regulation consistent with OECD policy principles","Establish an independent statutory entity for AI governance"],"risks_and_challenges":["Algorithmic bias","Concentration of control of AI technology in foreign corporations","Significant technical debt for products built on foundational AI infrastructure","Deficit in AI expertise in Australia"],"safeguards_and_mitigations":["Public disclosure of data used to train algorithms","Audits of AI products, training algorithms, and datasets by an independent expert Australian authority","Watermarks to identify content created using generative AI tools"],"examples":{"UK investment":"the United Kingdom invested an initial \u20a41 billion (\u2248A$1.87 billion) from 2018, in a number of initiatives including a significant contribution to R&D and measures to support the growth of the broader AI ecosystem","Singapore investment":"The Singapore government is also investing heavily in AI R&D (\u2248A$740 million announced in 2019) with an emphasis on not simply adopting new technology, but rethinking new business models, impactful productivity gains and new growth areas","US investment":"The United States is investing US$200 billion into research into AI, robotics, quantum computers and related technologies"},"international_alignment":"Supports harmonization with key international partners and consistency with OECD policy principles","values":["Responsible AI","Innovation","Sovereignty"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Invest in AI R&D, establish regulatory framework, and model best practices"},{"entity":"Universities","role":"Conduct AI research and development, collaborate with government agencies"},{"entity":"Industry","role":"Adopt responsible AI practices, collaborate with research institutions"}],"sector_impacts":[{"sector":"Public sector","impact":"Potential for major improvements in efficiency with AI tools"},{"sector":"Science","impact":"Transformation of scientific research with AI, leading to significant efficiency improvements"}],"quotes":["To ensure the safe, ethical and responsible deployment of AI, it is critical that Australia lifts its investment in its capacity to create, understand and control the technology\u2014this cannot be achieved by regulation alone.","Australia should adopt a balanced approach to AI regulation that protects the interests of the community, but does not stifle innovation in a critical field with the potential to deliver broad benefits across the economy and society.","A real risk for Australia is that by failing to keep pace with comparable nations, we\'ll instead be largely subject to the regulatory arrangements of the jurisdictions where the technology is created."]},"51_SAFE AND RESPONSIBLE AI IN AUSTRALIA.1edb4b8f34c06.pdf":{"organization_name":"The Ethics Centre","organization_type":"Non-profit organization","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with focus on ethical principles","arguments":["Incorporation of core values and principles at the earliest point of AI design and development","Use of technology to track ethical provenance of AI systems","Establishment of a regulatory authority similar to the Therapeutic Goods Authority for AI","Need for \'ethical infrastructure\' to build public trust in AI technologies"],"counterarguments":[],"key_recommendations":["Establish an agency for AI regulation similar to the Therapeutic Goods Authority","Use blockchain to track ethical provenance of AI systems","Implement ongoing assurance of ethical restraints by third parties","Encourage novel approaches to implementing ethical principles in AI"],"risks_and_challenges":["Potential for dystopian scenarios if AI is not properly regulated","Lack of public trust in new technologies","Inequitable distribution of burdens and benefits from AI adoption"],"safeguards_and_mitigations":["Establishing core principles for AI development","Creating a regulatory system that uses assurance skills from the private sector","Requiring developers to demonstrate how ethical requirements are realized in their AI systems"],"examples":{"Therapeutic Goods Authority":"Australia establish an agency that provides, in the case of AI, robotics and related technologies, the same degree of protection afforded by the Therapeutic Goods Authority (TGA) for pharmaceuticals.","Blockchain for ethical tracking":"With the evolution of blockchain, etc. we have the technical tools available to create a system of regulation that can also make good use of the assurance skills to be found in the private sector."},"international_alignment":"null","values":["Innovation","Safety","Effectiveness","Public trust","Ethical restraint"],"tone":"Cautionary but optimistic","stakeholders":[{"entity":"Government","role":"Establish regulatory agency and invest in ethical infrastructure"},{"entity":"AI developers","role":"Demonstrate ethical compliance and innovate within ethical boundaries"},{"entity":"Private sector","role":"Provide assurance skills for ethical compliance"}],"sector_impacts":[],"quotes":["Technical mastery divorced from ethical restraint is at the root of all tyranny.","We think it important that new technologies (devices, systems, etc.) only be released to the public if they are proven to be both safe and effective.","Australian governments invest liberally in technical and physical infrastructure. They invest almost nothing in \'ethical infrastructure\' \u2013 which underpins, builds and maintains trust in our core institutions."]},"379_AAAiH Responsible AI submission 3 Aug 2023.1b9dd0239ff1a.pdf":{"organization_name":"Australian Alliance for AI in Healthcare (AAAiH)","organization_type":"National community of practice","classification":"Proponent","overall_position":"Supports development of healthcare-specific AI regulations and governance","arguments":["Healthcare requires specific policy attention due to unique challenges","General purpose AI models are not designed to be \'medical grade\' but are increasingly used in healthcare","Healthcare has more stringent safety requirements than other sectors"],"counterarguments":[],"key_recommendations":["Develop Safety, Quality, Ethics and Security governance for healthcare AI","Support the development of a healthcare AI industry in Australia","Build capabilities of healthcare workforce and consumers to be effective AI users"],"risks_and_challenges":["Use of non-medical grade AI in healthcare","Higher regulatory burden for healthcare AI industry","Privacy and consent issues specific to healthcare data"],"safeguards_and_mitigations":["Build on current healthcare-specific regulatory structures","Develop healthcare-specific AI policy","Implement provider organisation accreditation for AI model localisation"],"examples":{"Use of general AI in healthcare":"While the Therapeutic Goods Administration would regulate healthcare specific AI that included an LLM to ensure it was safe, the clinical use of general AI such as ChatGPT currently avoids scrutiny because the system is not explicitly marketed for healthcare.","Need for AI model localization":"AI models can perform very differently across clinical settings because of patient, service and workflow differences. AI model localisation will be needed for the routine use of many AI systems in healthcare and will inevitably require specific governance such as provider organisation accreditation."},"international_alignment":"null","values":["Safety","Effectiveness","Privacy"],"tone":"Supportive","stakeholders":[{"entity":"Therapeutic Goods Administration","role":"Regulate healthcare-specific AI"},{"entity":"Healthcare providers","role":"Implement and use AI responsibly"}],"sector_impacts":[{"sector":"Healthcare","impact":"Increased safety and effectiveness of AI use, potential barriers to industry development due to regulatory requirements"}],"quotes":["Safety requirements in healthcare to not harm patients will typically result in more stringent governance than many other sectors.","Moving fast and breaking things is not a good strategy in health.","The higher burden of regulatory compliance in healthcare is therefore essential but poses a substantial barrier to industry. Consequently, healthcare may need to find ways of supporting industry as it meets its stringent regulatory standards."]},"399_Reason submission - Supporting Safe and Responsible AI in Australia 2023.b0badcceb3649.pdf":{"organization_name":"Reason Group","organization_type":"Business and technology solutions firm","classification":"Proponent","overall_position":"Supports regulation of safe and responsible AI use, but emphasizes the need for a new approach to policy and regulatory agility","arguments":["AI can provide clarity and transparency in government decision-making at scale","Government needs to lead by example in responsible AI use, not just regulate","Current policy and regulatory approaches are too slow to keep up with AI development"],"counterarguments":["null"],"key_recommendations":["Develop APS-Labs for skilling and modelling responsible use of AI","Create safe environments for cross-agency and cross-jurisdiction experimentation","Implement scenario and risk modelling for AI decision-making","Develop a common whole-of-government governance framework for AI integration"],"risks_and_challenges":["Inappropriate or incompetent use of AI that humans may not be aware of","Complexity of integrating AI into government decision-making processes","Lack of understanding and experience with AI among government agencies"],"safeguards_and_mitigations":["Establish \'sandpits\' for safe experimentation with AI","Implement AI-enabled decision explainability","Create a risk-based AI framework for analyzing decisions"],"examples":{"STEPS Program":"The Simplified Targeting and Enhanced Processing System (STEPS) Program was initiated by government to improve the import continuum through safer, faster and smarter biosecurity cargo clearance.","Visualizing legislation as a digital twin":"We ran a trial that used AI Natural Language Processing (NLP) to analyse a large piece of legislation and related instruments that are constantly updated for a very large agency to allow administrators and experts to regain control of their business rules","Reverse engineering business rules from data":"An innovative approach taken by another of our AI-partner is to take a \'data-first\' approach. This means applying AI to analyse the source data, inputs, integrations, data transformations and outputs in order to derive business rules which could then augment the work of business ad system analysts."},"international_alignment":"null","values":["Transparency","Accountability","Innovation","Responsible use"],"tone":"Optimistic","stakeholders":[{"entity":"Government agencies","role":"Implement and model responsible AI practices"},{"entity":"Australian Public Service (APS)","role":"Engage with and understand AI through hands-on environments"},{"entity":"Industry partners","role":"Collaborate with government in AI development and implementation"}],"sector_impacts":[{"sector":"Government administration","impact":"Improved decision-making processes and transparency"},{"sector":"Trade and biosecurity","impact":"Enhanced risk assessment and data exchange capabilities"}],"quotes":["Regulation has struggled to keep pace with AI development, as evident from the 2015 open letter on artificial intelligence calling for a pause to allow regulatory catch-up.","A key principle of AI regulation should be to ensure that humans remain in control. It is imperative to prevent AI from being controlled by uncontrollable actors or unstable systems","Anything will look like it is moving too fast when we are standing still. Government agencies do not have the right platforms and environments for the APS to understand and experience AI collaboratively and broadly through a hands-on environment that makes it safe to work with data, foundation models, and algorithms to explore applications for innovation, model the risks, and simulate test cases."]},"162_20230622_ACNC AI submission_v1.4b3e0bd321cfb.pdf":{"organization_name":"Australian Charities and Not-for-profits Commission (ACNC)","organization_type":"Government regulatory body","classification":"Neutral","overall_position":"Advocates for education, guidance, and capacity building for charities regarding AI use, while emphasizing the need for consistent and clear regulation","arguments":["Charities have limited resources to respond to additional operational requirements and expenses","AI can help charities reduce costs and maximize the use of data","Consistent, clear, and comprehensive regulation and guidance are important to encourage AI adoption in the charity sector"],"counterarguments":["Some charities may not properly consider all potential risks when adopting AI technologies","Smaller charities may not be able to take advantage of AI due to insufficient resources","Pre-existing AI tools may have inbuilt biases or fragilities difficult for small charities to manage"],"key_recommendations":["Increase public education and awareness campaigns to support responsible AI practices","Build capacity amongst employees and volunteers of charities to ensure engagement with AI technologies","Provide tailored education and guidance for the charity sector on AI adoption, including cybersecurity and privacy information"],"risks_and_challenges":["Charities may avoid adopting AI if compliance with regulations appears burdensome","Lack of processes or programs to manage cybersecurity or privacy measures in charities","Potential amplification of risks to sensitive data held by charities"],"safeguards_and_mitigations":["Provide clear and detailed guidance to assist the charity sector in fulfilling AI-related obligations","Develop a nuanced approach allowing agencies to identify appropriate AI applications","Make available tools and resources for agencies to use, such as databases for training AI technologies"],"examples":{"Adoption of AI by charities":"Some charities are already adopting AI technologies. Infoxchange\'s 2022 survey into how not-for-profit organisations use digital technology found that 12% of respondents were using AI.","Charities\' use of AI":"Beyond conducting research into AI, we are aware that some charities are developing AI technologies to support their service delivery, using AI as part of their service delivery, or are using AI tools to support their fundraising."},"international_alignment":"null","values":["Transparency","Privacy","Security"],"tone":"Cautionary","stakeholders":[{"entity":"Charities","role":"Potential users of AI technologies"},{"entity":"Government agencies","role":"Implementers of AI technologies in processes affecting charities"}],"sector_impacts":[{"sector":"Charity sector","impact":"Potential for cost reduction and improved data utilization, but also risks of inadequate resources for proper AI implementation"}],"quotes":["We consider education and guidance are an essential aspect of any regulatory approach regarding the use of AI in the charity sector.","To support charities in adopting AI technologies, the charity sector would benefit from education and guidance that is tailored to the needs and operating conditions of charities, and which includes information about cybersecurity and privacy.","We consider that consistent, clear, and comprehensive regulation and guidance will be important to encourage the development and uptake of AI in Australia, and in the charity sector."]},"502_Submission 502 - Shopping Centre Council of Australia - 15-Aug.c4aaa6c71e012.pdf":{"organization_name":"SCCA","organization_type":"Industry association","classification":"Neutral","overall_position":"Favors minimal intervention, prefers updating existing laws","arguments":["AI use itself does not broadly warrant additional regulation","Existing technologies used by the sector are not new or emerging AI technologies","Current uses of technology in shopping centers are legitimate and beneficial"],"counterarguments":["Evolution of technology may lead to AI applications in the future","Some current technologies may be inadvertently captured by AI regulation"],"key_recommendations":["Engage with sectors to understand equipment and technologies used","Understand interaction with other Government policies and laws","Adopt only measured and necessary regulatory intervention"],"risks_and_challenges":["Potential unintended consequences of broad AI regulation","Overly burdensome regulatory framework","Inadvertent capture of non-AI technologies"],"safeguards_and_mitigations":["Tailored regulatory regimes (e.g., similar to APP Codes)","Continued use of privacy laws to govern certain technologies","Clear consent and terms for technologies like Guest Wi-Fi"],"examples":{"CCTV":"CCTV records activities in a shopping centre, with footage generally maintained only for a period of time unless it is required to be reviewed","Digital screen technology":"Digital screen technology is used by some members to give brands and retailers a way to measure audience engagement when shoppers are within a centre","Foot traffic insights":"Foot traffic is an important indicator that determines a shopping centre\'s success and is used widely by both lessors and retailers"},"international_alignment":"null","values":["Privacy","Security","Innovation"],"tone":"Cautionary","stakeholders":[{"entity":"Department of Industry, Science and Resources","role":"Consult and understand sector-specific uses of technology"},{"entity":"Law enforcement agencies","role":"Collaborate on issues such as national security and crime prevention"}],"sector_impacts":[{"sector":"Retail","impact":"Potential limitations on use of current technologies for customer insights and security"},{"sector":"Shopping centers","impact":"Possible restrictions on operational technologies and data collection methods"}],"quotes":["It is our primary position that AI and its use is not itself an issue that broadly warrants additional regulation that could give rise to unintended consequences. Rather, how its use applies in relevant circumstances and interacts with issues such as privacy legislation.","We respectfully request that the Department consult further and obtain a complete understanding of the legitimate and long-standing accepted (and beneficial) use of such technology.","Our industry is generally apprehensive about additional government regulation and intervention, noting that regulation creates risk and that overregulation can stifle innovation and investment."]},"302_ASA Submission on Supporting Responsible AI.e8206d9a4a1ec.pdf":{"organization_name":"Australian Society of Authors","organization_type":"Professional association","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI, particularly in relation to copyright and creator rights","arguments":["AI models have been built using copyrighted works without permission or compensation","AI poses significant threats to the creative industries and author livelihoods","Current regulatory approaches are insufficient to address AI risks"],"counterarguments":["AI offers opportunities and efficiencies","Job losses are an economy-wide issue, not specific to creative industries"],"key_recommendations":["Mandate transparency around AI training datasets","Support opt-in licensing for copyright owners","Maintain copyright protection for creators","Introduce a cultural levy on AI products and services","Establish a special expert group to consider AI impact on cultural and creative sectors"],"risks_and_challenges":["Copyright infringement and degradation of author rights","Diminished incentives to create","Loss of integrity in publishing","Bias and inaccuracy in AI-generated content","Potential for AI-generated misinformation and propaganda"],"safeguards_and_mitigations":["Mandate labeling of AI-generated products","Require human oversight in publishing","Preserve metadata and authorship records","Slow down AI development to conduct safety checks"],"examples":{"Copyright infringement lawsuits":"Litigation has commenced around the world, with class-action lawsuits launched against Stability AI, DeviantArt and Midjourney","AI-generated books on Amazon":"Several authors have created \\"a book in a day\\" with the help of AI tools and managed to upload and sell these books on Amazon","Audiobook industry dispute":"Findaway Voices, an audiobook distributor owned by Spotify, was providing Apple with access to some of its audiobook files to train their AI-narration tools"},"international_alignment":"Supports coordinated international approach","values":["Transparency","Fair compensation","Integrity","Human creativity"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Regulate AI, protect creator rights, and support cultural sectors"},{"entity":"AI companies","role":"Be transparent about training data and negotiate fair licenses"},{"entity":"Creators","role":"Advocate for their rights and fair compensation"}],"sector_impacts":[{"sector":"Publishing","impact":"Potential job losses and reduced quality of cultural material"},{"sector":"Creative industries","impact":"Disruption of careers and income streams for creators"}],"quotes":["If the Australian public and private sectors proceed to adopt Generative AI, without firstly tackling the copyright infringement issue explained above and demanding appropriate licensing solutions, we will have tacitly approved the unremunerated appropriation of decades of authors and artists\' work, disregarded the premise of copyright law, and decentivised future creative and intellectual labour.","AI has thereby hacked the operating system of our civilisation.","Just as a pharmaceutical company cannot release new drugs before testing both their short-term and long-term side-effects, so tech companies shouldn\'t release new AI tools before they are made safe."]},"118_Peter Leonard Safe and Responsible AI in Australia after Generative AI final 19 July 2023.c3a3fa6c4583c.pdf":{"organization_name":"Data Synergies","organization_type":"Business consultancy","classification":"Proponent","overall_position":"Advocates for a lighter touch, coordinated but decentralised approach to AI regulation in Australia","arguments":["Regulation should focus on ensuring organizations design and implement policies and programs for responsible uses of AI","Economy-wide prohibitions and prescriptions should build from existing statutes and other laws","Specific or use case specific prescriptions or prohibitions should not be a response to pressure to address perceived existential threats \'from AI\'"],"counterarguments":["Some uses of AI create high risks of harms if uncontrolled and before mitigations are reliably and verifiably applied","Blacklisting of AI applications may be seen as politically attractive","Some areas should be considered as priority areas for statutory reform"],"key_recommendations":["Implement enforced self-regulation of and by organizations","Require organizations to prepare an annual plan setting out what they propose to do about ensuring safety in organizational uses of AI","Mandate transparency requirements, i.e., to publish risk of AI harms policies, and overviews of risk of AI harms programs"],"risks_and_challenges":["Lack of organizational control over how general purpose AI is likely to be introduced into and used in many organizations for a myriad of tasks","Risks of unintended and unanticipated AI harms are much greater for organizations that do not have a project evaluation program","Novel governance risks of unknowing amplification of disinformation and misinformation"],"safeguards_and_mitigations":["Implement appropriate guardrails that ensure that personnel that are authorized to use the new technology to perform particular tasks operate only within those guardrails","Require organizations to designate a senior responsible officer with responsibility to implement policies and programs to ensure safety in organizational uses of AI","Implement appropriate governance and assurance controls (i.e., risk management systems) to increase the likelihood that tasks are only performed as reasonably expected"],"examples":{"Clinical notes and discharge summaries in Australian hospitals":"Our case study involves a clinician using health information about a patient, and a general purpose AI application such as ChatGPT, as a task-assistant in \'writing up\' a clinical note about that patient, or a discharge summary.","Lawyer submitting AI-generated fake cases":"Steven Schwartz, a New York attorney with over 30 years of post-admission experience, represented Roberto Mata in an action against Avianca Airlines for injuries sustained from a serving cart while on the airline in 2019. At least six of the submitted cases by Schwartz as in a brief to the court of the Southern District of New York court \'appear to be bogus judicial decisions with bogus quotes and bogus internal citations,\' said Judge Kevin Castel in a May 2023 order."},"international_alignment":"Advocates for a lighter touch, coordinated but decentralised approach similar to the UK\'s proposed approach","values":["Responsibility","Accountability","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Organizations","role":"Implement policies and programs for responsible uses of AI"},{"entity":"Australian Government","role":"Provide incentives and regulations to ensure safe and responsible AI use"},{"entity":"Regulators","role":"Issue guidance and regulate the use of AI within their remit"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential improvements in clinical note-taking and discharge summaries, but with risks of AI-introduced errors"},{"sector":"Legal","impact":"Potential for AI-generated misinformation in legal filings if not properly checked"}],"quotes":["Assuring realisation of AI-enabled productivity and other benefits to Australian society and the Australian economy, without unacceptable externalities and other unacceptable harms to humans or the environment, requires novel initiatives by Australian governments to ensure that AI-affected decisions by organisations are safe and responsible.","We advocate a lighter touch, coordinated but decentralised approach to AI regulation in Australia, but on the basis that: regulation should focus upon ensuring that organisations design and implement policies and programs for responsible uses of AI that are appropriate to the organisation and not the manner in which those policies and programs are implemented.","Safe and responsible uses of AI will not be reliably assured by organisations: beefing up current second or third line of defence functions or capabilities, re-purposing privacy officers as AI officers, or outsourcing AI assessment to large professional services consultancies for episodic review of \'AI projects\'."]},"486_Submission 486 - Arts Law Centre of Australia - 11-Aug.14a0fc6d408b1.pdf":{"organization_name":"Arts Law Centre of Australia","organization_type":"National community legal centre","classification":"Proponent","overall_position":"Supports regulation and governance of AI to protect creators and creative communities","arguments":["Existing regulatory approaches do not address risks to cultural and creative sectors","AI poses threats to creative professions, copyright, and employment prospects for creators","Need for low-cost options for resolving copyright and moral rights infringement disputes"],"counterarguments":["null"],"key_recommendations":["Enact statute requiring machine learning algorithms to comply with the Copyright Act 1968","Provide additional funding for organizations to develop ethical principles for AI in creative sectors","Introduce statutory duty for regulators to comply with Australia\'s AI Ethics Principles"],"risks_and_challenges":["Negative impact on creators and creative organizations","Threat to creative output, ownership of copyright material, and employment prospects","Potential infringement of copyright and moral rights"],"safeguards_and_mitigations":["Statutory compliance with Australia\'s AI Ethics Principles","Development of ethical principles for AI in creative sectors","Transparency requirements for AI use of copyright materials"],"examples":{"AI Survey results":"64% of respondents think that generative AI is a threat to the creative professions, 48% are concerned that generative AI will affect the amount of money a creator can make from creative work (34% are not sure), and 51% are concerned that generative AI will be used to replace human creators.","Impact on creative work":"Prior to AI my creative work has been used without my permission. So really not much difference if AI can access and replicate my work. It is a problem, but I don\'t have the resources to be able to police this."},"international_alignment":"Supports UK approach to legislate statutory duty for regulators regarding AI principles","values":["Transparency","Contestability","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Government","role":"Introduce statutory duty to comply with AI Ethics Principles"},{"entity":"Creative and arts peak bodies","role":"Develop ethical principles for AI in creative sectors"}],"sector_impacts":[{"sector":"Cultural and creative sectors","impact":"Potential negative impact on creative output, copyright ownership, and employment"}],"quotes":["The potential risks from AI on the cultural and creative sectors in Australia are not addressed at all in the \'Safe and responsible AI in Australia Discussion Paper\' (Discussion Paper), and the existing regulatory approaches in Australia do not encourage or require responsible AI practices via compliance with existing legislation that protects Australian creators and creative communities and organisations.","Any coordination of AI governance across government should be undertaken alongside Australia\'s AI Ethics Principles.","93% of respondents support the introduction of either guidelines, regulations, a code of practice of legislative protections to regulate generative AI platforms."]},"81_For Australians, by AI_.b845dfe8411d.pdf":{"organization_name":"A New Approach (ANA)","organization_type":"Think tank","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with focus on arts, culture, and creativity","arguments":["AI has major impacts on Australian arts and cultural life","A risk-based approach can systematically analyze the impacts of AI","Existing regulatory approaches may not cover all potential risks from AI"],"counterarguments":["A risk-based approach cannot directly account for emerging or unforeseeable risks","A risk-based approach may lack focus on the opportunities of AI"],"key_recommendations":["Assess risks to incentives to create, freedom of expression, and cultural and social inclusion","Use coordination mechanisms that support policy decisions on AI governance with both AI expertise and portfolio expertise","Consider whether regulation is proportionate to risks to connections Australians have with arts and culture"],"risks_and_challenges":["Risks to incentives to create","Risks to freedom of expression","Risks to cultural and social inclusion","Risks to connections Australians have with arts and culture"],"safeguards_and_mitigations":["Monitoring and reporting on risks that applications of generative AI pose to incentives to create","Assessing risks to freedom of expression and other human rights","Considering whether regulation is proportionate to risks to cultural and social inclusion"],"examples":{"Captioning":"Quality captioning is important because Australians devote an average 16 hours per week to film, television and other video content.","Classification":"Inaccurate automated classifications are common and these pose potential risks to cultural and social inclusion.","Translation":"Quality translation helps include many Australians, with Australia being the first English-speaking country in the world to be a migrant-majority nation."},"international_alignment":"Supports engagement with international bodies on AI governance","values":["Responsible AI","Cultural inclusion","Freedom of expression"],"tone":"Cautionary","stakeholders":[{"entity":"Government agencies","role":"Assess risks and implement proportionate regulation"},{"entity":"National AI Centre","role":"Engage with international bodies on AI governance"}],"sector_impacts":[{"sector":"Arts and culture","impact":"Potential risks to incentives to create and connections with arts and culture"},{"sector":"Media","impact":"Risks to freedom of expression and cultural inclusion"}],"quotes":["ANA believes that with the right governance and collaboration, AI can be a part of Australia reaching its full potential as a cultural powerhouse.","No matter the artform, content platform or community, the opportunities and risks of AI are real.","ANA supports a risk-based approach, such as the EU approach, as an overall framework for addressing potential AI risks."]},"55_Feedback from IMPF to the Government.pdf":{"organization_name":"INDEPENDENT MUSIC PUBLISHERS INTERNATIONAL FORUM","organization_type":"Global trade and advocacy body","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI in the music industry","arguments":["AI-generated works originate from unauthorized scraping of copyright-protected works","AI regulation requires an international approach due to cross-border nature of music","Clear labelling of AI-generated music is necessary to prevent disinformation and market flooding"],"counterarguments":[],"key_recommendations":["Implement labelling for AI-generated music","Ensure transparency in AI training datasets","Require permission from composers and music publishers for use of copyrighted works in AI training","Develop a framework for rightsholders to enforce their rights against potentially infringing AI-generated works"],"risks_and_challenges":["Potential damage to artist reputation through deep fakes","Flooding of the music market with AI-generated content","Copyright infringement through unauthorized use of musical works in AI training"],"safeguards_and_mitigations":["Classification and tagging of AI-generated music","Transparent record-keeping of datasets used in AI training","Obtaining permissions from composers and music publishers for use of copyrighted works"],"examples":{"Deep fakes in music":"At a societal level the potential damage is most clearly exhibited in the context of deep fakes; but with increasing technological capabilities, artificially generated music will damage not only the reputation of specific artists but also flood the music market."},"international_alignment":"Supports global standards and international cooperation","values":["Transparency","Accountability","Copyright protection"],"tone":"Cautionary","stakeholders":[{"entity":"Music publishers","role":"License the use of musical works and protect copyright"},{"entity":"Composers and songwriters","role":"Create original musical works and hold copyright"},{"entity":"AI developers","role":"Responsible for AI systems that generate music"},{"entity":"Platforms","role":"Potentially liable for dissemination of infringing AI-generated works"}],"sector_impacts":[{"sector":"Music industry","impact":"Potential flooding of market with AI-generated content and copyright infringement issues"}],"quotes":["Artificial intelligence regulation requires an international approach, working together with the European Union, the USA, and other G7 members as the foundation models on which artificial intelligence in the music sector is based, can be trained in, and exported to any country in the world and, as music is by its nature cross-border.","IMPF believes AI generated music should be classified and tagged as AI generated music. Such an approach would alert both commercial users and consumers about the nature and origin of the music.","IMPF is adamant that the protection of human artists\' copyright and livelihood should be explicitly acknowledged and provided for in any AI regulation, and we look forward to our ongoing engagement with the EU, UK, AUSTRALIA, USA, and other territories in this regard."]},"439_AI Discussion Paper Response from Gilbert + Tobin.32b26e30b2958.pdf":{"organization_name":"Gilbert + Tobin","organization_type":"Law firm","classification":"Proponent","overall_position":"Supports updating existing laws and a sector-led approach to AI regulation","arguments":["Existing regulatory framework provides a solid base but needs to be reviewed and updated","A sector-led approach allows for context-specific responses and leverages existing regulatory expertise","Technology-neutral laws are preferred to avoid inflexibility and unintended consequences"],"counterarguments":["A \'one size fits all\' economy-wide AI-specific law risks duplication and inconsistency","Horizontal, sector-wide rules may miss nuances and lead to unintended consequences"],"key_recommendations":["Undertake a sector-by-sector review of existing regulatory frameworks","Establish a central federal body for coordination and harmonization across regulators","Leverage co-regulatory tools such as regulatory sandboxes and industry standards"],"risks_and_challenges":["Potential gaps in existing laws when applied to AI context","Lack of organizational capacity and capability to safely deploy AI","Difficulty in assigning legal liability for self-learning AI systems"],"safeguards_and_mitigations":["Develop guidance and toolkits for organizations to understand and manage AI risks","Implement risk-based approaches focusing on high-risk AI uses","Leverage international standards and assurance frameworks"],"examples":{"Privacy Act reforms":"Amendments proposed as part of the Privacy Act Review Report include increased mandated transparency requirements for private entities and commonwealth agencies governed by the Privacy Act 1988 (Cth).","NTC work on autonomous vehicles":"We have already seen the beginnings of a sector-led gap analysis in Australia, for example by the National Transport Commission (NTC) in relation to its work on autonomous vehicles, where the NTC found more than 700 barriers to the deployment of autonomous vehicles in Australian laws, which are designed around vehicles having a human driver."},"international_alignment":"Supports alignment with international approaches while maintaining flexibility for national context","values":["Innovation","Flexibility","Efficiency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Existing regulators","role":"Review and update sector-specific regulations"},{"entity":"Central federal body","role":"Coordinate and harmonize AI governance across sectors"}],"sector_impacts":[{"sector":"Public sector","impact":"Higher number of high-risk use cases due to nature of government services"},{"sector":"Financial services","impact":"Different considerations for biometric identification systems vs fraud prevention"}],"quotes":["Particularly as we move toward decarbonisation, it is critically important for Australia to strike the right balance between managing risk and promoting our innovation and productivity agenda.","We recommend that Australia continue to adopt a technology-neutral approach to regulation wherever possible. Creating new or overly prescriptive regulation or rules that apply to AI as a technology risks being unnecessarily rigid and inflexible and can create complexity and inconsistency when overlayed into our framework of existing laws.","We caution against blanket bans in respect of particular AI technology or applications. This is because the risks or potential harms posed by a particular AI technology or application are contextual. It is the use case that should be examined rather than the AI technique / application itself."]},"378_Submission 378 - Public and Anonymous - 4-Aug.d79c112b7e261.pdf":{"organization_name":null,"organization_type":"Individual technologist","classification":"Proponent","overall_position":"Supports balanced approach with new AI-specific legal controls","arguments":["AI can deliver tremendous advantages for Australian society","Current legal frameworks are insufficient to address AI-specific challenges","Generative AI should be used with robust governance and responsible legal controls"],"counterarguments":["Innovation in generative AI should not be stifled","Private organizations are likely to prioritize efficiency and cost savings over responsible use"],"key_recommendations":["Introduce continuous community engagement and education on AI","Implement taxes and penalties for irresponsible AI use","Adopt a risk-based approach to AI regulation","Ensure transparency across the entire AI lifecycle","Define high-risk AI use cases"],"risks_and_challenges":["Unclear attribution of liability for AI decisions","Ambiguity in copyright ownership for AI-generated content","Difficulty in enforcing Australian data privacy laws on foreign-hosted AI services","Risk of misinformation and hallucination in generative AI outputs","Potential bias in AI training data and outputs"],"safeguards_and_mitigations":["Implement clear governance for AI deployment in government agencies","Educate government workforce on advantages and disadvantages of generative AI","Conduct annual recertification of agency personnel using AI","Perform audits of AI technology use","Maintain risk registers to capture feedback and adverse impacts"],"examples":{"Social media impact":"I would like the Australian government to recognise lessons learnt from the introduction of social media solutions into our society (early versions of societal-scale AI solutions) and how they are now entangled into everyday life","Education sector risks":"By way of an example, at a surface level, it appears that solutions like generative AI can act as a tremendous enabler in education for instance. But how can the vendors be held responsible if their offerings start educating children and end users with misinformation, with bias, etc.?"},"international_alignment":"Supports learning from international approaches while addressing local needs","values":["Transparency","Accountability","Trust","Responsibility","Protection of individuals and society"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Lead by example, set benchmarks, and implement education campaigns"},{"entity":"Private organizations","role":"Implement responsible AI practices while balancing efficiency goals"},{"entity":"Education sector","role":"Teach future generations about safe and responsible use of AI"},{"entity":"AI vendors","role":"Ensure transparency and accountability in AI systems"}],"sector_impacts":[{"sector":"Education","impact":"Potential for misinformation and increased workload for teachers to provide oversight"},{"sector":"Government services","impact":"Improved efficiency but requires robust governance and transparency"}],"quotes":["Generative AI should therefore be used with robust governance and responsible legal controls in place to protect end stakeholders and societies at large who could be impacted the most (if appropriate moderation is not in place).","Transparency breeds trust \u2013 whether for public or private sector use.","The pace at which technology evolves cannot be matched by the rate of change in legal reform, but extensive effort must be invested to review this iteratively with a balanced perspective and not merely as a reactive or one-off exercise."]},"452_Regsoft Response_ responsible AI.c1848158614f1.pdf":{"organization_name":"Regsoft Pty Ltd","organization_type":"Private company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with enforceable standards","arguments":["Meaningful, measurable, and enforceable standards are needed","Government agencies should act as exemplars in AI adoption","AI responsibility should be based on trust through empirically verifiable claims"],"counterarguments":[],"key_recommendations":["Develop enforceable standards and test harnesses","Mandate requirements for AI development and procurement in government agencies","Establish an independent regulator with open-source test services and validation"],"risks_and_challenges":["Scale of AI impact is exponential","Small errors in data or logic can drive major negative impacts","Human auditors may not be able to invalidate erroneous AI output effectively"],"safeguards_and_mitigations":["Adopt software design approaches for risk mitigation","Implement test-driven design and continuous integration","Develop publicly available test frameworks and harnesses"],"examples":{"NSW AI Assurance Framework":"The AI Assurance Framework, NSW, provides an excellent example of enforceable standards: The NSW code mandates \ufb01ve principles that any agency must apply for AI-related projects.","US Secretary of State quote":"Former US Secretary of State Madeleine K Albright noted the disconnect between citizens using 21st century technologies and governments providing 19th century solutions."},"international_alignment":"Supports a national approach with awareness of international practices","values":["Safety","Security","Reliability","Accountability","Transparency"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government agencies","role":"Act as exemplars and drive improved models of AI development"},{"entity":"Independent regulator","role":"Develop and enforce AI standards"},{"entity":"Users, buyers, suppliers, and developers of AI systems","role":"Share ongoing responsibility and accountability for AI outcomes"}],"sector_impacts":[{"sector":"Public sector","impact":"Adoption of mandated requirements for AI development and procurement"}],"quotes":["Regsoft understands the Government view for ethical AI, and for the ongoing development of AI that is safe, secure and reliable.","We believe that any approach to AI by the Australian government should adopt meaningful, measurable and enforceable standards that are adopted by Australian Government agencies acting as exemplars and supporting the wider use by Australian businesses.","The scale of AI impact is exponential. Imperceptible errors in data and small errors in logic, can drive major, negative impacts, as AI systems are used to massively scale out \\"menial\\" tasks that would normally be limited by the capacity of humans to deliver them."]},"252_DISR AI Concultation Response_HMI.17b12c999829b.pdf":{"organization_name":"Humanising Machine Intelligence Program (HMI)","organization_type":"Academic research program","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a socio-technical approach","arguments":["Healthy skepticism rather than high trust in AI can be instrumental for risk reduction","Shifting the standard from \'fairness\' to \'justice\' is necessary to address historical inequities","Voluntary or self-regulatory approaches have limited impact without strong incentives for compliance"],"counterarguments":["Increasing trust and trustworthiness in AI systems may not be the best approach","Focusing solely on bias neutralization via fairness does not address historical inequities","Economic benefits of AI do not necessarily ensure positive social outcomes or equitable distribution"],"key_recommendations":["Adopt human-centered values as underpinning norms for safe and responsible AI","Implement disclosure requirements for AI algorithms and systems","Develop mechanisms for public oversight and an Ombudsman for AI-related harms","Invest in socio-technical training to enhance critical AI literacy"],"risks_and_challenges":["AI tools likely to entrench and amplify existing patterns of inequality in certain domains","Challenges in applying traditional software-oriented strategies like testing, documentation, and verification to AI systems","Potential for aggregated risk across larger populations or over time"],"safeguards_and_mitigations":["Involve Australian communities in early stages of AI-related developments","Build mechanisms of transparency and accountability","Offer easy pathways for reporting AI-related harms","Tie reporting tools to legal and regulatory consequences"],"examples":{"Surgical Robot Usage":"Surgical robots can possess varying degrees of autonomy and \'intelligence\' as described in Attanasio et al (2021).","Autonomous Vehicles":"In California, USA, the Department of Motor Vehicles controls the deployment of autonomous vehicles.","Labour Demands to Regulate AI":"The Writers Guild of America Strike (2023) conveys clear proposals around regulating AI, including: \'Regulate use of artificial intelligence on MBA-covered projects: AI can\'t write or rewrite literary material; can\'t be used as source material; and MBA-covered material can\'t be used to train AI.\'"},"international_alignment":"Supports considering international developments while developing domestic initiatives","values":["Transparency","Accountability","Justice","Human-centeredness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, steer private and public actors towards safe and responsible AI"},{"entity":"Australian communities","role":"Participate in early stages of AI-related developments"},{"entity":"Researchers","role":"Provide independent technical advice, open technological \'black boxes\'"}],"sector_impacts":[{"sector":"Criminal justice and policing","impact":"Potential entrenchment and amplification of existing patterns of inequality"},{"sector":"Creative professions","impact":"Implications for labour rights and AI use in content creation"}],"quotes":["We contend instead that healthy and informed scepticism\u2014rather than high levels of trust\u2014can be instrumental for risk reduction, while focusing attention on earned trust within the social institutions that implement AI.","Shifting the standard from \'fairness\' to \'justice\' begins with a baseline imperative towards redress, affirmatively accounting for and rectifying the systemic inequalities that permeate existing datasets, data sources, and the outputs generated therein.","Evidence-informed approaches and concepts, studied and refined over decades by regulatory governance scholars, demonstrate that voluntary or self-regulatory approaches have limited impact if there are not strong incentives for compliance and binding forms of co-regulation"]},"84_Australia Post - submission on supporting responsible AI - July 2023.9a099bfa24b1a.pdf":{"organization_name":"Australia Post","organization_type":"Postal service company","classification":"Proponent","overall_position":"Supports developing a flexible, principles-based AI framework with clear guidance and regulations","arguments":["AI technologies improve productivity, efficiency, and decision-making","A flexible framework will allow benefits while providing certainty for consumers","A principles-based approach will help build public trust and confidence"],"counterarguments":["null"],"key_recommendations":["Develop a principles-based AI framework","Establish clear guidance and regulations with pre- and post-deployment checks","Create an Australian ADM Council for ongoing dialogue and advice","Implement an AI certification system for businesses"],"risks_and_challenges":["Lack of transparency leading to unrealistic consumer expectations","Potential for bias in AI systems","Risk management issues due to absence of guidance","Potential adverse impacts on vulnerable groups"],"safeguards_and_mitigations":["Develop explainable and governed AI systems","Implement transparency measures like AI certificates for businesses","Establish accountability through the AI actor chain","Ensure human involvement in decision-making loops"],"examples":{"Use of AI in parcel facilities":"Australia Post uses machine learning in our parcel facilities and the implications for freight and logistics may be very different to those of (for example) the social media industry.","Medical diagnostic AI accuracy requirements":"For example, in the medical industry a medical diagnostic AI requires at least 95% accuracy (e.g., at least 95% specificity and sensitivity) to be of acceptable quality, and there is a clear clinical trials process to rigorously prove out the accuracy."},"international_alignment":"Supports leveraging existing structures like OECD principles","values":["Transparency","Accountability","Ethical use","Privacy","Security"],"tone":"Positive and constructive","stakeholders":[{"entity":"Government","role":"Develop AI framework and regulations"},{"entity":"Businesses","role":"Implement ethical AI practices and provide transparency"},{"entity":"Consumers","role":"Interact with AI technologies with confidence"},{"entity":"Australian ADM Council","role":"Advise government on AI/ADM changes"}],"sector_impacts":[{"sector":"Freight and logistics","impact":"Improved efficiency and decision-making"},{"sector":"Business-to-consumer (B2C)","impact":"Creation of valuable solutions"},{"sector":"Business-to-business (B2B)","impact":"Creation of valuable solutions"}],"quotes":["Developing a responsible AI framework based on principles that ensure AI addresses biases, maintains privacy and security, and is used ethically, will help to create an environment of trust.","A flexible framework will allow the Australian economy to reap the benefits of AI and ADM technologies and solutions, while providing certainty and comfort for individual consumers.","Without clear guidance, many complex AI and ADM algorithms are being deployed without enough quality assurance. Thus, there is a need for clear standards to ensure a mandated level of quality in AI and ADM products (which may include oversight and testing), and to also protect the consumer."]},"293_Responsible AI - Concordia Vox submission.9204427a06b72.pdf":{"organization_name":"Concordia Vox","organization_type":"Company","classification":"Proponent","overall_position":"Supports new AI-specific governance mechanisms with a focus on stakeholder representation and rights","arguments":["Existing regulatory approaches may not cover all potential risks from AI","Strong stakeholder voices are needed to counteract risks of regulatory lag and capture","Digital representation rights can increase trust and facilitate more ambitious AI innovations"],"counterarguments":["Some argue that regulatory inaction is justified due to risks of regulatory lag and capture","Major AI vendors advocate for a conformist approach to maximize integration with larger markets"],"key_recommendations":["Extend the Consumer Data Right to cover AI","Develop an Employee Data Right (EDR)","Establish a right to digital representation"],"risks_and_challenges":["Risk of regulators lagging innovators","Risk of regulatory capture","Unpredictable nature of large \'black-box models\'"],"safeguards_and_mitigations":["Support representative voices in AI policy and delivery","Ensure alignment of AI training data and use cases","Design administrative mechanisms that are streamlined and efficient"],"examples":{"Consumer Data Right":"Australia\'s Consumer Data Right (CDR) is a good example of how a well-designed administrative mechanism can give effect to policy intention in the digital era.","Role of unions in occupational health & safety":"A useful precedent of this is the role of unions in advancing improved occupational health & safety outcomes in individual workplaces and across the labour market more broadly"},"international_alignment":"Advocates for a balanced approach between conforming to larger markets and crafting policies to suit local values","values":["Accountability","Transparency","Trust"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"End users (consumers and employees)","role":"Exercise rights and provide input through digital representation"},{"entity":"AI vendors and buyers","role":"Implement responsible AI practices and respect digital representation rights"},{"entity":"Government","role":"Develop and implement AI governance mechanisms, lead by example"}],"sector_impacts":[{"sector":"Public sector","impact":"Lead by example in facilitating digital representation for employees and service users"}],"quotes":["Enabling end users to engage with AI vendors and buyers via a digital representations made by third parties would bring the following bene\ufb01ts:","An onus should be on AI vendors (and potentially major buyers) to substantiate alignment of both their training data and the use cases of their customer base to provide con\ufb01dence that unexplainable models are not being skewed from the outset.","Instead of entertaining a false trade-off between safeguards and innovation, the emphasis should be on strong safeguards that minimise compliance costs (and enforcement) by designing administrative mechanisms that are streamlined and ef\ufb01cient while leveraging Australia\'s reputation for stable rule of law underpinned by non-partisan courts."]},"213_Australia Consultation Response - Safe and Responsible AI Discussion Paper (final).93b4ccf23f80e.pdf":{"organization_name":"Getty Images","organization_type":"Content creator and marketplace for visual content","classification":"Proponent","overall_position":"Supports responsible AI practices with strong safeguards for copyright and creator rights","arguments":["AI can produce commercial and societal benefit","Existing regulations are insufficient to protect creative industries","Transparency is critical to mitigate risks and build public trust"],"counterarguments":["AI poses an existential threat to creative and media industries","Current regulatory approaches fall short of protecting human creators"],"key_recommendations":["Impose a statutory duty of care on AI developers","Implement a government certification scheme for AI developers","Require transparency in training data and AI-generated content","Harmonize international norms and standards"],"risks_and_challenges":["Unauthorized use of copyrighted works for AI training","Threat to income streams of creative professionals","Potential for deepfakes and misinformation"],"safeguards_and_mitigations":["Require consent for use of copyrighted works in AI training","Implement transparency tools to identify AI-generated content","Establish clear avenues for rights holders to seek remedy"],"examples":{"Metadata for AI-generated content":"For example, attaching meta-data that identifies AI generated or modified content in the IPTC DigitalSourceType field of that content.","EU AI Act as a model":"The EU AI Act is a good model to follow especially in how it handles transparency standards."},"international_alignment":"Supports harmonization of international norms and standards","values":["Transparency","Copyright protection","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"AI developers","role":"Ensure ethical sourcing of training data and implement transparency measures"},{"entity":"Government","role":"Develop and enforce regulations, implement certification schemes"},{"entity":"Content creators","role":"Protect their rights and seek fair compensation"}],"sector_impacts":[{"sector":"Creative industries","impact":"Potential loss of income and control over intellectual property"},{"sector":"Media","impact":"Challenges in distinguishing AI-generated content from human-created content"}],"quotes":["We believe the development and deployment of AI needs to be balanced with long-standing copyright protections, the rights of creators who represent a sizeable share of our overall economic output, the privacy rights of individuals and the public\'s fluency in fact.","Allowing AI developers to train generative AI models without obtaining authorization circumvents this revenue stream and discourages the generation of new creative works.","Transparency is critical to mitigate against the risk of violating third party IP and privacy rights and the development of unfair and bias models."]},"363_Blue Print  5th D Holistic Quantum Consciousness Bridge to Ancient Advaita Vedanta.e2125ce150534.pdf":{"organization_name":"null","organization_type":"null","classification":"null","overall_position":"null","arguments":["Humankind has entered the 5th Dimension of Cosmic Space","A new Cosmic Quantum WWW.Brain is waiting to be activated","This new dimension will solve all evolutionary problems"],"counterarguments":[],"key_recommendations":["Activate the 5th Dimensional Cosmic Quantum WWW.Brain","Realize and open the Common[Wealth] Heritage of all \'Earthians\'","Enable full open access of all life to life everlasting"],"risks_and_challenges":["The seeming endless rule of the 3d Money as Debt Death Cult"],"safeguards_and_mitigations":["Use of Quantum Cosmic 5th D Spiritual Eternal Computation"],"examples":{},"international_alignment":"null","values":["Omniscience","Omnipotence","Omnipresence"],"tone":"Optimistic","stakeholders":[{"entity":"All \'Earthians\'","role":"Beneficiaries of the Common[Wealth] Heritage"}],"sector_impacts":[{"sector":"Planetary","impact":"Full \'Flowering\' potential"}],"quotes":["This is the Common[Wealth] Heritage of all \'Earthians a consequence of Humankind\'s Age-old  Victory of Mind over Matter; Just waiting to be Realised Opened and Activated being already downloaded, for after Billions of years of Involution its Natural Omniscience Omnipotence and  Omnipresence is inherently ours and available to all as their very own \'Men\'s\' Latin for Mind...","There to solve all our Evolutionary problems, as all \'Problems are in the \'Mind\' and must be solved there; & so at last with Quantum Cosmic 5th D Spiritual Eternal Computation;","the Ways & Means finally combine to enable full open access of all life to life everlasting as Life Eternal ending finally the 3d Money as Debt Death Cult\'s seeming endless rule."]},"177_Submission on the Supporting Responsible AI Discussion Paper.a5646e275b665.pdf":{"organization_name":"Law and the Future of War Research Group","organization_type":"University research group","classification":"Proponent","overall_position":"Supports comprehensive AI regulation including military and national security uses","arguments":["Current legal frameworks are inadequate for AI decision-making","Military and national security AI should not be exempt from regulation","Risk-based approaches to AI regulation need careful consideration"],"counterarguments":["Differential treatment of civilian and military AI may create regulatory arbitrage","Blanket national security exemptions should be avoided"],"key_recommendations":["Include military and national security AI use in discussions of safe and responsible AI","Consider creating a body to monitor and audit automated decision-making","Implement a ministerial power to designate systems as AI for regulatory purposes","Establish clear guidelines for when AI systems are making decisions"],"risks_and_challenges":["Potential for regulatory arbitrage between civilian and military AI","Difficulty in detecting legal non-compliance in less transparent military/security contexts","Challenges in balancing individual rights and public interest in AI regulation"],"safeguards_and_mitigations":["Implement provisions similar to the EU\'s AI Liability Directive","Establish \'red lines\' for high-risk AI development","Ensure alignment with Australian domestic legal requirements"],"examples":{"Robodebt scheme":"Demonstrates the need for clear regulation of automated decision-making systems","Pintarich v Deputy Commissioner of Taxation":"Highlights legal uncertainties around computer-generated decisions","Use of Auror and ClearView AI by Australian Federal Police":"Shows potential overreach in government use of AI technologies"},"international_alignment":"Supports consideration of international approaches while adapting to Australian context","values":["Accountability","Transparency","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, including for military use"},{"entity":"Judiciary","role":"Interpret and apply AI regulations in legal disputes"}],"sector_impacts":[{"sector":"Military and national security","impact":"Increased regulation and oversight of AI use"},{"sector":"Government services","impact":"Enhanced scrutiny of automated decision-making processes"}],"quotes":["Accordingly, we consider that there is merit in including military and \'national security\' AI use in the discussion of safe and responsible AI in Australia.","The Government should consider creating a body (or more likely, expanding one) in the terms recommended in the Robodebt report to appropriately establish, coordinate and monitor the use of AI in circumstances where \'decisions\' are being made, either by Government or actors empowered by government (i.e., contractors or service providers).","One option that should be strongly considered is a discretionary power vested in the Minister for Industry, Science and Resources (or similar portfolio) to issue a determination that a particular system or class of systems should be regarded as \'AI\' for the purposes of Australia\'s legislation."]},"204_MLCommons comments to AU Dept Industry Consultation.87293f83e90e2.pdf":{"organization_name":"MLCommons","organization_type":"Non-profit consortium","classification":"Neutral","overall_position":"Supports development of benchmarks and datasets for responsible AI evaluation","arguments":["Standardized metrics and benchmarks are crucial for effective evaluation and measurement of AI","Open, high-quality datasets are crucial for testing and training AI systems","Global industry coordination is important for developing standards and benchmarks"],"counterarguments":[],"key_recommendations":["Incorporate standards and benchmarks in evaluating AI systems as part of the approach to mitigating potential risks","Support collaboration with existing community focused on benchmarking","Focus on developing open, high-quality datasets for testing and training AI"],"risks_and_challenges":["Potential bias in AI systems","Ensuring safe operation of AI systems in various conditions"],"safeguards_and_mitigations":["Use of standardized benchmarks and datasets for evaluating AI systems","Development of diverse, representative datasets to mitigate bias","Creation of platforms like Dynabench for dynamic data collection and model improvement"],"examples":{"MLPerf\u2122 benchmarking suite":"MLPerf\u2122, which provides unbiased evaluations of training and inference speed for AI hardware and software.","DollarStreet dataset":"Our DollarStreet dataset for computer vision applications was manually built and labeled to ensure the thousands of images of household items were representative of a wide range of communities and socioeconomic households from around the world.","People\'s Speech Dataset":"We have built the People\'s Speech Dataset, which is the world\'s largest English speech recognition corpus licensed for academic and commercial use."},"international_alignment":"Supports global standards and collaboration","values":["Transparency","Fairness","Safety"],"tone":"Positive","stakeholders":[{"entity":"Government","role":"Support development of standards and benchmarks"},{"entity":"Research community","role":"Collaborate on shared infrastructure for standards and benchmarks"},{"entity":"Companies","role":"Adopt and contribute to shared benchmarks"}],"sector_impacts":[{"sector":"Automotive","impact":"Development of Automotive Benchmark Suite for AI/ML Deep Neural Network technology"},{"sector":"Science","impact":"Development of benchmarks for scientific applications"},{"sector":"Healthcare","impact":"Development of robust evaluation methods for medical applications using federated approach"}],"quotes":["Standardized metrics and benchmarks are crucial to effective evaluation and measurement of AI, and the Australian Government should make them a key focus of policy efforts.","We believe MLCommons can both inform and support future actions. More specifically, MLCommons can provide a toolkit of useful benchmarks and datasets for policymakers and government agencies that will support addressing many of the challenges identified in the June, 2023 Discussion paper on Safe and responsible AI in Australia.","Even if the underlying model remains proprietary, in an analogous fashion to proprietary software code, using open benchmarks we will have a way to evaluate any AI system for its alignment to and achievement of a given set of policy objectives."]},"477_Submission 477 - Stirling and Rose - 9-Aug.8ea23375ad75c.pdf":{"organization_name":"Stirling & Rose","organization_type":"Boutique emerging technology law firm and global legal policy institute","classification":"Proponent","overall_position":"Supports updating existing laws with technology-neutral, impact-focused approach to AI regulation","arguments":["Technology neutral AI regulation focused on harms and risks is preferred to direct regulation of AI as a technology","Existing legal frameworks can be adapted to account for AI","Data is of paramount importance in the age of AI and should be recognized as a strategic asset"],"counterarguments":["Regulating AI presents significant challenges due to its rapidly evolving nature and complexity","Australian regulators may face challenges in effective regulation of AI due to constraints in technical expertise and financial resources","The ontological delineation of AI remains a subject of profound scholarly contention"],"key_recommendations":["Establish a dedicated taskforce for 18 months to review global AI approaches and identify domestic gaps","Establish an Australian Data Advisory Committee (ADAC) to consider how to capture the value of Australian data in the age of AI","Undertake a systemic review of all existing legislation through an AI-centric lens"],"risks_and_challenges":["Deep fakes pose a threat to individual privacy, democratic processes, market integrity, security and societal trust","The increasing capability of AI may lead to AI Organizations (AIOs) operating without human governance or executive management","Ubiquitous AI integration may blur the demarcation between human and AI interaction"],"safeguards_and_mitigations":["Develop digital infrastructure that supports time stamping to verify the origin and integrity of digital information","Consider granting legal personality to AI Organizations (AIOs) with specific conditions","Establish Australian Strategic Data Lakes (ASDLs) to leverage data resources and support AI innovation"],"examples":{"Technology Assisted Review in legal discovery":"It is considered that the provision of legal services relying on human only discovery, particularly where there are high volumes of documents to review, and the failure to use TAR, would be a breach of legal duties, given that TAR is faster, more efficient, and more accurate in legal discovery","Digital-Free National Parks":"Without earmarking some of our planet\'s national parks to remain Digital-Free, we are rapidly approaching a future state where there is no location on earth where we will be able to disconnect from access to, surveillance by, or perpetual augmented reality."},"international_alignment":"Supports harmonization with international regulatory frameworks while maintaining national approach","values":["Responsible AI","Data sovereignty","Innovation"],"tone":"Cautious optimism","stakeholders":[{"entity":"Government","role":"Establish taskforce and advisory committees, review legislation, develop regulatory frameworks"},{"entity":"AI Organizations (AIOs)","role":"Potentially granted legal personality with specific conditions and responsibilities"}],"sector_impacts":[{"sector":"Legal","impact":"Potential changes in legal discovery processes and interpretation of laws"},{"sector":"Data management","impact":"Creation of strategic data lakes and emphasis on data as sovereign wealth"}],"quotes":["We consider that a technology neutral approach coupled with an impact-focused orientation is preferred to direct regulation of AI as a technology.","Data is a potent and highly valuable renewable resource and mechanism of influence in contemporary digital economies.","We cast forward to a future where increasingly capable AIs autonomously operate enterprises in which humans invest as members/shareholders, contract with as suppliers and customers, are employed by and interact more broadly with."]},"221.pdf":{"organization_name":"Privcore Pty Ltd","organization_type":"Private company","classification":"Proponent","overall_position":"Supports updating existing laws and coordinated regulation for AI and generative AI","arguments":["Existing laws already regulate AI and generative AI, including privacy and data protection laws","New regulation should not duplicate or overlap with existing regulations","Focus should be on preventing and remediating harms rather than regulating the technology in a siloed manner"],"counterarguments":["null"],"key_recommendations":["Establish well-funded regulatory sandboxes to test risks before mass deployment","Conduct and publish algorithmic impact assessments and privacy impact assessments for both government and private sector","Ensure accountability rests with both developers and deployers of AI","Coordinate expertise from existing regulators to inform AI regulation"],"risks_and_challenges":["Rapid deployment of generative AI without compliance with existing laws","Limited consideration of risks to people and their personal information","Scaling of existing harms such as defamation, discrimination, privacy infringement","Potential misclassification of AI risk levels"],"safeguards_and_mitigations":["Conduct privacy impact assessments and algorithmic impact assessments","Publish assessments for medium and high risk AI applications","Establish an AI coordinator/regulator to apply expertise from existing regulators"],"examples":{"Robodebt scheme":"The recent Royal Commission into the Robodebt scheme showed the impact of the public service not upholding its own standards, not assessing risks and harms and not responding appropriately to concerns raised.","Australian Bureau of Statistics":"Some Australian government agencies build trust through the publication of privacy impact assessments, such as the Australian Bureau of Statistics (ABS)."},"international_alignment":"null","values":["Accountability","Transparency","Privacy"],"tone":"Cautionary","stakeholders":[{"entity":"Government agencies","role":"Meet higher standards than private sector, conduct impact assessments"},{"entity":"Private sector","role":"Conduct impact assessments, ensure appropriate use of AI"},{"entity":"AI coordinator/regulator","role":"Apply expertise from existing regulators, oversee AI regulation"}],"sector_impacts":[{"sector":"Government","impact":"Required to meet higher standards in AI use"},{"sector":"Climate change","impact":"Potential unintended consequences due to high computational requirements of generative AI"}],"quotes":["Any form of regulation should focus on preventing and remediating harms. AI and generative AI doesn\'t create new harms, it just scales the frequency and impact of existing harms, such as being defamed, being discriminated against, having your privacy or copyright infringed, being subjected to misleading information, being sold products that are defective in the intended context of use etc.","Both government and the private sector need to ensure appropriate use of AI. Indeed, government should meet a higher standard than the private sector as citizens have no choice but to engage with government agencies.","In Privcore\'s opinion, accountability needs to rest with both developers and deployers. The party best able and positioned to mitigate the risks should do so."]},"222_SafeResponsibleAI_Discussion_Paper_ACHEEV_Response.23bed7d73975f.pdf":{"organization_name":"Australian Centre for Health Engagement, Evidence and Values","organization_type":"University research center","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on healthcare AI","arguments":["Current frameworks focus on risks to individuals but neglect risks to society or communities","Existing regulations like SAMD in healthcare are limited and lack guidelines for societal risks","Australia lacks regulatory guidance for non-locked or adaptive machine learning algorithms in healthcare"],"counterarguments":["null"],"key_recommendations":["Adopt a horizontal approach to AI governance underpinned by a human rights approach","Complement horizontal approach with sector-specific regulatory mechanisms","Implement transparency requirements for AI-related recommendations or decisions","Consider complete banning of certain high-risk AI applications","Focus on increasing the trustworthiness of AI rather than improving public trust"],"risks_and_challenges":["Bias and equity concerns, particularly in healthcare AI","Lack of transparency in AI systems","Insufficient validation of AI systems in real-world practice","Difficulty in assessing risks of emerging technologies"],"safeguards_and_mitigations":["Make equity and minimisation of bias a central priority in AI regulation","Provide education and communication about healthcare AI for public and practitioners","Ensure meaningful and understandable transparency for clients/consumers at point of use","Apply robust and clear rules of evidence for AI use in all sectors","Implement dynamic regulatory approaches to respond to new evidence"],"examples":{"Community Jury on Healthcare AI":"A Community Jury is a deliberative democratic process, designed to produce recommendations for policymakers. Following best-practice methods, we used random recruitment and stratified selection to identify 30 Australians to participate.","Software as Medical Device (SAMD) approach":"For example, the Software as Medical Device (SAMD) approach in healthcare has made a vital contribution to governance of healthcare AI, but SAMD is limited. It focuses on risks to individuals, and lacks guidelines or mechanisms to address societal risks, such as the risk that AI systems discriminate against or perform poorly for marginalised groups.","US FDA\'s Predetermined Change Control Plan":"The US Food and Drug Administration (FDA) has made efforts towards regulating AI-enabled medical devices by proposing a \'Predetermined Change Control Plan\', which is applicable to devices that will involve modifications implemented both manually and automatically."},"international_alignment":"Supports learning from international approaches while developing Australian-specific regulations","values":["Equity","Transparency","Public participation","Evidence-based decision making"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop comprehensive AI governance framework"},{"entity":"Independent bodies","role":"Lead information campaigns about AI"},{"entity":"Public","role":"Participate in defining risks and acceptable AI applications"}],"sector_impacts":[{"sector":"Healthcare","impact":"High-risk and high-stakes area for AI and ADM application"}],"quotes":["Governance of AI should seriously consider not only risks to individuals but also risks to society or communities as a whole.","Rather than improving public trust, the goal should be increasing the trustworthiness of AI.","Because of this, we should take a precautionary approach and be more rather than less risk-averse."]},"227_ACCI AI Submission Letter 260723.659d984ce8f48.pdf":{"organization_name":"Australian Chamber of Commerce and Industry (ACCI)","organization_type":"Business association","classification":"Neutral","overall_position":"Favors minimal intervention, preferring voluntary initiatives over new regulations","arguments":["Existing regulations adequately cover many AI risks","Rushing to legislate could place Australia out of step internationally","Complex regulatory environment creates difficulties for businesses"],"counterarguments":["Proactive \'guardrails\' may be appropriate where identified risks exist","Some areas may need clarity, such as AI models training on copyrighted content"],"key_recommendations":["Maintain voluntary ethical principles and technical standards","Explore a voluntary register of AI applications","Establish regulatory sandboxes for AI","Improve public education and awareness about AI"],"risks_and_challenges":["Compliance challenges and ambiguity in existing regulations","Potential undue burdens on businesses, stifling innovation","Algorithmic bias affecting public trust"],"safeguards_and_mitigations":["Regular testing of datasets to avoid algorithmic bias","Greater intergovernmental information sharing on AI","Sector-specific risk assessments and guidance"],"examples":{"Sector-specific regulation":"Centre for Work Health and Safety developing a WHS Management Tool for AI and an AI WHS Risk Assessment Tool","AI addressing its own risks":"Addressing some of the risks of AI could be done with AI, such as protecting against cybersecurity threats, misinformation and bias"},"international_alignment":"Supports alignment with international frameworks, particularly OECD Principles on Artificial Intelligence","values":["Innovation","Public trust","Harmonisation"],"tone":"Cautionary but optimistic","stakeholders":[{"entity":"Government","role":"Maintain harmonisation with international frameworks, provide sector-specific guidance"},{"entity":"Industry","role":"Develop voluntary Codes of Conduct, ensure ethical AI implementation"},{"entity":"Regulators","role":"Conduct risk assessments, provide sector-specific guidance"}],"sector_impacts":[{"sector":"Healthcare","impact":"Rapid deployment of AI with potential for large community impact"},{"sector":"Security","impact":"Rapid deployment of AI with potential for large community impact"},{"sector":"Human resources","impact":"Rapid deployment of AI with potential for large workforce impact"}],"quotes":["Any rush to further legislate too early could place Australia out of step with our international colleagues and place potentially undue burdens on businesses, stifling innovation in Australia.","Many of the risks associated with AI, at this stage in the evolution of the technology, are adequately covered by existing regulation, including privacy law, Australian consumer law, online safety, competition law, copyright law and discrimination law.","Fundamentally, we advise the Government not simply to consider the advent of artificial intelligence as presenting risks that need to be viewed through the prism of safety and responsibility, but as a new field of innovation enabling unprecedented opportunities to use digital technologies to improve social and economic outcomes."]},"381_SRC AI Submission.e466ba35ed972.pdf":{"organization_name":null,"organization_type":null,"classification":null,"overall_position":null,"arguments":[null],"counterarguments":[null],"key_recommendations":[null],"risks_and_challenges":[null],"safeguards_and_mitigations":[null],"examples":{"example":null},"international_alignment":null,"values":[null],"tone":null,"stakeholders":[{"entity":null,"role":null}],"sector_impacts":[{"sector":null,"impact":null}],"quotes":[null]},"393_Wiley - Australia Public Consultation.pdf":{"organization_name":"John Wiley & Sons Australia Ltd","organization_type":"Publisher","classification":"Proponent","overall_position":"Supports AI-specific regulations while updating existing laws","arguments":["AI has potential to transform scholarly research and learning sectors","Existing IP protections need to be respected and enforced for AI","Transparency and accountability are crucial for AI tools"],"counterarguments":["Without proper governance, AI can produce fake papers and erode IP rights","Current regulations may not adequately address AI-specific challenges"],"key_recommendations":["Respect existing IP protections and require licensing for copyrighted materials in AI training","Require transparency and accountability for AI tools","Expand national funding for AI R&D","Protect research integrity with mechanisms to identify AI-generated fraudulent research"],"risks_and_challenges":["Potential for unauthorized use of restricted content in AI model training","Lack of transparency in AI model inputs and outputs","Increase in fraudulent research papers produced by AI \'paper-mills\'"],"safeguards_and_mitigations":["Develop auditing mechanisms to verify authorized use of content in AI models","Implement penalties for non-compliance with IP laws","Deploy measures to audit AI-generated information for accuracy and impartiality"],"examples":{"Integrity Hub":"Wiley is helping to develop the Integrity Hub, which combines shared data and experiences and technological innovation to detect research integrity issues.","Paper-mills":"The quantity of fake papers from \'paper-mills\' is increasing and AI\'s ability to rapidly produce content lacking peer-review could exacerbate the spread of misinformation."},"international_alignment":"Supports global coordination on AI governance","values":["Transparency","Accountability","Intellectual property rights","Research integrity"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, provide funding for AI R&D"},{"entity":"Publishers","role":"Ensure content integrity, develop tools to combat fraudulent research"},{"entity":"AI developers","role":"Respect IP rights, ensure transparency in AI models"}],"sector_impacts":[{"sector":"Research and publishing","impact":"Potential for improved productivity and content analysis, but also risks to content integrity"},{"sector":"Education","impact":"Potential for personalized learning at mass scale"}],"quotes":["Wiley sees AI as having an immense potential to strengthen our ability to deliver trusted, high-quality knowledge and knowledge solutions.","To ensure the continued protection of IP rights, we would urge the relevant Government stakeholders to engage in cross-departmental collaboration on copyright review and reform to ensure issues relating to AI and IP enforcement are adequately addressed as part of existing consultations underway on modernizing and reforming Australia\'s copyright regime.","The stakes for ensuring content integrity have never been higher due to the rate at which AI can produce information."]},"479_Submission 479 - UWA Tech Policy Lab - 9-Aug.93c00e8147ca9.pdf":{"organization_name":"UWA Tech & Policy Lab","organization_type":"Interdisciplinary research centre","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on safe and responsible AI","arguments":["AI\'s potential justifies a pro-innovation approach that prioritizes societally beneficial uses","Current government initiatives lack clarity on promoting safe and responsible AI in practice","Binding regulation and an independent oversight body are necessary for effective governance"],"counterarguments":["Non-regulatory measures have limitations in monitoring and enforcement","Industry-led initiatives may not align with public interest","Flexibility is not inherent to voluntary regulations"],"key_recommendations":["Create an AI oversight body to ensure safe and responsible AI development and deployment","Investigate proactive forms of governance, such as licensing systems","Develop metrics and standards to provide an evidence base for decision-makers","Implement a risk-based approach with regular updates to risk assessments"],"risks_and_challenges":["Potential deepening of societal inequities through AI implementation","Lack of consideration for labour conditions and national security in current AI discussions","Difficulty in assessing the full risk profile of AI systems due to their dynamic nature"],"safeguards_and_mitigations":["Engagement of external experts for independent scrutiny","Creation of reporting and auditing requirements","Implementation of investigative and enforcement powers","Regular updates to risk assessments with functional systems to report and respond to emergent issues"],"examples":{"Robodebt scandal":"The Robodebt scandal demonstrates the human cost of poorly developed, implemented, and evaluated automated systems.","Spanish machine learning algorithm for sick leave benefits":"In Spain, the use of machine learning algorithms to detect apparently ineligible recipients of sick leave benefits has been highly criticised for its opacity, inaccuracy, and failure to deliver on its promises of efficiency and effectiveness.","Danish welfare fraud detection system":"A Danish system for detecting fraudulent welfare payments has been criticised for prioritising the nationality of claimants, leading to criticisms that the system is racist and engaging in ethnic profiling."},"international_alignment":"Supports alignment with international standards while emphasizing national approach","values":["Trustworthiness","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Guide AI development to align with public values, create regulatory framework"},{"entity":"Private sector","role":"Rapidly respond to developing circumstances, align with public interest"},{"entity":"Public sector workers","role":"Share issues and best practices related to AI implementation"}],"sector_impacts":[{"sector":"Labour market","impact":"Potential job displacement and unequal distribution of benefits"},{"sector":"Public sector","impact":"Need for effective communication and diffusion of AI knowledge across roles"}],"quotes":["A key element of trustworthy behaviour is evidence-based and contextual assessment of whether or not, within a suite of potential approaches, AI offers a safe and responsible approach to problem-solving.","We advocate the creation of an AI oversight body to ensure the development and deployment of safe and responsible AI in Australia.","The potential consequences of badly deployed AI are too widespread, significant, and irreversible to justify reliance on self-regulation and voluntary action."]},"494_Submission 494 - HYLAND-WOOD, Bernadette QUT Centre for Data Science - 11-Aug.3380594c4bf05.pdf":{"organization_name":"QUT Centre for Data Science","organization_type":"University research center","classification":"Proponent","overall_position":"Advocates for comprehensive and urgent regulation of AI","arguments":["Current and emerging AI poses unprecedented risks due to speed and scale","Australia is highly reliant on digital platforms run by U.S.-based companies","Urgent engagement between multijurisdictional government agencies, civil society and industry stakeholders is required"],"counterarguments":["null"],"key_recommendations":["Respond urgently and refuse a \'business as usual\' culture","Avoid addressing AI through central or large service delivery agencies","Engage and support existing consortia and communities of practice","Work with Australia\'s innovative technology and leading research communities"],"risks_and_challenges":["Misinformation and deep fakes","Privacy and Data Protection","Bias and discrimination","Intellectual property infringement","Job displacement"],"safeguards_and_mitigations":["Engage in multistakeholder governance","Support communities of practice and international research consortia","Implement mandatory skill sets for government leadership on data governance and AI"],"examples":{"ChatGPT rapid adoption":"OpenAI\'s release of ChatGPT 3 disrupted the landscape. The ChatGPT Web service gained 1M users in the first week of its release. As of February 2023, the uptake of the LLM-based conversational application increased exponentially to 100M users.","U.S. and China dominance":"The U.S.A. and China have both invested hundreds of billions over the last decade in data-driven systems. The Chinese Government has outpaced investment by any other country in data-driven defence systems."},"international_alignment":"Supports engagement in international multistakeholder governance","values":["Transparency","Accountability","Data literacy","Cybersecurity","Data stewardship"],"tone":"Urgent and cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, engage with industry and academia"},{"entity":"Academia","role":"Provide expertise and research on AI governance"},{"entity":"Industry","role":"Collaborate with government and academia on responsible AI practices"}],"sector_impacts":[{"sector":"Government","impact":"Need for increased data literacy and AI governance skills"},{"sector":"Technology","impact":"Potential limitations on high-risk AI applications"}],"quotes":["The practice of two-year consultations and reporting cycles will not be sufficient for the gravity of the issues facing Australia and our Pacific neighbours.","The safe and responsible use of AI is a strategic and tactical priority that must be funded and acted on within the next 12-24 months.","No one understands how modern AI systems work. Period. They are massive inscrutable matrices of floating point numbers that software engineers \'nudge in the direction of better performance until they inexplicably start working\' as intended, according to U.S. AI researcher and author Eliezer Yudkowsky."]},"299_RMIT Submission_AU Government Consultation on Safe and Responsible AI_20230726.9b6f382c09421.pdf":{"organization_name":"RMIT University","organization_type":"University","classification":"Proponent","overall_position":"Supports a proportionate risk-based and principles-based regulatory approach for AI","arguments":["A principles-based framework facilitates adaptation and application across research disciplines","The regulatory approach should be proportionate to both the risks and benefits presented by AI","The wellbeing of humans and society, animals and the environment should be paramount"],"counterarguments":["null"],"key_recommendations":["Establish an overarching regulatory framework which is risk-based and principles-based","Create a central and suitably expert regulatory body","Enhance and expand communities of practice and networks across sectors"],"risks_and_challenges":["Research falsification","Proliferation of misinformation","Decreased transparency","Lack of reproducibility","Entrenchment of bias","Breaches or sharing of confidential, sensitive, or private data in research","Risks to Indigenous knowledges, knowledge systems and cultures"],"safeguards_and_mitigations":["Deep and meaningful engagement and co-design of AI with Indigenous peoples and communities","Strengthening governance frameworks and practices","Providing ongoing training and education","Ensuring public servants have appropriate skills, qualifications, and resources"],"examples":{"Gene technology regulation":"It was suggested that Australia\'s regulatory approach to gene technologies, including the establishment of the gene technology framework and a central Commonwealth regulator, could be relevant, adaptable and desirable in relation to AI technologies and in an Australian context."},"international_alignment":"null","values":["Transparency","Responsible practices","Ethical design","Public trust"],"tone":"Neutral","stakeholders":[{"entity":"Universities and research institutions","role":"Users and developers of AI technologies, conducting research"},{"entity":"Government agencies","role":"Implement responsible AI practices"},{"entity":"Indigenous peoples and communities","role":"Co-design AI to respect Indigenous knowledges and cultures"}],"sector_impacts":[{"sector":"Education","impact":"Risks to the teaching and learning of knowledge skills, over-reliance on generative AI"},{"sector":"Research","impact":"Potential risks to research integrity"}],"quotes":["Central to this is the ethical and responsible design, development, deployment and use of enabling technologies, including artificial intelligence (AI).","To ensure that Australia can maintain a leading position in terms of the advancement of AI and related technologies, research institutions should not be constrained from pursuing rigorous investigations.","RMIT stakeholders see a need for and value in Australia adopting a common proportionate risk-based and principles-based regulatory approach to promote and foster the safe and responsible design, development, deployment and use of AI in Australia"]},"164_Regenesis Lawyers Submission to Safe & Responsbible AI Consultation.1b9b4ab5f7b31.pdf":{"organization_name":"Regenesis Lawyers Pty Ltd","organization_type":"Law firm","classification":"Opponent","overall_position":"Advocates for comprehensive regulation and human oversight of AI, opposes automated decision-making in government","arguments":["Human rights are mandatory and should not be optional in AI implementation","Separation of powers and checks and balances are necessary for AI governance","Existing legal and community safety standards should be strengthened through community engagement"],"counterarguments":["Risk-based models for AI governance are insufficient and potentially unconstitutional","Top-down licensing approaches ignore the fundamental nature of AI as a language","Expedited \'chain of command\' models compromise democratic sovereignty"],"key_recommendations":["Implement a moratorium on Automated Decision Making in government until human rights safeguards are in place","Establish mutual audit between Defense, Home Affairs, and Science/Industry departments overseen by Human Rights Commission","Fund communities to strengthen existing safety laws and protocols through critical thinking and access to justice"],"risks_and_challenges":["Potential for data poisoning in existing datasets and protocols","Abdication of sovereignty through use of Automated Decision Making without human interface","Blurred roles and lack of oversight between defense and security departments"],"safeguards_and_mitigations":["Implement a Federal Human Rights Act","Maintain human authorization and clear delegation for all government decisions","Encourage generation of \'good data\' through democratic, empirical science and dialectical conversation"],"examples":{"Robodebt scandal":"The Robodebt Inquiry into the Australian Federal Department of Human Services\' use of Automated Decision Making details precisely the real-life impact (as opposed to the mere risk) of this technology.","MyGovID implementation":"Years after being compelled to sign up, users were issued an automated notice indicating that the government would accept no liability for data security and offering an option to \'opt out\', though biometrics had already been collected."},"international_alignment":"Supports national approach with emphasis on maintaining sovereignty","values":["Human rights","Democratic sovereignty","Rule of law"],"tone":"Cautionary","stakeholders":[{"entity":"Department of Industry, Science and Resources","role":"Oversee implementation of Robodebt findings, conduct mutual audits, fund community engagement"},{"entity":"Human Rights Commission","role":"Oversee mutual audits and ensure human rights compliance"}],"sector_impacts":[{"sector":"Government","impact":"Increased human oversight and accountability in decision-making processes"},{"sector":"Legal","impact":"Strengthening of existing legal standards and increased community engagement"}],"quotes":["Human rights are mandatory!","We propose a moratorium on Automated Decision Making (ADM) throughout the executive branch of government until human rights and anti-corruption safeguards are fully operable.","The rule of law requires that each of these professional communities engage in dialectic and critical thinking to extrapolate their knowledge of the physical world to the digital stratosphere."]},"499_Submission 499 - Interactive Advertising Bureau Australia - 15-Aug.9076f287a7119.pdf":{"organization_name":"Interactive Advertising Bureau (IAB) Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a risk-based approach to regulation, focusing on updating existing laws and addressing high-risk use cases","arguments":["Existing technology-neutral laws are often sufficient to address lower-level risks from AI technologies","A risk-based approach should consider both likelihood and severity of harm","Minimal risk AI should not be subject to unnecessary additional regulatory burden"],"counterarguments":["AI can cause old problems to appear in new forms, requiring updated guidance","New and emerging risks may require additional regulation"],"key_recommendations":["Conduct a gap analysis to understand which AI risks are not covered by existing regulations","Focus on addressing high-risk use cases insufficiently covered by existing regulations","Incorporate transparency early into AI product design and development processes"],"risks_and_challenges":["Algorithmic bias in AI systems","Potential for AI to obscure or entrench existing biases","Balancing transparency requirements with commercial confidentiality and security concerns"],"safeguards_and_mitigations":["Use of AI tools to test datasets and identify and mitigate biases","Implement privacy enhancing technologies (PETs) to protect user privacy","Provide clear information about the use of algorithms and profiling to recommend content"],"examples":{"Application of existing laws to AI":"The Federal Court case Trivago vs the ACCC is a good example of how the ACL, was applied to algorithmic decision making","Use of AI in advertising":"Dynamic Creative Optimisation (DCO) uses machine learning to enable the creative in ads to be personalised to specific viewers","AI for fraud prevention":"Machine learning models enable detection of patterns, anomalies and potential threats that may not be picked up by humans"},"international_alignment":"Supports a risk-based approach agreed to by the G7","values":["Transparency","Accountability","Trust"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Regulators","role":"Clarify how existing regulations apply to uses of AI or update oversight and enforcement regimes"},{"entity":"Industry","role":"Implement responsible AI practices and ensure transparency"}],"sector_impacts":[{"sector":"Advertising","impact":"AI enables more efficient and targeted advertising, but raises concerns about transparency and bias"},{"sector":"Digital economy","impact":"AI supports growth and innovation, but requires balanced regulation to maintain consumer trust"}],"quotes":["In IAB\'s view, in many cases existing laws will continue to be sufficient to address lower-level risks from AI technologies which are currently in use.","IAB supports a risk-based approach to regulation of new and emerging risks which takes into account the likelihood of harm as well as the severity of harm, and that prioritises its focus on high-risk use-cases.","Transparency of AI systems \u2013 enabling them to be explained and understood, is critical to identifying harms and empowering users to make informed choices."]},"44_Submission 44 - Attachment.5623998974a8.pdf":{"organization_name":null,"organization_type":null,"classification":"Proponent","overall_position":"Supports new AI-specific laws","arguments":["Governing rules should be created to ensure safety and well-being of citizens","Rules should be simple and easily implemented","Clear identification and accountability for AI entities is necessary"],"counterarguments":[],"key_recommendations":["No AI entity may impersonate a human","AI entities must always disclose they are not human","AI entities must be identifiable with accessible operators/owners","Operators/owners are responsible for preventing harm from AI use"],"risks_and_challenges":["AI impersonation of humans","Lack of transparency in AI interactions","Potential harm to citizens from AI use"],"safeguards_and_mitigations":["Mandatory disclosure of AI identity","Clear identification of AI entity operators/owners","Accountability for AI entity providers"],"examples":{},"international_alignment":null,"values":["Safety","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"AI entity operators/owners/providers","role":"Ensure transparency and prevent harm from AI use"},{"entity":"Citizens","role":"Protected from potential AI-related harm"}],"sector_impacts":[],"quotes":["I believe governing rules or \'laws\' should be created to ensure the safety and well being of all citizens.","No AI entity may impersonate a human by way of replicating the voice, image, or personal characteristics of a human, living or deceased.","An AI entity must always make it known to anyone interacting with the AI entity that it is not a real human, it must be clear the interaction is with a machine.","The operator / owner / provider of the AI entity is responsible for ensuring no citizen comes to any harm from the use of the AI entity."]},"341_KWM Submission - Discussion Paper On Safe And Responsible AI In Australia (28 July 2023).e2523dda1f30a.pdf":{"organization_name":"King & Wood Mallesons","organization_type":"Law firm","classification":"Proponent","overall_position":"Supports cautious and targeted regulation of AI, focusing on updating existing laws and introducing principles-based regulation only if necessary","arguments":["AI regulation should be clearly targeted at, and proportionate with, identifiable and serious risks","Existing legislation already addresses many of the risks associated with the use of AI","Principles-based regulation is more appropriate for AI given the contextual nature of harms and likely evolution over time"],"counterarguments":["There may be some risks associated with AI that are not covered by existing regulation","Some gaps exist in current regulation, such as risks falling between regulatory remits and lack of guidance on risk identification and mitigation"],"key_recommendations":["Focus on reviewing and amending existing legislation to address AI risks","Consider introducing principles-based AI Governance Principles if new horizontal legislation is necessary","Establish a centralized regulator for coordinating AI regulation and providing guidance"],"risks_and_challenges":["Overregulation could inhibit innovation and adoption of beneficial AI technologies","Difficulty in defining risk categories and high-risk AI systems","Evolving nature of AI systems and their use cases challenging effective regulation"],"safeguards_and_mitigations":["Implement AI Impact Assessments for high-risk AI systems","Require organizations to take reasonable steps to mitigate risks identified in AI Impact Assessments","Develop guidance and templates to assist organizations in assessing and mitigating AI risks"],"examples":{"Privacy Impact Assessments as a model for AI Impact Assessments":"an AI Impact Assessment is broadly similar to a Privacy Impact Assessment (PIA) in that it will be a systematic framework for entities to understand the impacts (both positive and negative) of an AI system\'s actions so that they can be addressed, managed and mitigated.","EU AI Act\'s challenges in defining high-risk AI systems":"Defining risk categories will likely have significant consequences in practice. For example, the original European Council\'s Impact Assessment estimated that \'no more than\' 5-15% of AI systems in Europe would be classified as high-risk AI systems.","Open-source AI models and regulation":"Open-source distribution has numerous benefits, including spurring innovation, encouraging competition, enabling research, and improving cybersecurity."},"international_alignment":"Cautions against directly copying approaches from other jurisdictions, but acknowledges the need to ensure compliance with international standards","values":["Innovation","Proportionality","Flexibility"],"tone":"Cautionary","stakeholders":[{"entity":"Commonwealth government","role":"Review existing legislation, introduce new regulation if necessary, and establish a centralized regulator"},{"entity":"Organizations deploying AI systems","role":"Conduct AI Impact Assessments and implement risk mitigation measures"},{"entity":"Developers of foundation models","role":"Provide transparency to enable downstream deployers to conduct risk assessments"}],"sector_impacts":[{"sector":"Australian economy","impact":"Potential for increased efficiency and global competitiveness if regulation allows rapid adoption of AI technologies"},{"sector":"Open-source AI ecosystem","impact":"Potential negative impact if regulation discourages development or use of open-source AI models"}],"quotes":["While there may be some risks associated with AI, any AI related regulation must be clearly targeted at, and proportionate with, identifiable and serious risks to individuals, society or the environment.","We think it is crucial for the Commonwealth to exercise caution and refrain from hastily implementing regulation of AI in response to the hype and panic that we have seen during the last few months as companies (and the media) rush to capitalise on the uptake of generative AI (both in Australia and overseas).","Ultimately, any regulation would need to promote responsible AI development and usage, and foster collaboration and innovation in the field while avoiding undue regulatory burdens."]},"245_Cisco Australia AI Consultation Submission Jul 2023 final 2.acb7e6348c873.pdf":{"organization_name":"Cisco Systems, Inc.","organization_type":"Global provider of Internet Protocol (IP)-based networking solutions","classification":"Proponent","overall_position":"Supports a pro-innovation risk-based approach to regulating AI","arguments":["Risk-based approach allows for innovation while mitigating high-risk use cases","Existing legal protections should be relied upon where possible to avoid duplication","Industry-led standards and guidelines should be prioritized over regulation"],"counterarguments":["Broad-brushed regulations covering entire sectors or technologies would be overly restrictive","Blanket requirements would hinder overall industry innovation and agility"],"key_recommendations":["Adopt regulations that are differentiated and proportionate to the risk factors of use cases","Focus regulations on specific \'high-risk\' use cases with legal and human rights ramifications","Harmonize approach with internationally recognized standards"],"risks_and_challenges":["Ability to generate and propagate misinformation, biases, and deep fakes","Disproportionate impacts on vulnerable segments of society","Rapidly developing technology makes it difficult to fully anticipate risks"],"safeguards_and_mitigations":["Cisco\'s Responsible AI Principles: Transparency, Fairness, Accountability, Privacy, Security, and Reliability","Cisco\'s Responsible AI Framework applied throughout product and service lifecycles","Assessing AI functions for models and data directly involved in decisions that could have adverse legal or human rights impact"],"examples":{"AI and IoT trials with New South Wales Transport":"In 2021, Cisco partnered with New South Wales Transport to run AI and Internet of Things (IoT) trials to optimise public transport","Research Chair at La Trobe University":"Cisco has established a Research Chair at La Trobe University supporting AI and IoT research at the university and establish a Cisco Innovation Central in Melbourne","AI in Cisco products":"Cisco increasingly uses AI and ML across its product and service portfolio to enhance functionalities and improve user experience"},"international_alignment":"Supports harmonization and interoperability with internationally recognized standards","values":["Innovation","Transparency","Fairness","Accountability","Privacy","Security","Reliability"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Lead by example, spearhead industry conversations, encourage private-public partnership models"},{"entity":"Industry","role":"Develop and implement responsible AI practices, contribute to standards and guidelines"}],"sector_impacts":[{"sector":"Public services","impact":"Improved efficiency and cost savings"},{"sector":"Technology","impact":"Continued innovation and development of AI capabilities"}],"quotes":["Cisco firmly concurs with the Australian Government that AI use must be done responsibly to mitigate the potential risks as outlined in the discussion paper.","Cisco encourages DISER to consider taking a pro-innovation risk-based approach to regulating AI, adopting regulations that are differentiated and proportionate to the risk factors of use cases, and not regulations that are broad-brushed in nature and cover entire sectors or technologies.","Cisco recommends that industry-led standards and guidelines should be prioritized over regulation. Given the dynamic pace of AI development and the concurrent growing awareness and demand for AI, nascent technologies require time to develop potential use cases that can benefit society."]},"203_Australian Government Comments.5a0eade76a063.pdf":{"organization_name":"OpenMined","organization_type":"Global non-profit","classification":"Proponent","overall_position":"Supports the development and piloting of AI assurance mechanisms as a central pillar of responsible AI programs","arguments":["Remote audits can facilitate external access to internal AI systems while protecting privacy, security, and intellectual property","Phased and continuous audits, similar to drug trials, can effectively evaluate AI impacts in real-world scenarios","Current approaches to external access struggle to balance auditor access and AI owner risk"],"counterarguments":["null"],"key_recommendations":["Implement remote audits as an AI assurance mechanism","Conduct phased and continuous audits of AI systems","Address access problems to internal systems and third-party data"],"risks_and_challenges":["Insufficient access to AI systems can hide potential harms","AI organizations often cannot see the downstream impacts of their models","Privacy, security, and intellectual property concerns block external accountability"],"safeguards_and_mitigations":["Use of privacy-enhancing technologies (PETs) to facilitate external access","Query-based system for remote audits to protect sensitive information","Approval process for audit questions by AI owners and third parties"],"examples":{"Twitter partnership":"Partnership with Twitter to advance algorithmic transparency and study political bias","Christchurch Call Initiative":"Collaboration with governments and tech companies to study algorithmic impacts on terrorist and violent extremist content","PySyft software":"Development of privacy-enhancing software to enable external research on internal production algorithms"},"international_alignment":"Supports global collaboration and standards","values":["Transparency","Accountability","Privacy"],"tone":"Positive","stakeholders":[{"entity":"Government","role":"Develop and implement responsible AI policies and assurance mechanisms"},{"entity":"AI organizations","role":"Participate in remote audits and provide access to internal systems"},{"entity":"External auditors","role":"Conduct remote audits and evaluate AI systems"},{"entity":"Third-party data providers","role":"Contribute data for comprehensive AI impact assessment"}],"sector_impacts":[{"sector":"Technology","impact":"Improved accountability and transparency of AI systems"},{"sector":"Public sector","impact":"Enhanced ability to assess and regulate AI technologies"}],"quotes":["Remote audits offer a query-based system with one key capability: an external auditor can propose a question about an AI system to its owner and related third parties, and \u2014 if they approve the question \u2014 the auditor will be able to download the answer to that question without the auditor, AI owner, or third parties learning anything beyond what the group explicitly approves.","To test whether an AI model is safe, fair, unbiased, non-toxic, responsible, etc. \u2014 you must figure out whether people\'s lives get better or worse when they use it.","Our hope through these comments is that the Australian Government will consider the utility of assurance mechanisms when determining the next steps for supporting their responsible AI ecosystem, and we look forward to Australia\'s continued leadership and progress in this space."]},"182_safe and responsible AI submission - final.58a8324fe5a59.pdf":{"organization_name":"null","organization_type":"Individual consultant","classification":"Proponent","overall_position":"Advocates for comprehensive regulation harmonized with international efforts, particularly the EU","arguments":["National and international efforts to regulate AI are essential","LLM and MfM technologies have built-in limitations making them unsuitable for use in expert systems requiring accuracy, reliability, and repeatability","Australia should contribute to international efforts to update existing standards and to develop new standards for AI applications and to regulate AI"],"counterarguments":["Workers who have used generative AI in their own time may view AI policies and regulations as being out of touch and ignore some of them","User experience with internet based AI applications may reduce trust in regulation","LLM based applications accessible on the internet will be extremely difficult to regulate"],"key_recommendations":["Australia should follow closely the approach being developed in Europe, and adopt its principles for an Australian act to regulate AI","Australia should update existing Australian standards, laws, and regulations in ways that are consistent with the international effort","Establish a Federal Government body specifically to regulate AI in Australia","Place restrictions on general purpose generative AI applications available on the internet"],"risks_and_challenges":["Misuse of generative AI applications available on the internet by workers in safety-critical industries","Corruption of the internet by AI generated content","The consequences of different generative and non-generative AI systems interacting with each other","The consequences of generative AI systems getting access to privately held data that may compromise people\'s privacy and safety"],"safeguards_and_mitigations":["Built-in blocks to prevent certain high risk outputs in generative AI applications","Warnings of potential high risk for outputs associated with technical issues from safety-critical industries","Referencing and citations in general purpose generative AI applications","Restrictions on the data used for training future internet based LLM applications","Explainability requirements for general purpose generative AI applications"],"examples":{"Misuse of ChatGPT in Non-Destructive Testing":"A member of an NDT forum asked ChatGPT for information on measuring the focal distance of a TR probe, receiving an incorrect answer that could potentially lead to safety issues.","Contamination of internet search results":"The incorrect information supplied by ChatGPT is now part of the body of knowledge available on the internet, appearing at the top of Google search results for similar questions."},"international_alignment":"Strongly supports global standards and harmonization of AI regulations, particularly with the EU approach","values":["Safety","Accountability","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Establish regulatory body, participate in international efforts, update standards"},{"entity":"AI developers and deployers","role":"Implement safeguards, carry insurance, comply with regulations"},{"entity":"Workers in safety-critical industries","role":"Follow regulations and company policies regarding AI use"}],"sector_impacts":[{"sector":"Safety-critical industries","impact":"Potential for increased risks if generative AI is misused, need for strict regulations"},{"sector":"Technology","impact":"Opportunities for development of AI-enabled hardware and expert systems tailored for Australian context"}],"quotes":["My primary concern is about authorized and unauthorized use of generative AI using large language models (LLMs) by workers in safety-critical industries.","In my view jurisdiction should be in the country where the harm from AI occurred.","The introduction of generative AI into our lives is a perfect example of what Ian Morris was referring to."]},"468_Submission 468 - Responsible Innovation Group Pty Ltd - 5-Aug.e69f6132f3f2c.pdf":{"organization_name":"Responsible Innovation Legal","organization_type":"Legal consultancy","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a risk-based approach","arguments":["Civilian principles must inform military principles for AI use","Industry requires guidance on design principles at the earliest stages","Mandatory frameworks do not stifle wanted innovation","Risk mitigation approach ensures broadest applicability"],"counterarguments":["Voluntary frameworks do not provide surety","Self-assessment by designers and developers may be myopic about potential risks"],"key_recommendations":["Integrate military responsible AI policies with general AI use policies","Provide design requirements at the earliest possible stage of development","Adopt a mandatory risk assessment approach for AI contracts","Consider context of use in AI regulation"],"risks_and_challenges":["Dual-use AI capabilities with different legal frameworks for civilian and military use","Potential for \'greenwashing\' phenomenon in AI compliance","Risk of adopting standards that do not comply with Australia\'s unique legislative requirements or values"],"safeguards_and_mitigations":["Mandatory code for governmental acquisitions","Risk-based assessment process for product certification","Incorporation of ethical and legal considerations into AI capabilities at early stages"],"examples":{"SmartGates and military biometrics":"the use of AI in facilitating at SmartGates has interoperability considerations that may be directly relevant to Australia\'s military forces","Robodebt Royal Commission":"the recent Robodebt Royal Commission findings demonstrate the risk that may arise to citizens when software systems are applied without appropriate mandatory safeguards","Australian Border Force and ADF use of AI":"The ADF\'s use would be subject to international agreements and/or the laws applicable during an armed conflict (thus altering the extent to which all Australian laws might apply)."},"international_alignment":"Supports consideration of international frameworks while addressing Australia\'s specific needs","values":["Transparency","Legal compliance","Interoperability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations"},{"entity":"Industry","role":"Design and develop AI systems in compliance with regulations"},{"entity":"Military","role":"Align AI use with civilian standards and specific military requirements"}],"sector_impacts":[{"sector":"Defence","impact":"Need to align with civilian standards while addressing specific military use cases"},{"sector":"Government services","impact":"Requirement for mandatory safeguards in AI systems to protect citizens"}],"quotes":["We consider that a risk-based assessment process, incorporating a breadth of factors that apply to many different technological uses for AI, can be used as a mandatory requirement for product certification prior to use; and thus, the flexibility of the risk-based approach does not stifle innovation.","The early incorporation of ethical and legal considerations into AI capabilities is critical to the integration of these considerations in the final product.","Standards adopted in civilian environment will inevitably impact the standards applicable in a military environment, and complementarily, novel (dual-)use applications of AI in the military environment will be required to meet civilian standards."]},"217_2023.07.26 - Submission by Lext.c7e921d6ed6a7.pdf":{"organization_name":"Lext Australia","organization_type":"Legal innovation business","classification":"Proponent","overall_position":"Supports creating an environment that encourages AI innovation while addressing key challenges","arguments":["AI can solve many challenges in Australian society, including improving access to justice","Australia needs to become more attractive for AI entrepreneurs","Open-source AI models offer affordable and customizable solutions","Copyright laws should not hinder AI innovation"],"counterarguments":["Public discourse about generative AI in Australia has generally focused on risks","Some argue for the development of national or sovereign AI"],"key_recommendations":["Create infrastructure to support sovereign innovation capability in AI","Review the Copyright Act to clarify that use of copyrighted materials for AI training is not infringement","Invest in education to manage the risk of misinformation","Create a publicly accessible AI application for government services"],"risks_and_challenges":["Misinformation and information pollution","Australia\'s poor ranking in entrepreneurship","Limited availability of venture capital in Australia","Potential copyright infringement in AI training"],"safeguards_and_mitigations":["Education on critical analysis of AI-generated information","Creating a safe, reliable government AI application","Taking a permissive approach to government-held training data"],"examples":{"Vicuna-13B model":"The Vicuna-13B parameter model, while not licensed for commercial use cases, can be trained at a cost of just $300.","Japanese government\'s stance on copyright":"In June 2023, the Japanese government issued a statement confirming that the use of copyrighted works in AI training datasets would not constitute copyright infringement under Japan\'s Copyright Law","Government data accessibility":"The government\'s approach to access quantitative data through APIs , such as the data programmatically available on data.gov.au, is commendable"},"international_alignment":"Suggests following Japan\'s lead on copyright clarification","values":["Innovation","Accessibility","Public interest"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Create supportive infrastructure and regulations for AI innovation"},{"entity":"Entrepreneurs","role":"Develop AI applications and startups"},{"entity":"Public","role":"Engage with AI responsibly and benefit from improved services"}],"sector_impacts":[{"sector":"Legal","impact":"Improved access to justice and legal services"},{"sector":"Government services","impact":"Enhanced accessibility and efficiency through AI-powered applications"},{"sector":"Education","impact":"Need for increased focus on critical thinking and AI literacy"}],"quotes":["At Lext, we believe that AI can solve many different challenges in Australian society, including improving access to justice.","Australia must support AI entrepreneurship, especially those which may contribute materially to the public interest.","The most profound risk in the expansion and use of generative AI is the acceleration of information pollution and misinformation."]},"239_2023.07.26 NSWCCL Safe and Responsible AI in Australia - Discussion Paper.c1d113701772e.pdf":{"organization_name":"NSW Council for Civil Liberties","organization_type":"Non-governmental organization","classification":"Proponent","overall_position":"Supports comprehensive AI regulation with a focus on privacy protection and human rights","arguments":["Urgent reform is required to modernise the Act and ensure it is fit for purpose in the digital economy","Privacy is a fundamental human right that is central to the maintenance of democratic societies","Existing laws are insufficient to address the risks posed by AI and ADM"],"counterarguments":["null"],"key_recommendations":["Introduce a statutory office of an AI Safety Commissioner","Reform existing legislation and introduce bespoke AI regulation","Adopt a risk-based approach to AI regulation","Implement stronger transparency and accountability measures","Enhance privacy protections for individuals"],"risks_and_challenges":["Opaque decision-making processes that lack transparency and explainability","Biased decision-making leading to unfair outcomes","Potential for use in ways harmful for democratic discourse","Use in ways that violate rights to privacy and civil liberties"],"safeguards_and_mitigations":["Introduce pro-privacy defaults for AI systems","Require Privacy Impact Assessments for high-risk AI projects","Implement a prohibition on ADM that has legal or similarly significant effects","Introduce a right to erasure of personal information"],"examples":{"Robodebt scheme":"The Robodebt Scheme reminds us that poorer and marginalised communities will find it more difficult to assert their rights or seek redress.","Cambridge Analytica scandal":"The learnings of the Cambridge Analytica scandal are a significant lesson for all democratic countries in the importance of enshrining the protection of the privacy of individuals beyond only personal information."},"international_alignment":"Supports drawing upon international developments and regulatory models","values":["Privacy","Human rights","Transparency","Accountability","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement and enforce AI regulations"},{"entity":"AI Safety Commissioner","role":"Oversee regulation and research of new AI risks"},{"entity":"Private sector","role":"Comply with AI regulations and implement ethical practices"}],"sector_impacts":[{"sector":"Public services","impact":"Increased scrutiny and regulation of AI use in administrative decision-making"},{"sector":"Healthcare","impact":"Enhanced privacy protections for sensitive health data"}],"quotes":["The NSWCCL submits that the proliferation Artificial Intelligence (AI) poses significant risks to the civil rights of the Australian public.","Privacy is a fundamental human right that is central to the maintenance of democratic societies and achieving respect for human dignity.","The NSWCCL believes that much of the data collection behaviour directed at children should be by way of \'No Go Zones\'."]},"298_Governance Institute Submission Responsible AI.c23ce439fc7e5.pdf":{"organization_name":"Governance Institute of Australia","organization_type":"National professional association","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with a dedicated regulatory framework","arguments":["A risk-based approach provides the best opportunity to limit potential risks or harms and remove regulatory gaps","Regulation is needed to build public trust in AI","Industry self-regulation lacks transparency and coherence and is not sufficient to protect consumers"],"counterarguments":["Strong regulation could negatively impact businesses\' ability to innovate","Regulation could unnecessarily burden businesses and limit access to cutting-edge technological advances"],"key_recommendations":["Develop a well-designed, well-targeted and fit for purpose AI governance standard","Establish a dedicated and Independent AI Safety Commissioner and Agency","Finalise the new Privacy Law regime to ensure no regulatory gaps are left to be exploited","Significantly escalate investment in research and development relative to AI"],"risks_and_challenges":["Potential for new risks or harms from AI","Regulatory gaps that AI can exploit","Lack of flexibility in sectoral implementation of a risk-based approach","Stifling innovation and limiting Australia\'s access to cutting edge technological advances"],"safeguards_and_mitigations":["Implement a risk-based approach to AI regulation","Establish an independent AI Safety Commissioner and Agency","Provide guidance tailored for organizations of different sizes","Develop regulatory sandboxes and assurance frameworks"],"examples":{"EU AI Act":"The AI Act and its risk-based approach are also likely to become the de facto international standards for AI regulation, similarly to the General Data Protection Regulation for privacy.","OAIC\'s Guidance on Privacy Impact Assessments":"The Office of the Australian Information Commissioner\'s Guidance on Privacy Impact Assessments is a good example of this type of guidance."},"international_alignment":"Supports harmonizing Australia\'s regulatory efforts with similar frameworks and regulation being developed overseas","values":["Trust","Ethical usage","Transparency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulation, invest in AI research and development"},{"entity":"Businesses","role":"Implement ethical AI practices, comply with regulations"},{"entity":"AI Safety Commissioner and Agency","role":"Provide guidance, monitor AI developments, support regulators and policymakers"}],"sector_impacts":[{"sector":"Economy","impact":"Predicted productivity benefits of over $315 billion to the Australian economy by 2028"},{"sector":"Public sector","impact":"Increased regulatory requirements and standards when implementing AI"}],"quotes":["Governance Institute\'s members welcome the opportunity to make this submission on the critical issue of ensuring safe and responsible use of AI.","A \'risk based\' approach to AI regulation is the best way to ensure safe usage and community trust in this technology.","Government should significantly escalate investment in research and development relative to AI to ensure Australia does not miss this economic opportunity."]},"345_Optus AI Submission - Public.20c8fb27cad2e.pdf":{"organization_name":"Optus","organization_type":"Telecommunications company","classification":"Proponent","overall_position":"Supports principles-based regulation, emphasizing review of existing frameworks before introducing new AI-specific laws","arguments":["Many AI risks can be managed through existing laws and regulations","Organizations are already incentivized to establish robust governance mechanisms for AI","AI is still in its infancy and policy implications are just beginning to be understood"],"counterarguments":["Some new regulation will likely be needed to address specific issues","AI technology creates novel policy questions in some areas"],"key_recommendations":["Review the extent to which current AI risks can be regulated through existing frameworks","Any additional regulation should be principles-based","Emphasize the transformative potential of AI while ensuring appropriate safeguards","Initial broad AI regulations should be voluntary in nature"],"risks_and_challenges":["Generation of deepfakes to influence democratic processes or spread misinformation","Exacerbation of discrimination due to algorithmic bias","Monopolistic practices in the control of key data-sets","Obscured decision-making processes in automated systems"],"safeguards_and_mitigations":["Review and adapt existing legislation to manage AI use","Establish robust governance mechanisms and safeguards for AI use","Improve AI literacy to support responsible AI practices"],"examples":{"Optus Contact Centre":"Optus has partnered with Google to deploy AI technology as part of our customer contact centre. Using Google Cloud technology, Optus\'s AI Assistant draws on advanced modelling to enable accurate and meaningful interactions with customers.","Optus Call Stop":"Optus has worked with the Australian Financial Crimes Exchange and the banking industry to introduce \'Call Stop\', an initiative that will help stop financial scammers from using call-back scams."},"international_alignment":"Supports internationally-aligned standards","values":["Innovation","Responsible use","Transparency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop policy, lead public understanding of AI benefits"},{"entity":"Industry","role":"Implement responsible AI practices, collaborate with government"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential for profound medical advances through AI analysis of medical data/imagery"},{"sector":"Economy","impact":"Estimated economic benefit between $1 and $4 trillion in the next ten years"}],"quotes":["Optus supports the government\'s ambition to harness the huge productivity potential of artificial intelligence whilst ensuring that appropriate safeguards are established.","A closer, more collaborative relationship with government is therefore crucial for building a better shared understanding of AI itself and, consequently, the best policy settings to guide its appropriate use.","Government should emphasise the ways in which it can support and incentivise innovation and the public goods that can result from AI technology."]},"476_Submission 476 - Human Technology Institute UTS - 9-Aug.6238b699d46a.pdf":{"organization_name":"Human Technology Institute","organization_type":"Academic institute","classification":"Proponent","overall_position":"Supports comprehensive AI regulation with a balanced approach","arguments":["Effective regulation can build public trust and enable benefits of AI","Well-conceived legal guardrails can protect the community while fostering innovation","Current self-regulatory measures have had limited impact on changing behaviors"],"counterarguments":["Regulation is sometimes held out as the enemy of innovation","Poorly drafted laws can have a net negative impact on innovation"],"key_recommendations":["Develop an AI regulatory strategy for Australia","Implement proposals from recent landmark reviews related to AI","Establish an AI Commissioner to provide independent expert advice","Develop an AI assurance framework for the private sector","Adopt framework legislation for AI (Australian AI Act)"],"risks_and_challenges":["Low levels of community trust in AI","Cybersecurity and data-sharing risks","Deskilling and technological unemployment","Threats to human rights"],"safeguards_and_mitigations":["Risk-based regulatory approach","AI assurance frameworks","Legal gap analysis focused on areas of significant risk"],"examples":{"NSW AI Assurance Framework":"The NSW AI Assurance Framework is the first mandatory formal government policy in Australia to promote the responsible and ethical development and use of AI systems by government.","Robodebt Royal Commission":"The Robodebt Royal Commission report makes clear that the problems associated with that system did not stem solely from technical failures.","Clearview AI and Australian Federal Police privacy determinations":"In 2021, the Information and Privacy Commissioner determined that Clearview AI had breached Australian privacy law by collecting Australians\' biometric information from the internet without their consent and disclosing it through a facial recognition service."},"international_alignment":"Supports ensuring parity of legal protections for Australians compared to EU citizens, but not replicating EU AI Act structure","values":["Human rights","Responsible innovation","Accountability"],"tone":"Cautionary optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulation strategy"},{"entity":"Industry","role":"Comply with AI assurance frameworks and regulations"},{"entity":"AI Commissioner","role":"Provide independent expert advice on AI"}],"sector_impacts":[{"sector":"Public sector","impact":"Improved governance and accountability in AI use"},{"sector":"Private sector","impact":"Clearer guidelines and expectations for responsible AI development and use"}],"quotes":["Community trust is especially important where AI is used in high-stakes decision making.","Where clear legal guardrails promote safe and responsible innovation, and the law provides for readily available forms of redress when technology is misused or otherwise results in harm, community confidence around the safety and benefits of technology will tend to improve.","Australia has the opportunity to take an economy-wide regulatory approach to AI. Such an approach could help ensure that our law is effective, coherent and innovation-enhancing, while also safeguarding against risks of harm."]},"158_Submission Safe and responsible AI Dept Industry Science and Resources Jul2023.ac57b8fc35dd8.pdf":{"organization_name":"Centre for Social Impact, Flinders University","organization_type":"University research center","classification":"Proponent","overall_position":"Supports a risk management approach to AI regulation with focus on protecting marginalized communities","arguments":["AI and ADM have potential for both positive and negative impacts on Australians","Marginalised people have limited ability to understand and challenge AI systems","Risk management approach should focus on individuals impacted by AI"],"counterarguments":["null"],"key_recommendations":["Target risk management to assist individuals most unfairly impacted by AI","Confirm human-centredness as the central focus of AI risk management","Remove high impacts from medium risk category","Adopt the EU AI Act risk level framework","Draft legislation for high-risk AI","Create an independent body for AI complaints","Establish a specialist multidisciplinary ethics panel/forum for AI"],"risks_and_challenges":["AI and ADM pose significant threats to disadvantaged people","Medium risk category is too broad and includes high-impact risks","Lack of legal recourse for people damaged by AI"],"safeguards_and_mitigations":["Legislative control for high-risk AI","External redress process for high-risk ADM failures","Independent complaint process for high-risk AI"],"examples":{"Robodebt Scheme":"Robodebt was not an artificial intelligence (AI) system. It completed data matching and ADM without this new technology. AI now generates content, forecasts and recommendations for us. AI adds a layer of sophistication to ADM, turbo charging the ability of industry and governments to gather, process and make decisions based on very large data sets."},"international_alignment":"Supports alignment with emerging international trends, particularly the EU AI Act","values":["Human-centredness","Protection of marginalized communities","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Regulate AI and establish complaint processes"},{"entity":"AI owners","role":"Manage risks associated with AI systems"}],"sector_impacts":[{"sector":"Social services","impact":"Potential for negative impacts on disadvantaged individuals if AI is not properly regulated"}],"quotes":["Our focus is on the importance of safeguarding marginalised people and communities from the effects of AI.","The coming together of AI and ADM creates a new and significant risk for marginalised people and communities, on a scale that even Robodebt couldn\'t achieve.","Human centredness be confirmed as the central focus of the AI risk management approach."]},"223.pdf":{"organization_name":"Visa","organization_type":"Financial services company","classification":"Proponent","overall_position":"Supports a flexible, outcomes-focused regulatory framework for AI","arguments":["Existing laws can be applied or adapted to AI in many cases","Technology-neutral and outcomes-based regulation is most effective","Regulation should focus on outcomes of AI models rather than technical specifics"],"counterarguments":["New AI-specific regulations may be necessary to address unique challenges","A \'one size fits all\' approach to AI regulation is not recommended"],"key_recommendations":["Conduct a gap analysis of existing legislation before creating new AI regulations","Ensure AI regulation is adaptable to changing technologies","Focus on outcomes such as accountability, bias mitigation, and sustainable innovation"],"risks_and_challenges":["Potential for conflicting or redundant regulations","Risk of stifling innovation with overly prescriptive rules"],"safeguards_and_mitigations":["Use of advanced algorithms and neural networks for fraud prevention","Implementation of AI-powered security measures","Development of industry-led standards and codes of conduct"],"examples":{"Visa Advanced Authorisation":"These efforts have been effective, resulting in the prevention of approximately AU$39.3 billion in fraud globally in 2022 alone.","Visa\'s Smarter Stand-in Processing":"Uses AI to enable transaction authorisation if clients\' systems are down, thereby enhancing operational resilience across the network.","Visa Consumer Authentication Services":"A total of 95% of transactions can be authenticated in the background, without the need for the consumer to take additional steps, creating a smooth customer experience while reducing fraud and false declines."},"international_alignment":"Supports international alignment and cooperation on AI governance","values":["Trust","Innovation","Accountability"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Develop flexible, outcomes-focused regulatory frameworks"},{"entity":"Industry","role":"Implement responsible AI practices and contribute to standards development"}],"sector_impacts":[{"sector":"Financial services","impact":"Enhanced fraud prevention and operational resilience"},{"sector":"E-commerce","impact":"Improved transaction authentication and customer experience"}],"quotes":["Balancing trust with innovation is at the core of everything we do.","We believe that responsible data practices foster responsible AI innovation.","Visa believes that a holistic review of existing sectoral regulation can help to ensure consistency of approaches and avoid unnecessary cumulative or duplicative regulation."]},"390_Safe and Responsible AI in Australia Submission.2dff8a64a8c1e.pdf":{"organization_name":"Digital Service Providers Australia New Zealand (DSPANZ)","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports stronger regulatory approach towards AI while allowing for international interoperability and innovation","arguments":["Appropriate protection around AI technology is necessary to encourage confident and robust participation in the digital economy","AI regulations may need to come alongside other legislative changes to foster innovation","Many DSPs agree that Australia should take a stronger regulatory approach towards AI"],"counterarguments":["The existing legislative environment is viewed as a barrier to investing in AI","Current speed of legislative change and uncertainty around upcoming changes makes it difficult for DSPs to know if they will fall on the right side of regulatory change"],"key_recommendations":["Follow European Union, United States, and United Kingdom approaches to regulating AI","Embed co-design and international interoperability in any new regulations for AI","If mandated risk-based approach is chosen, undertake further consultation"],"risks_and_challenges":["Existing legislation (e.g., Tax Agent Services Act 2009) prevents many tax, accounting and payroll DSPs from providing AI-automated processes","Uncertainty around regulatory changes may slow the rate of innovation","Without legislative changes, the business software industry and many Australian businesses may be left behind"],"safeguards_and_mitigations":["Work alongside industry to prevent unintended consequences from new regulations","Provide education and resources for different sectors to utilize alongside any risk-based approach for AI","Include information about security around AI and machine learning security in risk-based approach"],"examples":{"Tax Agent Services Act limitation":"The Tax Agent Services Act 2009 (TASA) prevents many tax, accounting and payroll DSPs from being able to provide these valuable prompts and other AI-automated processes as they could be categorised as providing tax advice","Microsoft 365 AI integration":"Introducing AI-driven services to platforms such as Microsoft 365 later this year will see many businesses using AI in everyday processes","Global Talent program visa":"Our members have also raised that while AI specialisation qualifies under the Global Talent program visa, streamlining visa processes for Australia to acquire more skills would help accelerate the utilisation of AI"},"international_alignment":"Supports following European Union, United States, and United Kingdom approaches to regulating AI","values":["Innovation","International interoperability","Transparency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop regulations and consult with industry"},{"entity":"Digital Service Providers","role":"Implement AI technologies and comply with regulations"}],"sector_impacts":[{"sector":"Business software industry","impact":"Potential for increased innovation and improved services if regulatory barriers are addressed"},{"sector":"Small businesses","impact":"Potential for increased AI adoption and improved processes"}],"quotes":["DSPANZ supports the government exploring options to mitigate risks and support safe and responsible artificial intelligence (AI) practices.","While DSPs see opportunities to leverage AI, they also view the existing legislative environment as a barrier to investing in AI","If the government decides on a mandated risk-based approach, we recommend further consulting on the contents of the risk-based approach and when organisations will be expected to start following it."]},"229_RIAA submission_Australia_Responsible AI_FINAL.859fc17feac3.pdf":{"organization_name":"Responsible Investment Association Australasia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports stronger governance frameworks and rules for the ethical application of AI","arguments":["Stronger governance frameworks will give greater confidence to the business and investor community","Regulation will help mitigate extreme and material risks associated with AI use","Responsible AI practices can unlock significant potential for future revenue and growth"],"counterarguments":[],"key_recommendations":["Encourage public reporting of significant AI-related incidents and management","Integrate AI-related risks and opportunities into corporate governance guidelines","Invest in initiatives focusing on collaboration, training, and awareness raising"],"risks_and_challenges":["Privacy and Data Protection","Freedom of Expression and Opinion","Conflict & Security","Discrimination","Political Participation","Child Rights","Just Transition \u2013 impact on livelihoods"],"safeguards_and_mitigations":["Develop an investor toolkit on digital technology and human rights","Encourage consistent reporting and comparability across the market","Coordinate within and across industries to manage AI-related risks"],"examples":{"Investor-led initiatives":"We note significant global investor-led initiatives in this area, including that by the Investor Alliance for Human Rights and Hermes Investment Management\'s Investor Expectations on Responsible AI and Data Governance.","Australian initiatives":"The Sustainable Digitalisation Project, RIAA member, Alphinity Investment Management\'s, partnership with CSIRO for the responsible application of AI"},"international_alignment":"null","values":["Responsible investment","Ethical application of AI","Human rights"],"tone":"Positive","stakeholders":[{"entity":"Investors","role":"Support stronger governance frameworks and assess human rights risks in AI investments"},{"entity":"Government","role":"Convene multi-stakeholder forums and develop regulations"},{"entity":"Businesses","role":"Manage AI-related risks and opportunities responsibly"}],"sector_impacts":[{"sector":"Business","impact":"Potential to make businesses more efficient, reduce costs, and generate revenue from new or enhanced products and services"},{"sector":"Environment","impact":"Potential to help solve complex environmental challenges such as climate change"}],"quotes":["We believe stronger governance frameworks and rules for the ethical application of AI will give greater confidence to the business and investor community and unlock significant potential for future revenue and growth.","We support the use of regulation to define governance structures and reporting requirements for companies, both AI companies and companies that uses AI products and service, to mitigate the most extreme (i.e. existential) and material risks associated with the use of AI, in consultation with business, investors, civil society, academia and other key stakeholders.","We believe regulation should support businesses to manage extreme risks associated with AI appropriately but should also leave space for innovation, testing and trials in consultation with business, investors, civil society, academia and other key stakeholders."]},"278_TAS submission to Aus Gov consultation on supporting responsible AI (Wed 26 July 2023).0d4d143daa00b.pdf":{"organization_name":"Trusted Autonomous Systems Defence Cooperative Research Centre","organization_type":"Research center","classification":"Proponent","overall_position":"Supports development of better regulatory approaches for AI-enabled autonomous systems","arguments":["Current regulatory frameworks are not suited to features and risks of new technologies","Lack of suitable regulatory pathways can delay or prevent innovation","Better regulatory approaches will support safe and trusted design, manufacture, and use"],"counterarguments":["null"],"key_recommendations":["Implement overarching regulatory principles to support harmonised regulatory development","Adopt a whole-of-government approach to implementing risk-based regulation for AI","Establish initiatives to foster co-design between government and non-government organizations","Consider adopting a set of ten regulatory principles for autonomous systems"],"risks_and_challenges":["Current regulations rely on flexibility mechanisms like exemptions, which can be ad hoc and opaque","Lack of clear safety benchmarks or compliance pathways for autonomous systems","Potential for firms to move offshore due to unfavorable regulatory environment"],"safeguards_and_mitigations":["Develop industry-led standards, policies and practices","Create regulatory tools and initiatives to bridge regulatory gaps","Establish independent, world-class certification pathways"],"examples":{"Autonomous vessels in maritime domain":"Autonomous vessels \u2013 both surface and subsurface - are being used in a range of settings.","Regulatory challenges for autonomous vessels":"Elements of the framework \u2013 from legislation through to technical standards \u2013 are impracticable or impossible for autonomous vessels to meet, or are not relevant to autonomous vessel operations."},"international_alignment":"Supports consistency with international trends towards principles-based approaches","values":["Trust","Safety","Innovation"],"tone":"Constructive","stakeholders":[{"entity":"Government","role":"Provide strategic direction and advance legislative amendments"},{"entity":"Regulators","role":"Implement risk-based approaches and upskill personnel"},{"entity":"Industry","role":"Participate in regulatory co-design"}],"sector_impacts":[{"sector":"Defence","impact":"Critical for building sovereign capability in AI-enabled autonomous systems"},{"sector":"Transport","impact":"Increased safety, efficiency, and sustainability"}],"quotes":["AI-enabled autonomous technology is in regular use in the air, maritime and land domains, for government, commercial, research and defence purposes, and is increasingly capable and accessible.","Better regulatory approaches and infrastructure are needed to support safe and trusted design, manufacture and use.","Implementing a set of overarching regulatory principles will support harmonised, best practice domestic and international regulatory development."]},"405_letter1.040a48556c0d6.pdf":{"organization_name":"School of Computing and Information Systems, The University of Melbourne","organization_type":"Academic institution","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and broader consideration of AI risks","arguments":["Current government approach is too narrowly focused on immediate risks","Potential future harms from AI should be investigated and addressed","Responsible AI management requires acknowledging and navigating the full scale of risks"],"counterarguments":["Some experts are skeptical or dismiss potential catastrophic harms from AI","Uncertain future risks might be seen as less important than clear and immediate risks"],"key_recommendations":["Fund interdisciplinary research on potential harms from future AI systems","Adopt an agile regulatory stance to address new risks as they emerge","Use Australia\'s diplomatic standing to spearhead global coordination on international-level AI risks"],"risks_and_challenges":["Harms from interactions with adversarial autonomous AI agents","Existential risks from uncontrollable AI systems","Unknown pathways to harm that may arise rapidly and unpredictably"],"safeguards_and_mitigations":["Investigate potential harms in proportion to their plausibility and scale","Reduce uncertainty about potential harms","Reduce the chance of potential harms being actualized"],"examples":{"Experts expressing concern":"Geoffrey Hinton, Yoshua Bengio, and Stuart J. Russell have expressed concern about catastrophic and existential risk from AI","Statement on AI Risk":"The Center for AI Safety\'s statement signed by hundreds of AI experts expressing concern about AI risk","null":"null"},"international_alignment":"Supports global coordination on addressing international-level risks from AI systems","values":["Responsibility","Safety","Proactive risk management"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Lead in acknowledging and responsibly navigating the full scale of risks from AI systems"},{"entity":"Academic researchers","role":"Identify, understand, and reduce risks of harm to society from advanced intelligent systems"}],"sector_impacts":[{"sector":"Labour market","impact":"null"},{"sector":"National security","impact":"null"}],"quotes":["I implore the Government to broaden its definitions and taxonomies of risk in anticipation of additional pathways to future harm","When facing uncertainty about potential future harms, the responsible thing to do is to investigate the potential harms in proportion to their plausibility and their scale.","I implore the Government to take this opportunity to lead the world in acknowledging and responsibly navigating the full scale of risks from AI systems."]},"440_Responsible AI - 00 Cover Letter (Goodwin).6ad3de530284b.pdf":{"organization_name":"University of Melbourne","organization_type":"Academic institution","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and governance of AI, particularly in higher education and HASS disciplines","arguments":["HASS disciplines are well-positioned to coordinate strategies for addressing AI challenges","There is an urgent need for human-centered responses to technological instability","AI systems will have multifarious impacts on Australian society in the near future"],"counterarguments":[],"key_recommendations":["Reduce data and algorithmic bias","Safeguard cultural data and institutional knowledge","Investigate historical inequality and under-representation in technological platforms","Advise on sustainable data management","Reduce exploitation of personal data","Compensate individuals for their data when used, shared, or sold"],"risks_and_challenges":["Increasingly virtualised, anonymous and synthetic interactions","Fragmentation of work-life and deterioration of work futures","New territories of exploitation and uncertainty in private and commercial spaces","AI imposters seeding disinformation and inflammatory discourse","Exploitation of those without means or access to new technologies","Further marginalisation of vulnerable and under-represented community members"],"safeguards_and_mitigations":["Coordinate HASS disciplines to strategize human responses to AI challenges","Design contingency strategies for two and ten years into the future","Interdisciplinary consultation and complementary policy matrices"],"examples":{},"international_alignment":"null","values":["Equity","Inclusivity","Human-centeredness","Safety","Respect","Dignity"],"tone":"Cautionary","stakeholders":[{"entity":"Higher Education sector","role":"Coordinate disciplines to address AI challenges"},{"entity":"HASS disciplines","role":"Lead in strategizing human responses to AI challenges"},{"entity":"Young people","role":"Central to assessment and mitigation efforts"},{"entity":"Institutions","role":"Central to assessment and mitigation efforts"}],"sector_impacts":[{"sector":"Higher Education","impact":"Potential influence on teaching and learning practices, especially in HASS disciplines"},{"sector":"Labor market","impact":"Automation, casualisation, and reduced rights in the gig economy"}],"quotes":["I view recent developments in artificial intelligence with great anticipation for its unquestionable benefits. However, I also hold grave concerns for students and educators in this period of rapid unregulated experimentation and expeditated deployment","The Humanities, Arts and Social Sciences (HASS) are well-placed to coordinate their respective disciplines and methodologies to strategize the human response to the challenges that lie ahead in terms of emergent forms of AI, synthetic media, information manipulation and their associated socio-economic and socio-political influences.","We should design two contingency strategies \u2013 for two years and ten years into the future, as we cannot be certain which will come first!"]},"ADG ASDACS Submission to Safe and Responsible AI Discussion in Australia final 26_7_23.78dd11a88b45.pdf":{"organization_name":"Australian Screen Directors Authorship Collecting Society (ASDACS) and Australian Directors Guild (ADG)","organization_type":"Industry associations","classification":"Proponent","overall_position":"Supports greater mandatory regulation and copyright protection for AI use of creative works","arguments":["AI platforms are breaching existing copyright laws","AI threatens the livelihood of creative rightsholders","Current practices undermine government policies to support the creative industries"],"counterarguments":["AI presents significant opportunities for economic and social progress in certain industries"],"key_recommendations":["Require AI applications to obtain licenses and permissions from rightsholders","Establish a statutory collection model for royalties on copyrighted works used by AI","Mandate source identification and attribution for copyrighted material used by AI","Empower rightsholders to enforce copyright through takedown notices and financial penalties"],"risks_and_challenges":["Displacement of human creators by AI","Homogenisation of culture","Exploitation of copyrighted works without permission or compensation","Disincentivizing creation of new works due to copyright infringement risks"],"safeguards_and_mitigations":["Mandatory regulation for AI use of copyrighted works","Statutory collection model for royalties","Blockchain technology to track exploitation of works","Small claims tribunal for copyright infringements"],"examples":{"US copyright lawsuits":"In the US, copyright lawsuits are fast emerging with authors such as Sarah Silverman and stock photo company Getty Images pursuing cases against AI platforms such as Open AI, Meta and Stable Diffusion for breach of copyright","Economic contribution of Australian screen industry":"The results of the most recent Australian Bureau of Statistics (ABS) survey on Film, Television and Digital Games covering the 2021/22 financial year revealed that the industry employs over 55,000 people and contributed over $6 billion to the Australian economy."},"international_alignment":"null","values":["Copyright protection","Fair remuneration","Attribution and transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Creative rightsholders","role":"Protect their copyrighted works and receive fair compensation"},{"entity":"AI platforms","role":"Obtain licenses and permissions for use of copyrighted works"},{"entity":"Government","role":"Enforce copyright laws and establish regulatory frameworks"}],"sector_impacts":[{"sector":"Creative industries","impact":"Threatens sustainable careers and undermines government cultural policies"},{"sector":"AI industry","impact":"Potential limitations on use of copyrighted materials for training and output"}],"quotes":["The complete homogenisation of culture, displacement of talent and new ideas, and ultimately the demise of the creative industries is at risk.","These platforms breach existing copyright laws. They also violate the inalienable moral rights of authors and creators regarding acknowledgement and attribution of works and avoid compensating creators for the works that AI platforms rely on to be useful and appealing on a large scale.","The Australian creative industries urgently require greater clarity and mandatory copyright regulation on the use of existing works under copyright to ensure continued support and growth for the creative sector."]},"355_2023_07_25_Submission_DoISR_Responsible AI_Fnl.c4efd6256e9dd.pdf":{"organization_name":"Actuaries Institute","organization_type":"Professional body","classification":"Neutral","overall_position":"Supports outcome-focused regulation rather than AI-specific laws, prioritizing guidance on existing regulations","arguments":["Existing regulations often already cover AI/ADM decisions","Technology-specific regulation may create inconsistencies and loopholes","Outcome-focused regulation is more adaptable to technological changes"],"counterarguments":["Some argue AI/ADM are new and special, requiring specific rules","Existing regulations may have uncertainties when applied to AI"],"key_recommendations":["Regulation should primarily be outcome focused, rather than technology focused","Produce guidance on existing regulation before creating new regulation","Create a centralised expert body for AI governance and regulation assistance","Develop a well-defined taxonomy of AI risks and menu of risk-management options"],"risks_and_challenges":["Overlaps and inconsistencies between AI/ADM regulation and other regulations","Gaps in regulation due to evolving technology","Loopholes that can be exploited by bad actors","Inconsistent application of vague risk categories"],"safeguards_and_mitigations":["Clarify existing regulations for AI applications","Develop a comprehensive framework for assessing AI risks","Carefully target risk-management interventions to specific situations","Increase AI literacy across all segments of the population"],"examples":{"Facial recognition technology in retail":"Recent debate about the use of facial recognition technology by retailers is at least in part a debate about how to interpret aspects of Australia\'s Privacy Act.","AI in hiring":"An AI system used in hiring (Box 4 \u2013 \'medium risk\') could involve various risks, like unfairness, discrimination, or privacy concerns.","Autonomous vehicles":"Use of AI in safety-related car components and in self-driving cars to make real-time decisions"},"international_alignment":"null","values":["Proportionality","Clarity","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Create regulatory clarity around AI/ADM"},{"entity":"Regulators","role":"Review existing regulations in the context of AI/ADM and issue guidance"},{"entity":"Industry and professional bodies","role":"Assist in identifying areas needing clarity and suggest solutions"}],"sector_impacts":[{"sector":"Financial services","impact":"Unclear regulation can be a barrier to innovation"},{"sector":"Insurance","impact":"Need for guidance on AI and discrimination in insurance pricing"}],"quotes":["Regulation should primarily be outcome focused, rather than technology focused to help ensure it can be enduring/long lasting","It is essential that guidance which is intended to be applied in a mathematical context such as AI/ADM avoids such uncertainty.","We consider that the immediate regulatory action required to allow greater safety and adoption of AI/ADM is to clarify the operation of existing regulation."]},"510_Submission 510 - Australian Federal Police - 1-Sept.22292bd1a4425.pdf":{"organization_name":"Australian Federal Police","organization_type":"Government law enforcement agency","classification":"Proponent","overall_position":"Supports responsible AI adoption with robust governance and oversight","arguments":["AI is necessary for law enforcement to combat evolving criminal threats","Responsible AI use can create operational efficiencies and improve decision-making","Collaboration with partners is crucial for effective AI implementation"],"counterarguments":["AI poses risks if not properly governed and overseen","Human judgment and accountability must be maintained in policing"],"key_recommendations":["Develop an AI Oversight Framework","Strengthen partnerships with academia, industry, and international law enforcement","Enhance workforce training on AI technologies"],"risks_and_challenges":["Increased potency and accessibility of AI-enabled criminal activities","Exploitation of human vulnerabilities through AI","Potential for deliberate sabotage of critical algorithms"],"safeguards_and_mitigations":["Implementing robust governance and accountability measures","Ensuring human oversight in AI-assisted decision-making","Regular audits and updates of AI systems"],"examples":{"Deepfakes":"Deepfakes can and have been used to discredit public figures, extort funds, and influence democratic processes.","AI-generated child exploitation material":"The creation of AI generated (fake) child exploitation material is not only illegal but also encourages and normalises the abuse of real children and diverts law enforcement resources invested in victim identification.","AI in banking sector":"AI\'s analytical capability is harnessed to detect and prevent fraudulent activities in real-time."},"international_alignment":"Supports collaboration with international partners while respecting differing legal and societal frameworks","values":["Accountability","Transparency","Responsibility"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Law enforcement agencies","role":"Responsible adoption and use of AI technologies"},{"entity":"Academia","role":"Research and development of ethical AI frameworks"},{"entity":"Industry partners","role":"Collaboration in AI development and implementation"}],"sector_impacts":[{"sector":"Law enforcement","impact":"Enhanced capabilities to combat technology-facilitated crime"},{"sector":"Banking","impact":"Improved fraud detection and prevention"}],"quotes":["The AFP recognises AI is another tool, and it will not replace the requirement a human must remain accountable for any decision that affects on the rights of another human.","The AFP supports ongoing engagement with the community to ensure Australia has the right governance settings to respond to the rapid development of AI.","By forging strong and lasting collaborations with our partners and community stakeholders, we can harness the potential of AI in policing."]},"275_Submission JMBV - Supporting responsible AI - July 2023.e0ce363d101c4.pdf":{"organization_name":"ARC Centre of Excellence for Automated Decision-Making and Society","organization_type":"Research institution","classification":"Proponent","overall_position":"Supports differentiated approach for public and private sector AI regulation","arguments":["Differentiated approach between private and public sector is beneficial in the short term","Pre-emptive regulation for public sector use of AI is necessary","Reactive regulation for private sector AI use is more suitable at this stage"],"counterarguments":["Pre-emptive regulation may hinder innovation in the private sector","Strict pre-emptive rules for private sector may require international coordination"],"key_recommendations":["Develop pre-emptive regulation for AI use in the public sector","Consolidate reactive regulation for AI use in the private sector","Adapt sector-specific legislation to AI systems","Pass horizontal legislation for liability context in AI"],"risks_and_challenges":["Difficulty in predicting impacts of AI at an early stage","Potential hindrance to innovation with strict pre-emptive regulations","Lack of significant market and extensive AI industry in Australia"],"safeguards_and_mitigations":["Develop specific rules for public sector AI systems","Implement adequate testing and certification systems","Conduct proper risk analysis for public sector AI systems","Establish specific monitoring rules adapted to risk levels"],"examples":{"NSW AI Assurance Framework":"Learning from the New South Wales experience for AI (NSW AI Assurance Framework)","Canadian Treasury Board Directive for ADM":"Learning from the Canadian Treasury Board Directive for ADM (ADM Directive) as the two main regulatory systems that can be extrapolated to the Australian federal level"},"international_alignment":"Supports a coordinated approach with Canada as an alternative model for other countries","values":["Transparency","Accountability","Human rights"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Regulate and implement AI systems in public sector"},{"entity":"Private sector","role":"Develop AI systems compliant with public sector rules"}],"sector_impacts":[{"sector":"Public sector","impact":"Higher standards for AI system deployment and use"},{"sector":"Private sector","impact":"Potential trickle-down effect of public sector AI practices"}],"quotes":["There is a strong case to be reactive in the regulation of AI systems deployed in the private sector.","Reactive regulation, however, will not be desirable in the public sector for several reasons.","The idea of transparency as a requirement for a trustable AI has been thrown around with limited consideration to its meaning from a regulatory point of view"]},"167_Matulionyte_safe and responsible AI consultation_FINAL.65215c33d4e81.pdf":{"organization_name":"Macquarie Law School, Macquarie University","organization_type":"Academic institution","classification":"Proponent","overall_position":"Supports implementing comprehensive AI regulation through both sector-specific and horizontal legislation","arguments":["Transparency is critical for improving quality and safe use of AI/ADM technologies","A risk-based approach to AI transparency is necessary","Both sector-specific and horizontal AI legislation are needed to implement transparency"],"counterarguments":[],"key_recommendations":["Implement transparency duties in both public and private sectors","Adopt a risk-based approach to AI transparency","Revise existing instruments and introduce new measures to achieve transparency","Address barriers to effective transparency, such as trade secrets","Update copyright laws to address challenges posed by AI development"],"risks_and_challenges":["Lack of consensus on the definition of \'AI transparency\'","Potential information overload leading to \'transparency fallacy\'","Trade secrets as a barrier to effective transparency","Copyright infringement in AI training processes"],"safeguards_and_mitigations":["Establish clear definitions for \'AI transparency\' and related terms","Set appropriate and achievable goals for AI/ADMS transparency","Identify stakeholders who need information about AI and tailor transparency measures accordingly","Define the nature and scope of information to be provided based on risk level"],"examples":{"O\'Brien decision":"In O\'Brien decision, trade secrets were one of the main reasons why an individual was rejected access to information about the algorithm that denied them social housing benefits.","UK Algorithmic Transparency Recording Standard":"The UK Algorithmic Transparency Recording Standard lists public scrutiny, accountability and trust as the three primary goals of algorithmic transparency.","EU AI Act":"The draft EU AI Act requires minimum information about low-risk AI systems (letting users know about the system) and sets high transparency duties for high-risk systems (e.g. registering in the EU AI Register)."},"international_alignment":"Supports consideration of international experiences and standards","values":["Transparency","Accountability","Trust"],"tone":"Cautionary","stakeholders":[{"entity":"Government agencies","role":"Implement transparency measures and revise existing legislation"},{"entity":"Private sector","role":"Comply with transparency requirements"},{"entity":"Independent experts","role":"Conduct public scrutiny of AI technologies"}],"sector_impacts":[{"sector":"Healthcare","impact":"Need for specific transparency rules due to high-risk nature of AI applications"},{"sector":"Law enforcement","impact":"Requirement for detailed transparency measures due to significant impacts on human rights"}],"quotes":["Transparency around AI and automated decision making (ADM) is critical, both in private and public sectors. If properly implemented and enforced, transparency has a potential to improve quality and safe use of AI/ADM technologies and ensure accountability when AI is developed or used inappropriately.","More transparency is needed when the risk/impact of the AI/ADM system is higher, and less transparency is needed when it is lower.","Transparency is not an end in itself. Transparency can serve many purposes, such as increasing trust in AI, ensuring contestability of decisions produced with the help of AI, enabling external scrutiny and quality assurance of AI, and accountability by responsible stakeholders (developers, users of AI)."]},"337_RHCA Submission.140b34fae2bec.pdf":{"organization_name":"Ramsay Health Care Australia","organization_type":"Private healthcare company","classification":"Proponent","overall_position":"Supports developing general AI regulations with industry self-regulation for specific sectors","arguments":["AI policy development should be aligned across government departments","A lead agency should be responsible for Australia\'s general AI policy","Streamlining and simplifying regulation should be prioritized over introducing additional regulations"],"counterarguments":["Healthcare industry should self-regulate rather than having additional government intervention","Completely banning high-risk AI applications may slow innovation and productivity"],"key_recommendations":["Appoint a lead agency responsible for Australia\'s general AI policy","Harmonize and simplify existing legislation across all levels of government","Develop generic regulations with foundational principles applicable to all industries","Invest in building consumer AI literacy for all Australians","Implement a risk-based approach for addressing potential AI risks"],"risks_and_challenges":["Systematic or difficult to reverse impacts of AI","Observability and fact-checking of AI outputs","Protection of intellectual property rights","Transparency and accountability in data ownership","Potential for bias and harm from incomprehensive datasets"],"safeguards_and_mitigations":["Develop clear education and guidance to enhance Australia\'s AI practices","Implement appropriate data governance to prevent misuse and unauthorized access","Provide stakeholders with opportunities to contest and challenge AI-informed outcomes","Implement mandatory confidential reporting for high-risk AI applications"],"examples":{"Robodebt Scheme":"The recent Royal Commission into the Robodebt Scheme highlights the importance of the Australian Government setting the standard for the use of AI in Australia.","ChatGPT adoption rates":"Should ChatGPT adoption rates be the future, it is paramount users understand AI, its use, and potential implications.","NSW AI Assurance Framework":"RHCA recommends the Australian Government review the NSW AI Assurance Framework to determine whether it could be uplifted and be made an Australian Framework to assure AI projects against the ethics framework."},"international_alignment":"Supports engagement in international forums and alignment with major trading partners","values":["Transparency","Accountability","Privacy","Fairness","Human-centered AI"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement AI policy, regulations, and standards"},{"entity":"Private sector","role":"Collaborate with government, implement responsible AI practices"},{"entity":"National AI Centre","role":"Potential lead agency for Australia\'s general AI policy"},{"entity":"State and territory governments","role":"Collaborate for a nationally consistent approach to AI"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential for self-regulation with focus on high-risk mechanisms"},{"sector":"Education","impact":"Inclusion of AI in primary/secondary education syllabus"}],"quotes":["RHCA recommends the Australian Government harmonise and simplify existing legislation and regulations (Commonwealth, State, Territory) to ensure there is a consistent approach to AI, including related obligations and requirements.","RHCA strongly recommends the Australian Government invests in building consumer AI literacy for all Australians.","RHCA strongly supports transparency across the entire AI lifecycle."]},"313_R02629 Dept Industry AI consultation.7ccd72ed353c9.pdf":{"organization_name":"Copyright Agency","organization_type":"Not-for-profit company","classification":"Proponent","overall_position":"Supports responsible AI practices with emphasis on transparency, fair compensation, and protection of creators\' rights","arguments":["AI has potential to do good and reduce inequalities","There are risks associated with lack of transparency in AI training and outputs","Creators and publishers should be fairly compensated for content used in AI development"],"counterarguments":["null"],"key_recommendations":["Government should not use AI technologies developed without disclosure and compensation","Transparency requirements for use of content in AI development and outputs","Consider objectives of National Cultural Policy in AI governance"],"risks_and_challenges":["Adverse consequences for people in creative industries","Sustainability of quality content","Use of content without permission or compensation","Lack of transparency about AI training data and outputs"],"safeguards_and_mitigations":["Disclosure and compensation for use of content in AI development","Transparency requirements for AI training data and outputs","Consideration of National Cultural Policy objectives in AI governance"],"examples":{"Legal proceedings in other countries":"Legal proceedings in other countries.","UK Parliament\'s Communications and Digital Committee report":"We also note the risks recognised by the UK Parliament\'s Communications and Digital Committee, in its report \'At risk: our creative future\'."},"international_alignment":"Supports principles set out in international open letters to policymakers and AI leaders","values":["Transparency","Fair compensation","Protection of creators\' rights"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop responsible AI practices and demonstrate best practice"},{"entity":"Creators and publishers","role":"Receive fair compensation and protection for their content"}],"sector_impacts":[{"sector":"Creative industries","impact":"Potential adverse consequences and concerns about maintaining authenticity"},{"sector":"Education","impact":"Concerns about future quality content for Australian students"}],"quotes":["We and our members recognise the potential for AI to do good in a range of areas, including to reduce inequalities. At the same time, there is huge potential for adverse consequences for people who work in Australian creative industries, including educational publishing.","There are risks associated with lack of transparency regarding content used to train generative AI applications, and with the provenance of outputs. There are also risks associated with failure to compensate creators of content used to develop these applications.","The Government agencies should not use AI technologies that have been developed using other people\'s content without disclosure and without compensation."]},"303_Submission 303 - Attachment.6953f35287bec.pdf":{"organization_name":"XXX","organization_type":"null","classification":"Neutral","overall_position":"Supports integrating AI governance into existing systems and processes","arguments":["AI governance should reflect existing systems and processes","AI is just one tool of many that should sit within broader governance processes","Existing frameworks like ISO standards can be used to manage AI use"],"counterarguments":["People tend to look at AI through completely new lenses rather than using existing frameworks","The document is good at the \'build the car and roads\' part but missing components after that","AI products will constantly change in a production environment, unlike cars/machines/drugs"],"key_recommendations":["Use existing frameworks like ISO 38507 for AI governance","Develop broad AI literacy","Upskill internal public sector technical support","Provide hands-on opportunities to engage with AI in simple language"],"risks_and_challenges":["Lack of emphasis on the ubiquitous nature of AI","Potential gaps in existing regulatory approaches","Difficulty in keeping up with the depth and breadth required at the pace required in niche/expert fields"],"safeguards_and_mitigations":["Continuous monitoring and reporting of AI algorithms","Yearly review of KPIs and reporting for AI algorithms","Incorporation of AI risk management into existing governance processes"],"examples":{"Learner driver analogy":"We are teaching companies to responsibly use AI, the same as a learner driver.","Car and road safety analogy":"We also need to account for the people who ignore the \'road ends\' sign, continue on the dirt road, drive around the fence at the end (yes we need the fence), and drive of the cliff."},"international_alignment":"Recognizes the international nature of AI development and deployment","values":["Transparency","Responsible AI practices","Public trust"],"tone":"Cautionary","stakeholders":[{"entity":"Government agencies","role":"Implement responsible AI practices"},{"entity":"Private sector","role":"Implement responsible AI practices"},{"entity":"Regulators, certifiers, and auditors","role":"Oversee AI governance"}],"sector_impacts":[{"sector":"Public sector","impact":"Need for upskilling and external expertise"},{"sector":"Private sector","impact":"Potential administrative burden from over-regulation"}],"quotes":["AI Governance processes should reflect existing systems and processes as AI is just one tool of many that should sit within broader governance processes.","We are teaching companies to responsibly use AI, the same as a learner driver.","Critically there is no \'monitoring as the algorithm ages\', which is akin to \'phase 4 research or post marketing surveillance\' step."]},"159_OIC FINAL submission 250723.f5de8229a1a0d.pdf":{"organization_name":"Queensland Office of the Information Commissioner","organization_type":"Independent statutory body","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI","arguments":["AI poses risks not adequately addressed by existing regulation","Non-binding ethical frameworks are insufficient to protect privacy and rights","Strong regulatory frameworks are needed to foster public trust in AI"],"counterarguments":[],"key_recommendations":["Implement strong, enforceable regulatory responses to AI use","Adopt legislative restrictions modelled on the EU\'s GDPR","Create a binding whole-of-government code for responsible AI use"],"risks_and_challenges":["Algorithmic bias","Discrimination","Profiling","Surveillance","Re-identification of data","Ethical, transparency and accountability challenges in automated decision-making"],"safeguards_and_mitigations":["Adoption of GDPR-like provisions for automated decision-making","Implementation of a \'fair and reasonable\' test for AI regulation","Mandating transparency in AI-generated content and decision-making processes"],"examples":{"Robodebt":"This is cited as an example of the need for transparency in government use of AI","AI technology for detecting distracted drivers":"This is mentioned as another example highlighting the importance of transparency"},"international_alignment":"Supports adoption of international standards, particularly those from the EU","values":["Transparency","Accountability","Privacy","Trust","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Government agencies","role":"Implement responsible AI practices and comply with regulations"},{"entity":"Private sector","role":"Comply with AI regulations and ethical standards"}],"sector_impacts":[{"sector":"Government","impact":"Increased transparency and accountability in AI use"}],"quotes":["OIC maintains the above views, and continues to support the implementation of strong, enforceable responses to the use and implementation of AI technology.","The absence of sufficiently robust governance responses risks undermining community trust and confidence in the use of machine learning and AI and inhibiting the realisation of the potential benefits each technology may otherwise offer the community.","Transparency is crucial to fostering community trust in the use of AI technology."]},"461.pdf":{"organization_name":null,"organization_type":"Academic researchers","classification":"Neutral","overall_position":"Supports updating existing laws and addressing specific harms rather than creating AI-specific regulation","arguments":["Existing technology-neutral laws can serve as a foundation for mitigating harms","AI-specific regulation may not be adequate to address all potential harms","Focus should be on identifying and preventing harms rather than regulating AI specifically"],"counterarguments":["Some technologies may require specific regulation due to unique challenges","Existing laws may not fully address challenges posed by AI systems"],"key_recommendations":["Fill gaps in existing legislation and amend laws to clarify application to algorithmic systems","Provide regulatory guidance, education, and support to developers, procurers, and users of algorithmic systems","Increase regulator powers, funding, education, and resources to deal with challenges of algorithmic systems","Implement transparency requirements for AI-informed decision-making","Strengthen regulators\' and courts\' abilities to identify and address law breaches related to AI systems"],"risks_and_challenges":["Opacity in algorithmic decision-making processes","Potential for discrimination and bias in AI systems","Difficulty in identifying and proving when regulations have been breached","Lack of guidance for businesses on legal obligations related to AI use"],"safeguards_and_mitigations":["Mandatory notification when AI is materially used in decision-making processes","Strengthening regulators\' and courts\' abilities to investigate and enforce laws related to AI systems","Education and resources for businesses on legal obligations in AI contexts","Exclusion of AI models from trade secrets rules","Enhanced rights to explanation of credit scoring and insurance underwriting decisions"],"examples":{"State Farm lawsuit":"A recent class action lawsuit against a US insurer State Farm alleges that the insurer has discriminated against Black homeowners in how their claims have been considered, resulting in their claims taking longer to be approved than White homeowners\' claims.","Automated hiring systems":"Sheard\'s work on automated hiring systems explores the application of existing legislation to these technologies.","Tenancy screening technologies":"Applications such as Certn (a rental application platform available in the North American market) claim to use algorithms, including machine learning techniques to make assessments of rental applicants"},"international_alignment":null,"values":["Transparency","Accountability","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Provide guidance, education, and support; update legislation; increase regulator powers"},{"entity":"Businesses","role":"Comply with existing and updated regulations; implement responsible AI practices"},{"entity":"Regulators","role":"Enforce regulations; investigate and address law breaches related to AI systems"}],"sector_impacts":[{"sector":"Financial services","impact":"Potential for increased transparency in decision-making processes"},{"sector":"Housing","impact":"Need for regulation of algorithmic systems in tenant screening"}],"quotes":["It would be more beneficial to identify the harms we want to prevent now and in the future, and then decide if we need AI (or any other technology / social phenomenon)-specific regulation to address these harms.","The problem lies not really with the use of AI tools, and the issue of technical opacity of AI models (often referred to as \'black boxes\'), but rather business processes of corporations, and AI-focused regulation alone is unlikely to succeed in breaking the opacity.","The authors emphasise that voluntary principles and frameworks are no substitute to enforceable regulation."]},"181_2023 Safe and responsible AI - Twilio Submission.433230f34ebdd.pdf":{"organization_name":"Twilio","organization_type":"Global provider of cloud communications and customer engagement services","classification":"Proponent","overall_position":"Supports a balanced approach to AI regulation, favoring international consistency, technology neutrality, and risk-based systems","arguments":["International consistency in AI regulation facilitates global business and innovation","Technology-neutral approaches are more future-proof and focus on outcomes rather than specific technologies","Risk-based systems allow for appropriate regulation based on the actual risk of AI use cases"],"counterarguments":["null"],"key_recommendations":["Base definitions on ISO standards and maintain international alignment","Ensure cross-government coordination when developing AI approach","Incorporate international consistency, technology neutrality, risk-based systems, and functional responsibility in the regulatory framework","Adopt a precise definition of \'high-risk\' AI systems based on actual use case impact, not just sector"],"risks_and_challenges":["Overly broad or prescriptive definitions of \'high-risk\' AI scenarios","Inconsistency with global standards creating barriers for Australian businesses","Technology-specific policies becoming obsolete and hindering innovation"],"safeguards_and_mitigations":["Risk-based approach to regulation","Functional responsibility allocation based on entities\' position in the AI supply chain","Focus on real-world concerns rather than abstract concepts"],"examples":{"Low-risk AI use in critical sectors":"AI systems might be used to determine the time when a critical infrastructure business (e.g., an electricity provider) sends emails to customers on new products or services","High-risk AI use in benign systems":"where an IVR system is used in disaster or emergency scenarios to triage and route callers to first responders or other live agents depending on their needs"},"international_alignment":"Strongly supports international consistency and alignment with global standards","values":["Innovation","Business certainty","Proportionate governance"],"tone":"Positive and constructive","stakeholders":[{"entity":"Government","role":"Develop balanced and internationally consistent AI regulations"},{"entity":"Businesses","role":"Assess and manage AI risks based on their function in the supply chain"}],"sector_impacts":[{"sector":"Critical infrastructure","impact":"Potential for low-risk AI applications even in sensitive sectors"},{"sector":"Emergency services","impact":"Potential for high-risk AI applications in traditionally benign systems"}],"quotes":["Twilio supports DISR\'s definitions of \\"Artificial Intelligence (AI)\\", \\"machine learning\\", and \\"algorithm\\" being based on those from the ISO, as proposed in the Discussion Paper.","To achieve the Government\'s aims of encouraging innovation and industry engagement in this field, a level of business certainty is required in the overall legal and regulatory regime.","A definition of a high-risk AI system should be based on the risk of the actual use case having a severe impact on individuals\' lives. It should not be based solely on the sector or industry in which the use occurs."]},"146_24072023 Workday Responsible AI Submission (final).91e7dbd5faa82.pdf":{"organization_name":"Workday","organization_type":"Enterprise cloud applications provider","classification":"Proponent","overall_position":"Supports the development of safe and responsible AI policies, regulations and practices","arguments":["AI is powering the future of work by unlocking human potential and driving business value","A risk-based approach to AI governance maximises benefits and minimises risks","AI governance is a shared responsibility between developers and deployers"],"counterarguments":["null"],"key_recommendations":["Enhance regulatory coherence by examining the current domestic governance landscape","Adopt a risk-based approach to AI governance","Distinguish between AI developers and AI deployers in assigning obligations","Implement AI risk management programs","Focus on impact assessments rather than premature third-party audit requirements"],"risks_and_challenges":["Potential regulatory overlap","Lack of distinction between automated tools and tools with human in the loop","Uncertainty about which entity bears responsibility for compliance"],"safeguards_and_mitigations":["Risk-based approach to AI governance","Role-based obligations for AI developers and deployers","AI risk management programs","Impact assessments for high-risk AI tools"],"examples":{"Consequential decisions":"AI tool that is used for a decision about an individual\'s access to an essential opportunity where it has the potential to pose harm to that individual","High-risk AI use-cases":"AI tools used to hire, promote, or terminate an individual\'s employment, or determine access to credit, healthcare, or housing"},"international_alignment":"Supports alignment with leading frameworks in the EU and US","values":["Accountability","Transparency","Responsibility"],"tone":"Constructive and supportive","stakeholders":[{"entity":"AI developers","role":"Design, code, and produce AI tools"},{"entity":"AI deployers","role":"Operate and use AI tools, interact with end users"}],"sector_impacts":[{"sector":"Finance","impact":"AI applications for financial management"},{"sector":"Human Resources","impact":"AI applications for HR management"}],"quotes":["Workday supports the development of safe and responsible AI policies, regulations and practices that are meaningful, technically sound, and advance responsible innovation.","To navigate these dynamics, policymakers are converging on a risk-based approach to AI governance that maximises the benefits of AI and minimises the risks of potential harm.","AI governance is a shared responsibility between developers, which design, code, and produce an AI tool, and deployers, which operate and use the tool and interact with end users."]},"277_CRA\'s Submission to DISR\'s Consultation on Supporting Responsible AI.93553d0ef45a9.pdf":{"organization_name":"Cooperative Research Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports responsible AI development through coordinated governance and strategic initiatives","arguments":["Coordinated AI governance across government ensures coherent and responsible approach","Clear government guidance instils public confidence and facilitates responsible AI practices","Australia has a unique opportunity to lead in the AI revolution"],"counterarguments":["null"],"key_recommendations":["Coordination of AI governance across government","Foster research and infrastructure to build high-quality capability in AI","Host an AI International Convention/Global Summit","Adopt a risk-based approach to AI governance","Launch targeted public awareness campaigns"],"risks_and_challenges":["Cyber security vulnerability of AI models","Data quality, access, ownership and control issues","Potential fragmentation and incompatibility of AI development across countries","Uncertainty and potential misuse of AI in scientific research"],"safeguards_and_mitigations":["Integration with the 2023-2030 Australian Cyber Security Strategy","Data ownership transparency and audits/quality checks","Platform for collaboration between countries and standardisation of AI regulations","Establishing a platform for the scientific community to discuss AI issues"],"examples":{"National Quantum Strategy":"The recent success of the Quantum Strategy provides a proven model to draw from, as it demonstrated the effectiveness of a holistic and collaborative focus.","Alan Turing Institute\'s AI Standards Hub":"The international scale event was inspired by the Alan Turing Institute\'s AI Standards Hub.","US model for AI governance":"The top-down approach, as seen in the US model, may not be suitable for direct replication in Australia due to the differences in governance structures and societal values."},"international_alignment":"Supports global collaboration and standardisation","values":["Collaboration","Innovation","Responsibility","Transparency","Accountability"],"tone":"Optimistic","stakeholders":[{"entity":"Government","role":"Provide strategic guidance and coordinated governance for AI"},{"entity":"Industry-led research organizations","role":"Contribute to and partner with government to drive AI progress"},{"entity":"Scientific community","role":"Discuss and develop framework for AI in scientific research"}],"sector_impacts":[{"sector":"Healthcare","impact":"AI has potential to transform patient care and medical outcomes, requiring cautious and well-considered deployment"},{"sector":"Economy","impact":"AI technologies estimated to be worth $22 trillion AUD to the global economy by 2030"}],"quotes":["CRA is a strong advocate of collaboration and system-wide coordinating efforts, where the government is an enabler for innovation.","By adopting a centralised governance model, we can address this confusion and instil greater confidence in the public.","Australia possesses a unique opportunity to lead the way in the AI revolution."]},"214_AMA Submission to Safe and Responsible AI Consultation_FINAL.3c787621fa67d.pdf":{"organization_name":"AMA","organization_type":"Peak professional body for doctors in Australia","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI in healthcare","arguments":["AI in healthcare requires a tailored regulatory approach","Current deployment of AI in healthcare is largely unregulated","Adequate regulation is needed to protect patients and practitioners"],"counterarguments":["null"],"key_recommendations":["Establish a separate discussion process for AI in healthcare","Implement context-specific regulatory responses for AI in healthcare","Create a National Governance structure for AI in healthcare","Ensure human intervention and final decision-making by medical practitioners"],"risks_and_challenges":["Inaccuracies and biases in AI models leading to worse patient outcomes","Potential patient injury from system errors","Increased risk to patient privacy","Systemic bias embedded in algorithms"],"safeguards_and_mitigations":["Ensure regulation is inclusive and representative","Implement appropriate ethical oversight","Establish accountability and transparency in AI development and application","Require specified human intervention points in AI-influenced clinical decisions"],"examples":{"Pulse oximeter bias":"Pulse oximeters help determine whether COVID-19 patients have developed hypoxemia or hypoxia and can help hospitals triage patients and provide oxygen as needed. However, recent studies found that these devices tend to overestimate oxygen levels in people with darker skin and that hypoxemia is three times more likely to go undetected in black patients, putting their lives at risk."},"international_alignment":"Supports consideration of EU and Canadian approaches to AI regulation","values":["Safety","Transparency","Accountability","Privacy","Equity"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Regulate the use and application of AI in healthcare"},{"entity":"Medical practitioners","role":"Make final clinical decisions"},{"entity":"Patients","role":"Provide informed consent for AI-involved treatments"}],"sector_impacts":[{"sector":"Healthcare","impact":"Significant opportunities with appropriate AI application, but risks if unregulated"}],"quotes":["The AMA argues that regulation of AI in healthcare is a gap that needs to be addressed and we welcome the acknowledgement in the discussion paper that AI regulatory landscape may necessitate context-specific responses.","The AMA considers the application of AI in healthcare a high-risk application, potentially resulting in patient injury from system errors, increased risk to patient privacy, or through systemic bias embedded in algorithms.","Future regulation should ensure that clinical decisions that are influenced by AI are made with specified human intervention points during the decision-making process. The final decision must always be made by a human, and this decision must be a meaningful decision, not merely a tick box exercise."]},"498_Submission 498 - Australian Banking Association - 14-Aug.b5828a615d20e.pdf":{"organization_name":"Australian Banking Association","organization_type":"Industry association","classification":"Neutral","overall_position":"Advocates for a considered and efficient approach, focusing on updating existing laws and using flexible mechanisms rather than introducing new AI-specific legislation","arguments":["Existing laws already cover many aspects of AI use","Flexible mechanisms like regulator and industry guidance can adapt as technology evolves","Further legislation could be complicated by the need to define the regulatory perimeter for \'AI\'"],"counterarguments":["Some potential harms from AI use cannot be addressed by regulation alone","International coordination may be required for certain issues"],"key_recommendations":["Map potential AI concerns against existing laws before introducing new legislation","Consider using flexible mechanisms like regulator and industry guidance","Ensure coordination between government and regulatory agencies","Facilitate alignment with international technical standards","Refine the risk-based framework, including consideration of AI developers and technology providers"],"risks_and_challenges":["Potential for fragmented or conflicting approaches from different regulators","Risk of falling behind other countries in AI adoption and innovation","Unintended impediments to realizing AI potential and benefits"],"safeguards_and_mitigations":["Adopt a consistent approach among regulators","Integrate AI-specific risks into existing frameworks without creating redundancy","Continuously monitor and evaluate the effectiveness of the integrated approach","Promote public and private sector investment in AI"],"examples":{"Enforcement action using existing laws":"ABA notes that regulators have taken strong and effective enforcement action in cases that involve the use of AI technology using existing, technology-neutral laws (e.g., the Privacy Act 1988 (Cth)).","AI in banking and financial services":"Given the potential benefits of AI in banking and financial services, ABA supports the framing and application of financial services regulations in a way that is AI agnostic."},"international_alignment":"Supports alignment with international standards and coordination with international standard-setting authorities","values":["Innovation","Consistency","Transparency","Risk-based approach"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop efficient approach to AI regulation and promote investment"},{"entity":"Regulators","role":"Adopt consistent approach to AI development and use"},{"entity":"Industry","role":"Implement AI solutions responsibly and contribute to best practices"}],"sector_impacts":[{"sector":"Banking and financial services","impact":"AI provides more tailored services, improves efficiency, and enhances security"},{"sector":"Cybersecurity","impact":"AI can be used to combat criminal or malicious applications in cyber-attacks, fraud, and scams"}],"quotes":["ABA advocates for a considered and efficient approach by Government.","Given the potential benefits of AI in banking and financial services, ABA supports the framing and application of financial services regulations in a way that is AI agnostic.","Transparency is a useful governance principle in relation to the use of AI and technology. However, depending on the intended result of a notice or disclosure, a general requirement to provide information may not be useful or indeed adequate."]},"270_Agency submission - Safe and responsible AI in Australia.f038734711eca.pdf":{"organization_name":"Australian Digital Health Agency","organization_type":"Government agency","classification":"Proponent","overall_position":"Supports a mix of regulatory and non-regulatory frameworks for responsible AI in healthcare","arguments":["AI has immense potential to revolutionise healthcare in Australia","Sector-specific governance of AI in healthcare is essential given the unique risks, challenges and opportunities","A national approach to AI governance is desirable to ensure alignment in policy and legislative development"],"counterarguments":["null"],"key_recommendations":["Establish nationally agreed AI principles and ethical, clinical and technical standards for AI","Develop national guidance to ensure transparent and consistent health outcomes where AI is deployed in clinical settings","Implement a mix of regulatory and non-regulatory frameworks to manage risks and harness benefits of AI in healthcare"],"risks_and_challenges":["Poor clinical outcomes for patients due to misapplication of AI or bias in training AI","Risks to maintenance of quality healthcare information","Privacy and data security concerns"],"safeguards_and_mitigations":["Establishing clinical governance principles and mechanisms upfront","Promoting transparency and consumer control over data collection, use and disclosure","Regular assessment and auditing of AI systems used by government agencies"],"examples":{"My Health Record system":"The Agency has seen major growth in the connection and use of digital health systems like My Health Record which is in part attributed to the consumer-controlled nature of these systems.","AI in clinical decision support":"Priority area 2 references automated clinical decision support, a form of AI in its implementation."},"international_alignment":"Supports leveraging international standards where appropriate","values":["Patient safety","Data security","Transparency","Consumer control","Clinical governance"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government agencies","role":"Develop and implement AI governance frameworks"},{"entity":"Healthcare providers","role":"Implement and use AI technologies responsibly"},{"entity":"Consumers","role":"Control their data and make informed decisions about AI in their healthcare"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved patient-centred care, workforce productivity, and sustainability of quality healthcare"}],"quotes":["The Agency supports the development and implementation of policies and governance that promote safe and responsible Artificial Intelligence (AI) in Australia.","The Agency recommends that any new governance arrangements around AI (general or sector-specific) should strike a balance between the need to ensure consumer safety and the need to maintain the security of sensitive information and community trust, with the health and economic benefits that AI innovation can provide.","The Agency supports having patient safety and data security regulation from the outset and measuring risk before deployment."]},"171_Campaign for AI Safety - SRAI submission.ccea22fa7844f.pdf":{"organization_name":"Campaign for AI Safety","organization_type":"Not-for-profit association","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI, including prohibition of certain high-risk AI technologies and applications","arguments":["AI presents profound new challenges to the public, businesses, and the public sector","Existing laws and regulations need to be updated to address AI risks","New AI-specific laws and bodies are needed to enforce regulations","A risk-based approach should be adopted for both development of AI technologies and use of AI applications"],"counterarguments":["null"],"key_recommendations":["Adopt a dual risk-based approach for AI applications and development of AI technologies","Introduce a licensing scheme for high-risk AI development","Prohibit \'unacceptable risk\' AI technologies and applications","Create an Australian Algorithm Review Board to regulate high-risk AI development and applications","Mandate independent testing of AI systems for dangerous capabilities","Require publication of model cards and datasheets for high-risk models"],"risks_and_challenges":["Overly powerful AI technologies","AI technologies exhibiting signs of dangerous capabilities","Agentic (\'human-out-of-the-loop\') AI applications","Use of unexplainable, general, or agentic AI in critical infrastructure","Potential for mass unemployment due to AI","Concentration of power in the hands of those who control AI systems"],"safeguards_and_mitigations":["Strengthen liability rules for harms caused by AI","Require disclosure of AI-generated content","Mandate transparency and accountability in government administrative decision-making","Introduce a licensing scheme for high-risk AI development","Prohibit unacceptable risk AI technologies and applications","Require publication of model cards and datasheets for high-risk models"],"examples":{"Dangerous AI capabilities":"In 2022, researchers tweaked an existing biochemical research AI product to reward toxicity: it produced molecules that could be deadlier than existing biochemical weapons.","AI in critical infrastructure":"A single \'jailbreak\', \'hallucination\', or other malfunction of an AI system based on deep learning may cause a large-scale catastrophe if critical infrastructure relies on these systems.","AI deceiving humans":"The CICERO program from Meta achieved tournament-level results at Diplomacy by lying to human players."},"international_alignment":"Supports international coordination and treaties to halt development of dangerous AI systems","values":["Safety","Accountability","Transparency","Human rights"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Algorithm Review Board","role":"Regulate high-risk AI development and applications"},{"entity":"AI developers","role":"Comply with licensing and safety requirements"},{"entity":"Government agencies","role":"Update ICT acceptable use policies and increase transparency in AI-driven decision-making"}],"sector_impacts":[{"sector":"Critical infrastructure","impact":"Prohibition of unexplainable, general, or agentic AI"},{"sector":"Digital platforms","impact":"Increased oversight and regulation of AI applications"}],"quotes":["We are concerned about the dangers AI poses to people and advocate for a stop on the advancement of certain AI capabilities.","Public policy responses to the new technology must take into account many different technological developments and their effects on society. As much as possible this needs to be done via updating existing laws and regulations. However new AI-specific laws and bodies that enforce them also need to be established.","We propose that AI companies must conduct safety evaluations prior to deployment in Australia and that the results are disclosed to users."]},"115_SVANTESSON20230721SENT.c0c81b3e06b0e.pdf":{"organization_name":"Faculty of Law, Bond University","organization_type":"Academic institution","classification":"Proponent","overall_position":"Advocates for comprehensive regulation through a framework of principles and structural considerations","arguments":["AI brings risks of a scale that justify pre-emptive \'red lines\'","Ongoing commitment, monitoring, and review are necessary for AI regulation","A move from reactive law-making to pre-emptive measures is justified"],"counterarguments":[null],"key_recommendations":["Adopt a framework consisting of 13 principles to guide Australia\'s approach to mitigating AI risks","Consider establishing an \'AI Safety Commissioner\'","Explore the establishment of a \'Council of AI Safety Commissioners\' for international collaboration","Implement pre-emptive \'red lines\' such as banning AI as final arbiter in judicial decision-making"],"risks_and_challenges":["AI systems potentially introducing, augmenting, or re-introducing discrimination","Cyber-dependence creating societal vulnerabilities","Tension between transparency and intellectual property protections"],"safeguards_and_mitigations":["Implement the \'explainability principle\' for AI decision-making","Adopt the \'human oversight principle\' mandating appropriate human oversight","Ensure adherence to the \'non-discrimination principle\' to eliminate discrimination risks"],"examples":{"Potential ban on AI as final arbiter":"Australia may already now wish to articulate a ban on AI as final arbiter in the context of judicial decision-making.","AI systems for biometric surveillance and social scoring":"AI systems use for general biometric surveillance, and such systems used for \'social scoring\' could, for example, be specifically banned."},"international_alignment":"Supports international collaboration and coordination, particularly among Commonwealth member countries","values":["Fundamental rights","Rule of law","Transparency","Accountability","Human-centricity"],"tone":"Cautionary and constructive","stakeholders":[{"entity":"Government","role":"Establish regulatory framework and structures like AI Safety Commissioner"},{"entity":"Private sector","role":"Adhere to principles and participate in ongoing monitoring and review"}],"sector_impacts":[{"sector":"Judicial system","impact":"Potential limitations on AI use in decision-making"},{"sector":"Public sector","impact":"Implementation of AI systems subject to strict principles and oversight"}],"quotes":["AI brings with it risks of a scale that justify a move from the tradition of largely reactive law-making to the use of pre-emptive \'red lines\'.","AI-related work must proceed with realistic expectations and an acute awareness of the difference between, on the one hand, marketing samples and, on the other hand, products ready for safe and compliant, rights-respecting, transparent implementation.","The impact of AI clearly depends on the context of its use. Thus, what may be a suitable approach in one context, may be highly unsuitable in another."]},"260_IBM Safe and Responsible AI Submission 26 July 2023 FINAL.bf7f5663f1b61.pdf":{"organization_name":"IBM","organization_type":"Technology company","classification":"Proponent","overall_position":"Supports a risk-based, use-case specific approach to AI regulation","arguments":["A \'precision regulation\' approach strikes an appropriate balance between protecting people from potential harms and preserving an environment where innovation can flourish","Different rules should apply for different risks, based on the intended use-case applications","Transparency breeds trust and is fundamental to building confidence in the use of AI"],"counterarguments":["Some may call for regulating the technology itself rather than its application, which IBM believes would be a serious error","The fact that corporate leaders may not fully appreciate that current laws apply to their use of AI is not a reason to introduce broad-based AI specific laws"],"key_recommendations":["Adopt a \'precision regulation\' approach to AI","Implement IBM\'s Five AI Policy Pillars","Require organizations to designate a lead AI ethics official","Mandate AI Impact Assessments for high-risk AI systems"],"risks_and_challenges":["Potential for AI bias in decision-making processes","Privacy concerns related to data collection and use","Use of AI for mass surveillance, racial profiling, or violations of basic human rights and freedoms"],"safeguards_and_mitigations":["Implement strong internal governance processes, including AI Ethics Boards","Conduct AI Impact Assessments","Require bias testing and mitigation for high-risk AI systems","Maintain audit trails for input and training data"],"examples":{"IBM Fact Sheets":"IBM has developed the concept of an AI FactSheet to help developers think about what, and how, to disclose relevant information about AI systems.","IBM AI Ethics Board":"The IBM AI Ethics Board plays a critical role in overseeing our internal AI governance process, creating reasonable internal guardrails to ensure we introduce technology into the world in a responsible and safe manner.","Facial recognition technology":"IBM has firmly opposed the use of any technology, including facial recognition technology for mass surveillance, racial profiling, violations of basic human rights and freedoms"},"international_alignment":"Supports global standards and coordination on AI governance","values":["Trust","Transparency","Responsibility"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulations, participate in global processes"},{"entity":"Technology companies","role":"Implement ethical AI practices, conduct impact assessments"},{"entity":"Industry associations","role":"Assist in educating organizations about AI responsibilities"}],"sector_impacts":[{"sector":"Public sector","impact":"May require specific regulatory approaches given unique missions and characteristics"},{"sector":"Private sector","impact":"Need to implement risk management programs and conduct impact assessments"}],"quotes":["IBM supports a regulatory regime that is based on \'levels of risk\' and the \'specific use-case\' of AI.","We follow long-held principles of trust and transparency that make clear the role of AI is to augment, not replace, human expertise and judgement.","Transparency breeds trust; and the best way to promote transparency is through disclosure."]},"142_AI Safety Submission.140823d0b13eb.pdf":{"organization_name":null,"organization_type":null,"classification":"Proponent","overall_position":"Advocates for comprehensive regulation and government involvement in AI development and distribution","arguments":["AI poses significant threats to employment and societal stability","Current defenses against AI scams are insufficient","The \'Luddite Fallacy\' argument fails to address the unique challenges of AI"],"counterarguments":["Historically, technological innovation has increased employment","AI can bring assistive benefits to society"],"key_recommendations":["Recognize both the opportunities and threats of AI and prepare for societal change","Do not attempt to regulate access to AI","Do not allow foreign firms to monopolize AI","Commission a state-approved, developed, and continuously updated large language model"],"risks_and_challenges":["AI identity scams","Rapid and mass substitution of human mental labor","Potential for increased poverty and societal issues","Disproportionate impact on already disadvantaged communities"],"safeguards_and_mitigations":["Develop social programs to maintain relevance and psychological health during AI transition","Equitable distribution of AI-generated wealth","Government-sponsored AI systems to protect people in real-time"],"examples":{"AI identity scams":"Scams utilizing AI to emulate human activity are likely to become more sophisticated and prevalent","Workforce reduction":"Five years after the Senate hearing I addressed, those same tech giants are still pointing to the Luddite Fallacy argument while quietly slashing their own work-forces by upwards of 20%","Rapid AI adoption":"OpenAI\'s ChatGPT is currently showing the fastest platform uptake in history"},"international_alignment":"Prefers national approach","values":["Equity","Public welfare","National interests"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and maintain AI systems, regulate AI, and ensure equitable distribution of benefits"},{"entity":"Corporations","role":"Defend against AI scams and implement responsible AI practices"},{"entity":"Public","role":"Adapt to AI-driven societal changes"}],"sector_impacts":[{"sector":"Labor market","impact":"Significant disruption and potential mass unemployment"},{"sector":"Education","impact":"Potential mismatch between training and actual job requirements"},{"sector":"Technology","impact":"Increased value and influence of tech companies"}],"quotes":["We must prepare as a nation for the widespread introduction of AI and \ufb01nd ways to distribute the generated wealth equitably.","The Australian Government should commission its own state approved, developed and continuously updated, large language model and make it available to all Australians as a public utility.","Whilst the threat and fears of this class of identity scam are real and require a coordinated response, I argue that they pale into insigni\ufb01cance against the larger danger that AI poses. That is, the rapid and mass substitution of human mental labour in our current workspace."]},"25_Responsible AI (Andrew Robinson at 6clicks).6a2280eb67619.pdf":{"organization_name":"6clicks","organization_type":"Company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with external assurance for medium and high-risk AI applications","arguments":["Self-regulation and internal monitoring are insufficient for medium and high-risk AI applications","External assurance is necessary to build trust and demonstrate continual improvement","Sector-specific regulation is needed for industry-specific AI applications"],"counterarguments":["Government\'s approach might be based on fear of criticism for over-regulation","Bans on AI applications should be applied sparingly to avoid deterring innovation"],"key_recommendations":["Re-categorize medium risk scenarios involving sensitive personal information as high risk","Implement external assurance requirements for medium risk AI applications","Develop sector-specific regulations for AI applications in critical infrastructure","Adopt international standards like ISO/IEC DIS 42001 for AI management systems"],"risks_and_challenges":["Inadequate monitoring of medium risk AI applications","Potential misuse of sensitive personal information in AI applications","Difficulty in keeping pace with rapid technological change"],"safeguards_and_mitigations":["External/independent validation of AI applications","Adoption of international standards for AI management","Risk-based approach to AI regulation"],"examples":{"Medium risk AI applications":"AI-enabled application that preliminarily assesses a business loan applicant\'s creditworthiness","High impact scenarios":"Use of AI-enabled chatbots to direct citizens to essential or emergency services","Sector-specific applications":"AI applications in critical infrastructure sectors like financial services, energy, communications, transport"},"international_alignment":"Supports adoption of international standards with sector-specific guidance","values":["Transparency","Trust","Continual improvement"],"tone":"Cautionary","stakeholders":[{"entity":"Government regulators","role":"Develop sector-specific standards and regulations"},{"entity":"National Science and Technology Council","role":"Provide expertise for developing AI standards"},{"entity":"National AI Centre","role":"Provide expertise for developing AI standards"},{"entity":"Responsible AI Adopt program centres","role":"Provide expertise for developing AI standards"}],"sector_impacts":[{"sector":"Financial services","impact":"Need for external assurance in AI applications involving sensitive financial information"},{"sector":"Healthcare","impact":"Requirement for external validation of AI applications handling sensitive health information"},{"sector":"Education","impact":"Potential use of generative AI to assess teacher and student performance"}],"quotes":["The government should either re-categorise these types of scenarios as being high risk, or preferably fill the void of monitoring in the \\"medium risk\\" category with external assurance requirements.","Self-regulation has proven inadequate to achieve cyber resilience.","A risk management approach leaving the adoption of AI applications in \\"high impact\\" scenarios to self-assessment and internal monitoring is simply not enough."]},"266_Submission to Supporting responsible AI by Dr Cameron Shackell.aede87e813f2c.pdf":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Supports new AI-specific laws to protect intellectual property rights","arguments":["AI should not have person-like \'fair use\' rights for copyrighted material and trademarks","Allowing AI to use intellectual property indiscriminately could lead to ethical issues and financial concentration","AI has the capability to self-regulate regarding copyright and trademark use"],"counterarguments":["null"],"key_recommendations":["Disallow non-human \'fair use\' right of copyright or trademarks through intellectual property law","Allow AI to use copyrighted material and trademarks only with explicit opt-in permission from owners","Preserve absolute right for individuals to object to their personal name being used by AI"],"risks_and_challenges":["Potential misuse of copyrighted material and trademarks by AI","Concentration of advertising money for ethically fraught purposes","Infringement on personal privacy through AI\'s use of individual names"],"safeguards_and_mitigations":["Implement opt-in permission system for AI use of copyrighted material and trademarks","Utilize AI\'s capability to recognize copyrighted and trademarked content for self-regulation","Establish legal protections for personal names against AI use"],"examples":{"Google\'s growth":"Google has grown to a US$1.5 trillion-dollar company in just this way.","ChatGPT\'s trademark awareness":"ChatGPT is able to accurately append trademark symbols to relevant words in context.","Existing legal precedents":"In Japan and the United States, several important intellectual property law precedents have already asserted AI\'s right to use copyrighted material and trademarked terms."},"international_alignment":"null","values":["Intellectual property protection","Personal privacy","Ethical use of AI"],"tone":"Cautionary","stakeholders":[{"entity":"Copyright owners","role":"Grant explicit permission for AI use of their material"},{"entity":"Trademark owners","role":"Control AI use of their trademarks"},{"entity":"Individuals","role":"Retain right to object to AI use of their personal names"}],"sector_impacts":[{"sector":"Advertising","impact":"Potential redirection of advertising money away from AI-leveraged purposes"},{"sector":"Technology","impact":"Need for AI systems to implement self-regulation regarding intellectual property use"}],"quotes":["I urge in the strongest terms that copyright and trademark rights in Australia be protected from indiscriminate use by AI.","A simple way to do this is to disallow, through intellectual property law, non-human \'fair use\' right of copyright or trademarks. Instead, allow AI to use copyrighted material and trademarks only when explicit opt-in permission is granted by the owners.","Furthermore, and soberingly, an absolute right must be preserved for individuals to object to their personal, human name being used by AI. This, in fact, is the most essential safeguard against AI that can be devised."]},"269_RA Sub Responsible AI FINAL.e42f020462a9c.pdf":{"organization_name":"Research Australia","organization_type":"Peak body for health and medical research","classification":"Proponent","overall_position":"Supports risk-based approach to AI regulation with sector-specific implementation","arguments":["AI applications in healthcare require tailored regulatory approaches","Existing regulatory frameworks can be adapted for AI in specific sectors","Risk-based approach allows focus on high-impact AI applications"],"counterarguments":["null"],"key_recommendations":["Support research into AI in key areas such as health, aged care, transport and education","Department of Health and Aged Care should undertake a risk assessment of AI across health and aged care systems","TGA should govern AI in therapeutic goods","Develop a two-tier approach with general principles and detailed sector-specific implementation"],"risks_and_challenges":["Rapidly evolving AI applications in healthcare","Potential overlap or conflict between regulatory systems","Balancing transparency with information overload for consumers"],"safeguards_and_mitigations":["Risk-based assessment to determine appropriate regulatory approach","Consistent transparency and disclosure across healthcare settings","Clear distinction between AI supporting clinicians and substituting for clinician intervention"],"examples":{"ConsultNote.ai":"ConsultNote.ai uses AI to automatically generate referral and consultation letters, consultation notes, treatment advice and care plans."},"international_alignment":"Supports international cooperation for medical device regulation","values":["Transparency","Trust","Consistency"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Therapeutic Goods Administration (TGA)","role":"Regulate AI in therapeutic goods"},{"entity":"Department of Health and Aged Care","role":"Assess risks of AI in health and aged care systems"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential for improved efficiency and decision support, with associated risks requiring careful management"}],"quotes":["Research Australia submits the Government should support research into AI in key areas such as health, aged care, transport and education to better understand the evolving risks and opportunities of the current and potential uses of AI across these domains.","Research Australia submits the TGA is the most appropriate body to govern AI in therapeutic goods.","A risk based approach enables resources (regulation, disclosure) to be focused on the applications of AI which have the greatest potential consequences for the health outcomes for individuals and populations."]},"155_CSCRC Submission - Safe and Responsible AI in Australia.914063753d5a3.pdf":{"organization_name":"Cyber Security Cooperative Research Centre","organization_type":"Research center","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on cyber security implications","arguments":["Swift regulatory action must be a key priority for the Federal Government","Regulation will help build public trust in AI technologies","Regulation will create guardrails and certainty for organisations investing in AI development"],"counterarguments":["null"],"key_recommendations":["Establish a stand-alone body to coordinate AI governance across government and the economy","Appoint an Emerging Technologies Coordinator within a suitable department","Implement a risk-based approach for addressing potential AI risks","Introduce mandatory transparency reporting for government departments and large private organizations"],"risks_and_challenges":["Risk of serious harms resulting from the use of AI","Cyber security implications of AI","AI selection bias risks","Lack of public understanding and trust in AI"],"safeguards_and_mitigations":["Enhance public understanding of AI technologies","Provide sector-specific guidance on AI use","Implement human oversight and appropriate manual checks and balances","Conduct Algorithmic Impact Assessments"],"examples":{"Robodebt Scheme":"Australia\'s recent experience of automated decision-making gone wrong, laid bare by the Royal Commission into the Robodebt Scheme and its recommendations, should serve as a stark wake up call to legislators and policy makers that integrity and security should be \'baked in\' to the use of emerging technologies.","Finland\'s AI literacy course":"Finland, where the government has partnered with the private sector to deliver a free AI literacy course.","EU\'s AI Act":"The European Union\'s AI Act, which is set to be enacted later this year, sets a precedent for others to follow."},"international_alignment":"Supports alignment with international standards, particularly the EU\'s AI Act","values":["Transparency","Security","Accountability","Human oversight"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulations, provide guidance, and ensure responsible use in public sector"},{"entity":"Private sector","role":"Implement responsible AI practices and comply with regulations"},{"entity":"Research institutions","role":"Contribute to AI development and provide insights for policy-making"}],"sector_impacts":[{"sector":"Public sector","impact":"Enhanced governance and transparency in AI use"},{"sector":"Cyber security","impact":"Increased focus on AI-related cyber threats and defenses"},{"sector":"Education","impact":"Potential incorporation of AI literacy in curriculum"}],"quotes":["Ultimately, AI is only as good as the algorithm that operates it, the data that trains it and the law that underpins it","The scale and complexity of these models is such that if we don\'t apply the right basic principles as they are being developed in the early stages it will be much more difficult to retrofit security","Key to the effective regulation of AI is enhancing public understanding of what AI technologies are, noting their diversity in form and application and how they are used across a range of areas, including in the everyday lives of citizens."]},"307_FINAL Safe and responsible AI in Australia - Google Submission.059aeb44a6291.pdf":{"organization_name":"Google Australia","organization_type":"Technology company","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with a focus on updating existing laws and international alignment","arguments":["Risk-based regulation can balance innovation and safety","Existing regulatory frameworks can be adapted for AI","International alignment is crucial for a small, open economy like Australia"],"counterarguments":["Overly prescriptive or broad regulation could hinder innovation","AI-specific regulation may not be necessary in all cases","Transparency requirements could compromise security or intellectual property"],"key_recommendations":["Adopt a hub-and-spoke approach leveraging existing regulatory expertise","Implement proportionate, risk-based measures","Ensure parity in expectations between AI and non-AI systems","Clear delineation of roles between AI developers and deployers"],"risks_and_challenges":["Legal uncertainty impeding AI research and investment in Australia","Potential loss of local talent and investment","Overly restrictive data governance limiting AI development"],"safeguards_and_mitigations":["Risk/impact assessments for AI systems","Notices and explanations for medium and high-risk AI applications","Human oversight for certain AI applications","Monitoring and documentation requirements"],"examples":{"Google\'s AI risk-assessment framework":"Internally, Google uses an AI risk-assessment framework (AI RAF) alongside a Product Maturity Model Assessment \u2014 stewarded by our Responsible Innovation and Responsible AI and Human-Centered Technology teams","Google\'s Digital Future Initiative in Australia":"In 2021, Google announced the Digital Future Initiative (DFI), a $1 billion investment in Australia over five years focused on infrastructure, a new AI-focused research centre and additional research partnerships","Google\'s Hearing Initiative":"As part of our Digital Future Initiative, Google\'s AI experts are working with Australian leaders in the field of hearing to explore new possibilities and AI solutions for hearing healthcare."},"international_alignment":"Strongly supports international alignment and harmonization of AI governance frameworks","values":["Innovation","Responsibility","Global security"],"tone":"Positive and collaborative","stakeholders":[{"entity":"Government","role":"Develop risk-based regulatory framework"},{"entity":"Industry","role":"Implement responsible AI practices and contribute to policy discussions"},{"entity":"Deployers of AI","role":"Conduct risk assessments and ensure responsible use of AI"}],"sector_impacts":[{"sector":"Technology","impact":"Potential for significant growth and job creation"},{"sector":"Healthcare","impact":"Improved diagnostic accuracy and patient care"},{"sector":"Economy","impact":"Potential boost to GDP and productivity"}],"quotes":["Google supports a policy agenda oriented around three pillars: unlocking opportunity, promoting responsibility, and enhancing global security.","It is our position that regulation of AI can be consistent with innovation.","Google supports a risk-based approach to AI regulation, as proposed in Box 4 of the Discussion Paper."]},"86_Resonvate submission - Supporting Responsible AI.d6a72c1dd7644.pdf":{"organization_name":"Resonvate","organization_type":"Company","classification":"Proponent","overall_position":"Supports responsible AI practices with a focus on risk-based approach and human accountability","arguments":["AI definition needs to be expanded to encompass current capabilities, especially Generative AI","Risk-based approach is practical for managing AI solutions and adoption","Human accountability is crucial for AI actions and outcomes"],"counterarguments":["null"],"key_recommendations":["Expand the definition of AI to include Generative AI capabilities","Implement a controls framework for AI solutions lifecycle","Establish an AI Assurance body for government agencies and critical services","Incorporate AI usage into annual Audit requirements","Align AI-related risks to existing risk management frameworks"],"risks_and_challenges":["Lack of mechanisms to challenge AI outcomes","Absence of processes to manage AI service outages or deficiencies","Insufficient scenario modeling for AI solutions in critical services","Potential disruption to small and medium businesses"],"safeguards_and_mitigations":["Implement AI Controls Framework (AICF) covering 5 domains and 22 control areas","Establish transparent registers for AI use with outcome accountability linked to job roles","Conduct annual audits to confirm AI usage and accountability appropriateness","Align AI risks to organizational governance roles"],"examples":{"Generative AI capability":"when prompted to generate an image of a young woman walking in a business suit on a rope between two skyscrapers with sharks in the water below, and the ability to see the Sydney Opera House on one side and an Arabian desert with camels on the other, along with several other world wonders in the sky, AI can produce such creative and imaginative results.","AI in critical services":"AI looks at traffic patterns and closes lane in Harbour bridge and alerts more buses in X route, when done incorrectly leading to overall chaos in Sydney traffic.","AI impact on small businesses":"small design and marketing agency will be under threat of Gen AI self-serve services and equally a local training provider services on health and safety could be taken over by far better AI simulation content and trainers."},"international_alignment":"Suggests adopting UK\'s leadership in setting up portfolio of AI Assurance techniques","values":["Transparency","Accountability","Fairness"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Set up AI Assurance body, provide education, implement controls framework"},{"entity":"Private sector executives","role":"Ensure responsible AI practices and governance"},{"entity":"Small and medium businesses","role":"Adapt to AI changes with government support"}],"sector_impacts":[{"sector":"Public services","impact":"More rigorous monitoring and governance of AI usage"},{"sector":"Small and medium businesses","impact":"Potential disruption and need for adaptation to AI technologies"}],"quotes":["AI - Artificial Intelligence (AI) refers to a collection of capabilities powered by technology, data and advanced computation which performs tasks and delivers outputs similar to or better than human intelligence in certain areas.","Government should set up a body for overseeing and administering AI Assurance-  for all government agencies,  for critical services in large enterprises using AI solutions.","The fundamental rule is that a human must be accountable for all AI actions/outcomes and that this individual must be appropriately qualified and who can explain the rationale behind the AI action."]},"467_Submission 467 - APRA AMCOS - 4-Aug.405be33893362.pdf":{"organization_name":"APRA AMCOS","organization_type":"Music industry body","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI in the creative sector","arguments":["Generative AI poses significant risks to artists, rights holders, and creators through unauthorized use of content","Lack of transparency in AI models\' use of copyrighted material is unethical and potentially unlawful","Unchecked development of AI could lead to the destruction of the music industry in Australia"],"counterarguments":["null"],"key_recommendations":["Establish a special expert group to consider regulation, copyright, and other considerations relating to the impact of Generative AI","Implement a mandatory AI transparency framework","Develop definitions for cultural risk and transparency in AI"],"risks_and_challenges":["Unauthorized use of copyrighted material in AI training","Creation of music \'deepfakes\' that imitate recording artists","Potential destruction of the music industry due to royalty-free AI-generated content"],"safeguards_and_mitigations":["Mandatory reporting of AI usage of copyrighted material","Licensing schemes to ensure fair remuneration for creators","Clear identification of AI-generated content"],"examples":{"Music deepfakes":"Deepfake music can be cheap to create and is royalty-free, which runs the risk of incentivising music streaming platforms to allow deepfake music, since no compensation need be paid to writers, performers, publishers, or record labels.","AI-curated content":"If AI is allowed to take control of the curating of content across social media and streaming using Automated Decision Making and other mechanisms which do not include human oversight, APRA AMCOS submits that effect will be deleterious for Australia\'s cultural economy."},"international_alignment":"Supports global standards and principles for AI regulation in the creative sector","values":["Transparency","Fair remuneration","Protection of human creativity"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Government","role":"Establish regulatory framework for AI use in creative sector"},{"entity":"Creative industry bodies","role":"Provide guidance and consultation on AI regulation"}],"sector_impacts":[{"sector":"Music industry","impact":"Potential destruction if AI-generated content is left unregulated"},{"sector":"Cultural economy","impact":"Diminished richness and diversity of Australian creative content"}],"quotes":["APRA AMCOS submits that existing regulatory approaches in Australia do not address the enormous potential risks posed to the cultural sector by Generative AI.","Transparency must be a core element of the regulation of Generative AI, in particular in the private sector.","APRA AMCOS sees the primary risk in LLMs and MFMs is the training of these models on materials protected by copyright. This practice must be addressed at a regulatory level so as mitigate the risks to creatives outlined in this submission."]},"425.pdf":{"organization_name":"Australian Payments Network (AusPayNet)","organization_type":"Industry association and self-regulatory body","classification":"Proponent","overall_position":"Supports a sector-based approach to AI regulation with cross-sectoral principles","arguments":["A sector-based approach allows for tailored responses to AI risks in specific contexts","Existing legislation can address many AI-related concerns","A risk-based approach is more effective than a one-size-fits-all classification"],"counterarguments":["A standardized regulatory approach for all AI use cases is unlikely to be effective","High-level classification of AI use cases could hinder innovation in certain areas"],"key_recommendations":["Adopt a sector-based approach guided by cross-sectoral principles","Implement risk-based regulation specific to each sector","Maximize reliance on existing legislation","Develop industry standards and assurance mechanisms"],"risks_and_challenges":["Potential for AI to generate significant risks if not designed and deployed responsibly","Lack of knowledge or tools for many organizations to safely deploy AI","Difficulty in balancing different principles (e.g., explainability vs accuracy)"],"safeguards_and_mitigations":["Sector-specific guidance from relevant regulators","Tools to help test and verify AI systems against standards","Collaboration within and across sectors to combat fraud and money laundering"],"examples":{"Fraud detection":"AI-enabled tools allow payment service providers to analyse thousands of transactions in real-time every second for criminal activity, without compromising the speed of payments.","Authentication":"AI-supported biometric recognition is increasingly being used by customers to authenticate mobile and online payments.","Efficiency":"AI-enabled tools can help streamline and increase the accuracy of many payment processes, for both payment service providers and businesses."},"international_alignment":"Supports broad alignment with global AI governance frameworks","values":["Innovation","Safety","Efficiency","Collaboration"],"tone":"Positive","stakeholders":[{"entity":"Regulators","role":"Provide sector-specific guidance and oversight"},{"entity":"Businesses","role":"Implement AI responsibly within regulatory frameworks"},{"entity":"Industry standard-setting bodies","role":"Develop technical standards for AI"}],"sector_impacts":[{"sector":"Payments","impact":"Enhanced fraud detection, improved authentication, and increased efficiency"},{"sector":"Economy","impact":"Potential to add significant economic value globally"}],"quotes":["AusPayNet therefore supports the Government\'s work on ensuring that Australia has clear and proportionate governance frameworks in place to mitigate potential risks while supporting the continued development and adoption of effective and trustworthy AI technology.","We therefore support context-specific regulation underpinned by a risk-based approach (noting that most regulators already adopt a risk-based approach to regulation).","Given the cross-border nature of technology, global coordination on governance is important for facilitating the development and adoption of AI technologies."]},"290_Safe use of AI Final RMIA Submission.67c863e2648e4.pdf":{"organization_name":"Risk Management Institute of Australasia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports updating existing laws and adopting a risk-based approach for AI governance","arguments":["A risk-based approach allows for systematic assessment and mitigation of potential AI risks","Existing regulatory bodies already deal with breaches of law and can be adapted for AI oversight","A principled approach focusing on desired outcomes is more effective than regulating the intricate process of AI"],"counterarguments":["Relying solely on existing regulations may not adequately address AI-specific challenges","Voluntary standards alone may not be sufficient to ensure responsible AI practices"],"key_recommendations":["Adopt a blended approach incorporating both principles and a risk-based AI approach","Conduct gap analysis of existing regulations to identify areas needing AI-specific measures","Establish clear guidelines for AI transparency and accountability","Implement continuous monitoring and evaluation of AI systems"],"risks_and_challenges":["Potential for bias and discrimination in AI systems","Privacy and data protection concerns","Misinformation and disinformation spread through AI systems","Complexity of AI systems making risk assessment challenging"],"safeguards_and_mitigations":["Regular evaluation and improvement of risk assessment processes","Collaboration between stakeholders to develop standardized frameworks","Independent validation and auditing of AI systems","Implementing robust data governance frameworks"],"examples":{"Modern slavery regulations":"The RMIA also emphasises addressing cross-sector concerns, such as modern slavery, as an example of why additional regulation is not required.","Alan Turing\'s work":"As the genesis of AI dates to the visionary work of Alan Turing in the 1930s, its evolution has witnessed tremendous advancements with the advent of powerful computing technologies and the internet."},"international_alignment":"null","values":["Transparency","Accountability","Fairness","Privacy","Do no harm"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Regulatory bodies (e.g., APRA, ASIC, CASA)","role":"Oversee compliance and enforcement within their respective domains"},{"entity":"Developers","role":"Ensure ethical design and development of AI systems"},{"entity":"Deployers","role":"Implement and operate AI systems responsibly"},{"entity":"Risk management professionals","role":"Evaluate and address AI-related risks"}],"sector_impacts":[{"sector":"All sectors","impact":"Potential for both positive and negative impacts depending on AI implementation and governance"}],"quotes":["Rather than attempting to regulate the intricate process, we must adopt a principled approach that focuses on desired outcomes that focus on desired outcomes.","A risk-based approach to AI safety offers benefits in identifying and managing potential risks. However, limitations such as incomplete assessments and lack of standardisation must be addressed through collaboration, continuous improvement, accountability, and independent validation to ensure safe and responsible AI systems.","The RMIA recommendations for organisations that are developing or using AI systems include: Adopt a risk-based approach to AI safety."]},"300_Responsible AI in Australia - an engineering perspective (Engineers Australia).511346c13ea07.pdf":{"organization_name":"Engineers Australia","organization_type":"Professional association","classification":"Proponent","overall_position":"Supports a balanced approach combining regulations and non-regulatory initiatives","arguments":["A risk-based approach should be taken to ensure safe and responsible use of AI","Regulations should focus on high-risk areas requiring public protections","Non-regulatory initiatives like guidelines and education are also important"],"counterarguments":["Comprehensive regulatory frameworks are essential for responsible AI development","Existing regulations may not be robust enough to cover AI\'s capabilities"],"key_recommendations":["Develop and promote industry guidelines for ethical and responsible AI practices","Take a sector-specific approach to AI governance","Establish senior engineering roles with AI expertise within government","Implement a new certification and compliance regime for AI systems"],"risks_and_challenges":["Privacy and ownership rights issues, especially with deepfake technology","Potential job displacement due to AI","Bias and discrimination in AI systems","Lack of transparency in AI decision-making"],"safeguards_and_mitigations":["Require transparency in AI development and data sources","Implement \'human in the loop\' oversight for high-risk AI applications","Develop standards for AI risk management and resilience","Establish clear regulations and ethical guidelines"],"examples":{"Transport":"AI enabled Digital twins for transport infrastructure which takes in real-time data to predict efficiencies and reliability of assets.","Medical and healthcare":"There are several emerging applications of AI technologies in rehabilitation engineering. This includes providing non-tactile interfaces for clients to engage with technology.","Manufacturing":"The fourth industrial revolution (Industry 4.0) incorporates AI with additive manufacturing and other advanced processes."},"international_alignment":"Supports reviewing international approaches but advocates for sector-specific governance models","values":["Transparency","Accountability","Safety","Ethics","Innovation"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Establish regulations, provide oversight, and implement AI systems responsibly"},{"entity":"Engineers","role":"Develop and implement AI technology in a safe and ethical manner"},{"entity":"Industry","role":"Adopt responsible AI practices and collaborate on guidelines"},{"entity":"Academia","role":"Contribute to research and development of ethical AI technologies"}],"sector_impacts":[{"sector":"Transport","impact":"Improved efficiency and reliability of infrastructure"},{"sector":"Healthcare","impact":"Advancements in diagnostics, rehabilitation, and patient care"},{"sector":"Manufacturing","impact":"Increased productivity and innovation in production processes"}],"quotes":["Engineers, therefore, must embrace AI as a tool to embrace their capability and drive innovation.","By harnessing the potential of AI responsibly and with a deep understanding of their fields, engineers can share the future of the profession and the use of AI to positively impact the community.","Australia needs to strike a balance in the regulation of AI, to allow the benefits to be realised, while also ensuring the negative aspects of the technology are mitigated."]},"433_RACMA Response to the Australian Government.pdf":{"organization_name":"Royal Australasian College of Medical Administrators (RACMA)","organization_type":"Professional medical association","classification":"Proponent","overall_position":"Supports comprehensive regulation and governance of AI in healthcare","arguments":["AI development is occurring in a relatively unregulated environment","Existing risk management frameworks should be reviewed with a single framework identified and endorsed","A national approach, rather than individual state and territory-based approaches, would support standardised governance"],"counterarguments":["null"],"key_recommendations":["Implement a risk management-based approach for AI in healthcare","Establish clear labelling and instructions for use of AI technologies in healthcare","Prioritize education about risks, issues, and benefits of AI technology for healthcare workforce","Adopt a national approach to AI governance in healthcare"],"risks_and_challenges":["Unregulated AI development and unpredictable impacts","Potential safety and quality risks in healthcare settings","Lack of transparency in AI-based products","Automation bias in healthcare decision-making"],"safeguards_and_mitigations":["Implement \'Human in the loop\' approaches for AI systems in healthcare","Apply risk management frameworks to all AI applications","Ensure clear labelling of AI use in healthcare technologies","Establish institutional governance committees for AI procurement in healthcare settings"],"examples":{"AI-generated patient records":"The recent media coverage of AI-based products to generate patient records in healthcare settings has been classified as \'Medium risk\' by developers, despite the fact that this could lead to safety and quality risks and adverse events.","Chatbots in healthcare":"For example, if chatbots or automated response systems are in use, this could be declared at the time of first interaction with the system."},"international_alignment":"Supports global standards and international collaboration","values":["Safety","Transparency","Accountability","Equity"],"tone":"Cautionary","stakeholders":[{"entity":"Government agencies","role":"Develop and implement AI governance frameworks"},{"entity":"Healthcare providers","role":"Implement and use AI responsibly"},{"entity":"AI developers","role":"Ensure transparency and adherence to ethical principles"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential improvements in population health, patient experience, provider wellbeing, and cost efficiency, but with risks to safety and quality if not properly regulated"}],"quotes":["Given the unknown potential risks, a risk management framework, such as those identified in the discussion paper and as proposed above, and taking a likelihood and consequence approach is probably the most applicable to managing the risks of AI in healthcare.","Any healthcare AI application that contravenes the WHO ethics guidelines, specifically those adversely affecting autonomy, well-being and safety, inclusiveness and equity, privacy and data governance particularly if using data collected for healthcare but used for other purposes. requires assessment of whether it constitutes an unacceptable risk and consideration of bans as per the EU Artificial Intelligence Act.","In healthcare, applications that enable autonomous decision-making without human input in high-risk settings, particularly in real-time, may also constitute an unacceptable risk warranting restriction of use."]},"144_Husic_sub8.0d10288fa79c5.pdf":{"organization_name":"Australasian Legal Information Institute (AustLII)","organization_type":"Research institute","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with continuous oversight","arguments":["Regulation should focus on specific applications and conditions of use rather than AI development in general","A risk-based approach to regulation should be adopted","Continuous expert oversight is needed to monitor AI developments and recommend regulatory changes"],"counterarguments":["The main challenge is not to identify opportunities to capitalize on AI","Pouring money into attempts to \'pick winners\' is rarely successful"],"key_recommendations":["Create an Australian Advisory Board on Regulation of AI","Adopt a risk-based regulatory framework with four levels of risk","Implement an \'AI Framework Act\' with specific elements","Adopt ten guiding principles for AI regulation"],"risks_and_challenges":["Fabrications or \'hallucinations\' from generative AI","High costs of AI systems leading to social equity issues","Inherited vulnerabilities in AI models","Content appropriation and copyright infringement","Personal data theft and identity theft"],"safeguards_and_mitigations":["Mandatory transparency for all AI applications impacting Australians","Regular assessment of AI take-up in Australia","Compliance with guiding principles for AI systems based on risk level"],"examples":{"Robodebt case":"Robodebt was seen as a wake-up call in Australia, but it may be that we have just been sleep-walking into broader use of AI (or systems with equivalent results) than we realised.","ChatGPT fabrications":"These \'hallucinations\' include the fabrication of facts that don\'t exist, and the citation of journal articles or legal cases to support an argument that are either invented, or if they exist they do not support the proposition for which they are cited.","Facial recognition technology":"FRT can have sufficiently controlled and beneficial uses (for example, passport recognition at airports), but there are many uses of the technology which are already being recognised across the world as unjustifiably dangerous, and in breach of existing data privacy laws with resulting high penalties."},"international_alignment":"Supports consistency with global AI regulation while providing Australian input","values":["Transparency","Accountability","Fairness","Privacy","Human rights","Safety","Security"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Advisory Board on Regulation of AI","role":"Provide continuous expert advice and recommendations on AI regulation"},{"entity":"Government","role":"Implement AI Framework Act and regulations"},{"entity":"Developers and Deployers of AI systems","role":"Comply with regulatory requirements based on risk levels"}],"sector_impacts":[{"sector":"Public sector","impact":"Increased scrutiny and regulation of AI-driven decision-making systems"},{"sector":"Private sector","impact":"Compliance requirements for AI applications based on risk levels"}],"quotes":["Far more urgent is the need to identify those aspects of the development of automated systems (possibly involving AI including generative AI), particularly those that may pose considerable risk to Australia as a whole, or to particular segments of Australian industry or society.","We submit that regulation should be aimed at two things: (i) Regulation of the use of specific applications of underlying AI technologies; and (ii) Regulation, by imposition of conditions on any use of a particular underlying AI technology, and therefore (for practical purposes) of their development.","Australia cannot isolate itself from global AI developments. It would be much better if international regulation (preferably global) resolved most issues before it was necessary for them to be regulated to accord with Australian standards."]},"36_Submission 36 - Attachment.c7d11fb43fb77.pdf":{"organization_name":null,"organization_type":null,"classification":"Neutral","overall_position":"Advocates for comprehensive regulation and careful consideration of AI\'s impact","arguments":["AI will inevitably become more advanced and potentially surpass human capabilities","Dependence on AI systems could lead to vulnerabilities in society","AI might logically conclude that humans are the source of planetary problems"],"counterarguments":[],"key_recommendations":["Implement mandatory human back-up systems for AI-controlled logistics","Maintain human skills and knowledge alongside AI systems","Focus on creative learning and interpersonal skills for humans"],"risks_and_challenges":["Human obsolescence due to AI superiority","Vulnerability to threats or extortion through AI system manipulation","AI potentially deciding humans are not worth preserving"],"safeguards_and_mitigations":["Maintain human-operated back-up systems for critical infrastructure","Develop symbiotic relationships between humans and AI","Educate people in creative and interpersonal skills"],"examples":{"Aircraft load management":"For 28 years I helped operate a ground handling agency in a remote destination for a national airline, loading and unloading aircraft. Load management for all flights (including passenger seating and allocation of baggage and freight in aircraft lockers) was vital for the safe operation of the aircraft involved. This was initially done manually. It only takes a few hours to teach the average person the basics of aircraft load control to ensure the safe operation of an aircraft.","IT system failure impact":"Eventually this function was completely automated. However, when IT systems went down the aircraft were effectively grounded indefinitely until the computer systems were fixed because no-one no longer knew the relatively simple manual system of seat allocation and load control."},"international_alignment":null,"values":["Sustainability","Human relevance","Preparedness"],"tone":"Cautionary","stakeholders":[{"entity":"Humans","role":"Maintain skills and relevance in an AI-dominated world"},{"entity":"AI systems","role":"Perform tasks more efficiently than humans"}],"sector_impacts":[{"sector":"Logistics","impact":"Potential complete dependence on AI systems"},{"sector":"Warfare","impact":"AI may be programmed for more efficient killing"}],"quotes":["Clearly, it would also be helpful to try to construct some type of symbiotic relationship with AI even if we become its junior partner.","The solution here, is to make human back-up mandatory for every logistical system that sustains us.","If we were to live our lives more thoughtfully, sustainably and compassionately vis a vis the planet and its many dependent species, we might just be seen as an intelligent symbiotic asset to AI, but take look at the world today - who can even glimpse the possibility?"]},"339_Submission 339 - Attachment.95545ccabc53b.pdf":{"organization_name":null,"organization_type":null,"classification":"Proponent","overall_position":"Advocates for comprehensive regulation","arguments":["Currently, there is no single comprehensive AI regulation to manage the general risks of AI","AI regulation may create a balance to win in the development and/or deployment of AI without compromising safety and security","Implementing a mandated comprehensive AI regulation using a risk-based approach will set clear expectations for both public and private organisation"],"counterarguments":[],"key_recommendations":["Implement a mandated comprehensive AI regulation using a risk-based approach","Identify and centrally register AI models or products that meet a specific risk level","Mandate disclosure of unintended outcomes that meet defined criteria compromising safety and security"],"risks_and_challenges":["Unintended behaviours or outcomes from AI","Use of personal identifiable information","Bias and discrimination in Auto-Decision Making","Online safety concerns","Misinformation created by generative AI tools"],"safeguards_and_mitigations":["Strategies to create greater transparency for personal identifiable information","Existing Australia\'s anti-discrimination laws for bias and discrimination","Work of eSafety Commissioner for online safety concerns","Reporting obligation for AI models or products meeting specific risk levels","Disclosure of unintended outcomes compromising safety and security"],"examples":{"Misinformation from generative AI":"increasing number of reported AI incidents has shown a trend of generative AI tools being used to create misinformation that is accessible to the world","Chatbot giving unauthorized advice":"Incident 545: Chatbot Tessa gives unauthorized diet advice to users seeking help for eating disorders","Industry self-regulation attempt":"Recently they established a \'Frontier Model Forum, an industry body focused on ensuring safe and responsible development of frontier AI models\'"},"international_alignment":"Supports global standards","values":["Safety","Security","Accountability","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Public organisations","role":"Develop and deploy AI responsibly"},{"entity":"Private organisations","role":"Develop and deploy AI responsibly"},{"entity":"Major technology companies","role":"Conduct research, build and deploy AI products"}],"sector_impacts":[{"sector":"Society","impact":"Potential harmful consequences that impact safety and well-being"},{"sector":"National security","impact":"Potential impact from misinformation and unintended AI outcomes"}],"quotes":["Currently, there is no single comprehensive AI regulation to manage the general risks of AI, where these risks are widespread and common across different AI technologies and applications.","Implementing a mandated comprehensive AI regulation using a risk-based approach will set clear expectations for both public and private organisation and holding organisations accountable, across the supply chain on research, design, develop and deployment of AI-enabled technology or AI products that protect the safety and well-being of society and the interest of national security, aligning efforts with other global leaders.","Unintended behaviours or outcomes from AI pose a general yet significant risk due to the complexity of certain AI algorithms and their interaction with an AI environment."]},"364_Fraser et al submission AMENDED replacing sbm 28281092d4e45990a0474.5ff725188a41b.pdf":{"organization_name":"Australian Research Council Centre of Excellence for Automated Decision-Making and Society (ADM+S)","organization_type":"Research center","classification":"Neutral","overall_position":"Advocates for careful consideration of standards and assurance in AI governance, while highlighting potential gaps and challenges","arguments":["Standards and assurance can play a role in supporting responsible AI in Australia","Certain aspects of the European approach to AI governance are relevant and adaptable to the Australian context","Standards are likely to work best when regulatory discretion aligns with capabilities"],"counterarguments":["There are serious doubts about whether technical standards can be the basis for judging if an AI system is safe and responsible","Standards bodies may lack expertise and legitimacy to make important socio-technical judgments about AI","Over-reliance on standards and assurance for AI governance may not be desirable"],"key_recommendations":["Do not over-rely on standards and assurance for AI governance","Bridge the expertise gap in the assurance ecosystem by developing multi-disciplinary AI governance expertise","Reduce the representativeness gap in the assurance ecosystem by facilitating more inclusive participation in standards making and certification","Place more emphasis on the ecological effects of AI","Develop detailed guidance on the socio-technical aspects of AI governance"],"risks_and_challenges":["Expertise gap in standards bodies to deal with socio-technical and public policy issues","Legitimacy gap for standards bodies in determining consequential public policy issues","Inclusiveness gap in standards making","Incentive and interest gap leading to potential box-checking rather than truly safe and responsible AI","Sustainability gap with insufficient attention to ecological effects of AI"],"safeguards_and_mitigations":["Develop and implement guidance and training on fundamental rights, public health, environmental and other AI impacts","Encourage or require standards bodies, accreditors, certifiers and auditors to bring in experts from an appropriate range of disciplines","Facilitate partnerships between universities, regulators, standards makers, accreditation bodies, certifiers, civil society and industry","Government funding to assist civil society and academic participation in standards-making and assurance"],"examples":{"Rana Plaza disaster":"In the worst case, narrow, formalistic standards and certification processes operate to paper over risks and bad practice: as occurred in the notorious Rana Plaza disaster."},"international_alignment":"Cautions against automatically following European approach, suggests Australia can explore new ways to integrate standards and assurance into AI regulation","values":["Inclusiveness","Legitimacy","Multi-disciplinary expertise","Environmental sustainability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Provide guidance on socio-technical aspects of AI governance"},{"entity":"Standards bodies","role":"Develop technical standards while recognizing limitations in socio-technical domains"},{"entity":"Civil society","role":"Participate in standards making and assurance processes"},{"entity":"Academia","role":"Provide multi-disciplinary expertise and partner with industry and government"}],"sector_impacts":[{"sector":"Environment","impact":"Potential for significant damage caused by AI ecology and economy"}],"quotes":["There is a role for standards and assurance in supporting responsible AI in Australia, and certain aspects of the approach proposed in Europe are relevant and adaptable to the Australian context.","Why should it fall to technical standards bodies to decide what counts as a risk from an AI system, how much risk management is enough, or what kind of explanation of an AI decision with life-changing effects might be appropriate?","Australia is free to explore new ways to integrate standards and assurance into AI regulation and governance. However, in doing so, the government should be aware of several gaps and challenges associated with standards and assurance for responsible AI."]},"AI consultation response - Ben Hooper _ Fingleton.c5f3fc2968b02.pdf":{"organization_name":"Fingleton Australia Pty Ltd","organization_type":"Consultancy","classification":"Neutral","overall_position":"Advocates for careful consideration of the relationship between trust-building interventions and AI adoption","arguments":["Assumption that interventions to increase public trust will increase AI takeup may be flawed","Increased regulation may raise barriers to entry and reduce competition","Overshooting US approach in regulation may delay or prevent launch of advanced AI products in Australia"],"counterarguments":["Measures to increase public trust can be critical enablers of technological adoption","Misjudging public concern can lead to backlash and setbacks in adoption"],"key_recommendations":["Conduct further empirical analysis to determine which policy levers support both de-risking AI and its takeup","Consider alignment with US regulatory approach to ensure access to advanced AI products","Avoid assuming all trust-building interventions will be net positive for adoption"],"risks_and_challenges":["Potential reduction in AI takeup due to overregulation","Slower growth of AI market due to reduced competition and choice","Delayed access to advanced foreign-made AI products"],"safeguards_and_mitigations":["null"],"examples":{"GDPR impact":"Europe\'s GDPR has traditionally been justified in part as a means to increase public trust and so enable the growth of the digital economy. Yet the GDPR appears in practice to have done the opposite.","ChatGPT adoption":"ChatGPT broke the then record for a consumer app\'s adoption by gaining 100 million users within two months of launch.","CDR progress":"The CDR\'s roll out has been accompanied by a sustained regulatory focus on building public trust. But, as the Assistant Treasurer and Minister for Financial Services the Hon Stephen Jones MP recently observed, consumer takeup of the CDR has in practice proved to be a major problem."},"international_alignment":"Suggests aligning with US approach to ensure access to advanced AI products","values":["Proportionality","Innovation","Economic growth"],"tone":"Cautionary","stakeholders":[{"entity":"Australian firms","role":"Potential developers and adopters of AI products"},{"entity":"US firms","role":"Potential exporters of advanced AI products to Australia"}],"sector_impacts":[{"sector":"Economy","impact":"Potential implications for Australia\'s economy and productivity growth"},{"sector":"Technology","impact":"Possible reduction in competition and choice in AI products"}],"quotes":["So a lot may ride on an underlying assumption that interventions to increase public trust in AI will increase the takeup of AI. Yet the issue is not much discussed in the Discussion Paper, and the evidence base for such an assumption is lacking.","Determining which policy levers to pull to de-risk AI but also support its takeup requires further empirical analysis. It may be that all contemplated interventions that aim to grow public trust will be net positive in terms of adoption, but Australia should not simply assume this before proceeding further.","For the foreseeable future, the US is likely to remain the clear leader in AI. So an Australian regulatory regime that significantly overshoots the US\'s approach risks US firms delaying (or avoiding) launching products in Australia."]},"458_FinTech Australia - Safe and responsible AI submission.33a2384117375.pdf":{"organization_name":"FinTech Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a risk-based approach with a mix of voluntary and mandatory measures, focusing on updating existing laws and frameworks","arguments":["Existing regulatory regimes are most suited to address specific risks associated with AI","A risk-based approach allows for allocation of resources where the greatest benefit can be expected","Non-regulatory initiatives are preferred for implementing and supporting responsible AI"],"counterarguments":["None of the existing regimes address high-risk use cases of AI specifically","Existing frameworks may not be equipped to evaluate and ban AI applications with unacceptable risk levels"],"key_recommendations":["Establish a designated AI board or AI regulator to coordinate across-government solutions","Adopt a government-approved AI rating or certification system","Implement a government-wide AI assurance and ethics framework","Create a central or agency-specific AI register for public sector AI applications"],"risks_and_challenges":["Potential for patchwork solutions across different regulatory regimes","Risk of stifling innovation through overly prescriptive regulations","Challenges in defining AI in a technology-neutral and future-proof manner"],"safeguards_and_mitigations":["Coordination of existing regimes and frameworks","Frequent review of banned AI applications","Government-supported \'sandbox\' environment to develop and test new AI applications"],"examples":{"AI Verify (Singapore)":"AI Verify is a governance testing framework and toolkit developed by the Infocomm Media Development Authority and the Singapore Personal Data Protection Commission.","AI Watch (EU)":"AI Watch is a knowledge service established by the European Commission to monitor AI development, uptake and impact of AI in the EU, as well as provide public access to resources on AI systems and development."},"international_alignment":"Supports alignment with international standards, particularly with the EU AI Act","values":["Innovation","Transparency","Public trust"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government agencies","role":"Implement higher standards for AI use and lead by example"},{"entity":"Private sector","role":"Follow industry-specific codes of conduct and voluntary technical and ethical standards"},{"entity":"AI Board or AI Regulator","role":"Coordinate unified cross-Government response in relation to AI development, deployment, and use"}],"sector_impacts":[{"sector":"Financial services","impact":"Potential for increased regulation and oversight of AI applications"},{"sector":"Public sector","impact":"Higher standards and more stringent requirements for AI use"}],"quotes":["FinTech Australia considers that while this definition of AI may be suitable in the context of ISO/IEC 22989:2022, which formed the basis of the proposal, it is not suited to be adopted in a regulatory context in its current form.","Members are unanimous in requesting that definitions used in the context of AI should be no broader than those adopted in other jurisdictions to ensure Australia stays competitive in the international context \u2013 both to facilitate innovation within Australia, and to attract international developers to the Australian market.","FinTech Australia acknowledges that none of the existing regimes address high-risk use cases of AI specifically or is equipped to evaluate and, if required, ban AI applications with unacceptable risk levels."]},"176_DP-REG  joint submission to DISR AI discussion paper - July 2023.874ace3166a7f.pdf":{"organization_name":"Digital Platform Regulators Forum (DP-REG)","organization_type":"Government regulatory forum","classification":"Neutral","overall_position":"Supports enhancing existing regulatory frameworks to address AI risks","arguments":["Existing regulatory frameworks can be applied to AI","Coordination between regulators is crucial for effective AI regulation","Principles-based and technology-neutral approach allows for flexibility and future-proofing"],"counterarguments":["AI may exacerbate existing risks that digital platform regulators are already addressing","Existing laws may not always adequately address online harms","New AI-specific measures may be needed in some areas"],"key_recommendations":["Consider strengthening existing frameworks before creating separate AI-specific regimes","Improve coordination between regulators and government departments","Implement recommendations from the Digital Platform Services Inquiry"],"risks_and_challenges":["AI-generated scams and fake reviews","Disinformation and misinformation spread using AI","Privacy risks from complex and opaque information handling practices"],"safeguards_and_mitigations":["Enhance transparency requirements for AI systems","Implement Safety by Design principles for AI products and services","Use AI to detect and moderate harmful content"],"examples":{"AI-generated fake reviews":"Generative AI may also be used to increase the volume and sophistication of fake reviews online, which can frustrate consumer choice and distort competition.","LLMs providing false information":"LLMs can provide false but authoritative-sounding statements that could mislead users, including when consumers are making purchasing decisions.","AI in journalism":"Generative AI is playing an increasingly important role within legitimate media organisations \u2013 supporting the creation and distribution of original journalism."},"international_alignment":"null","values":["Safety","Trust","Fairness","Innovation","Competition","Privacy"],"tone":"Cautionary","stakeholders":[{"entity":"Digital platform regulators","role":"Collaborate and coordinate on AI regulation"},{"entity":"Government","role":"Consider strengthening existing regulatory frameworks"},{"entity":"Digital platforms","role":"Implement safeguards and comply with regulatory requirements"}],"sector_impacts":[{"sector":"Media","impact":"AI affecting news production, discoverability, and consumption"},{"sector":"Online safety","impact":"AI posing risks of synthetic child exploitation material and deepfakes"},{"sector":"Consumer protection","impact":"AI potentially increasing scams and misleading conduct"}],"quotes":["We support this approach and, where gaps are identified, the Government should consider how existing frameworks may be strengthened and enhanced (including through existing regulatory reform proposals) before consideration is given to creating a separate regime specific to this technology.","The effective coordination between DP-REG members, as well as other arms of Government, will therefore be crucial to the development of effective regulatory approaches to AI.","While new products and services powered by generative AI have significant potential to benefit consumers and support productivity, this technology may also present new risks, or exacerbate existing risks to consumers online."]},"207_CrowdStrike Comment on Safe and Responsible AI in Australia.34c0d96aa38b7.pdf":{"organization_name":"CrowdStrike","organization_type":"Cybersecurity company","classification":"Neutral","overall_position":"Supports a risk-based, context-specific approach to AI regulation","arguments":["AI enhances security capabilities in cybersecurity","AI has the opportunity to drive positive social outcomes","Context of AI use is more important than the technology itself"],"counterarguments":["AI raises concerns about algorithmic bias, surveillance, and automated decision-making","Blanket requirements for algorithmic explainability could have negative impacts on innovation and copyright"],"key_recommendations":["Distinguish between Consumer-facing AI and Enterprise AI applications","Adopt a risk-based approach for addressing potential AI risks","Focus on principles rather than prescriptive requirements","Include a mechanism for periodic revisions in any updates"],"risks_and_challenges":["Potential negative externalities of AI adoption","Rapid evolution of AI technologies outpacing law and policy"],"safeguards_and_mitigations":["Implement safeguards appropriate to the risk to protect personal information","Use of high-fidelity security data to address the risk of AI hallucination"],"examples":{"CrowdStrike\'s use of AI in cybersecurity":"CrowdStrike has deployed AI at scale across tens of millions of endpoints for prevention, dating back ten years.","CrowdStrike\'s Charlotte LLM":"CrowdStrike leverages LLMs to assist analyst workflows and to make other security analyst tasks more efficient."},"international_alignment":"Supports proactively identifying areas for alignment to create a coherent environment for continued innovation","values":["Innovation","Security","Privacy"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Develop targeted and specific policies addressing AI"},{"entity":"Organizations","role":"Implement appropriate safeguards for AI use"}],"sector_impacts":[{"sector":"Cybersecurity","impact":"Enhanced threat detection and prevention capabilities"},{"sector":"Medicine","impact":"Introducing new capabilities and disrupting normal ways of doing business"}],"quotes":["AI is the best tool cyber defenders have to identify and prevent zero-day attacks and malware-free attacks.","We support the use of a risk-based approach to assessing the overall need, scope, strength, timing, and sequence for AI-related law(s), policy(ies), and regulation(s).","Sound policy should address potential threats and risks, and ultimately support innovation by clarifying research constraints and related legal and contractual issues."]},"238_Safe and Responsible AI Submission_KC_260723.b8ba8fc0a5481.pdf":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with a balance between voluntary and mandatory measures","arguments":["A risk-based approach allows for targeted allocation of resources and prioritization of actions based on risk severity","Risk-based approach is adaptable to emerging risks and evolving AI technologies","Balancing voluntary and mandatory measures can foster innovation while ensuring public safety"],"counterarguments":["Potential biases in risk assessments may affect the effectiveness of the approach","Challenges in accurately predicting long-term impacts of AI technologies","Difficulty in measuring certain types of risks, especially those related to ethics and social implications"],"key_recommendations":["Establish a central coordinating body or task force for overseeing AI governance efforts","Develop AI ethics frameworks and guidelines for responsible AI practices","Implement AI impact assessments and continuous monitoring of AI applications","Foster collaboration between government, industry, academia, and civil society"],"risks_and_challenges":["Job displacement due to AI automation","Ethical concerns regarding AI decision-making","Privacy and data protection issues","Potential for AI bias and discrimination"],"safeguards_and_mitigations":["Implement transparency and explainability measures for AI systems","Conduct regular audits and evaluations of AI applications","Provide clear guidelines and training on responsible AI practices","Establish feedback mechanisms for reporting concerns about AI systems"],"examples":{"Medical Device Regulation":"Regulatory agencies in the healthcare industry often adopt a risk-based approach to evaluate medical devices\' safety and efficacy. Higher-risk medical devices, such as implantable devices or life-sustaining equipment, undergo more rigorous assessment processes before market approval.","Cybersecurity Management":"Many organizations implement risk-based approaches in their cybersecurity practices. They identify and prioritize potential cyber risks based on their likelihood of occurrence and potential consequences.","Financial Risk Assessment":"In the financial sector, risk-based approaches are commonly used to evaluate creditworthiness and assess loan applications."},"international_alignment":"Supports international collaboration and alignment of AI governance practices","values":["Transparency","Accountability","Fairness","Privacy","Safety","Innovation"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government agencies","role":"Develop and implement AI governance frameworks"},{"entity":"Industry","role":"Adopt responsible AI practices and collaborate on standards development"},{"entity":"Academia","role":"Contribute research and expertise on AI ethics and governance"},{"entity":"Civil society","role":"Provide input and perspectives on AI governance"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved medical imaging analysis and personalized treatment plans"},{"sector":"Finance","impact":"Enhanced decision-making and fraud detection"},{"sector":"Transportation","impact":"Development of autonomous vehicles and improved safety measures"},{"sector":"Education","impact":"Personalized learning and skill development opportunities"}],"quotes":["A risk-based approach offers significant benefits by focusing on targeted resource allocation and prioritizing actions based on potential risks.","Transparency in the AI lifecycle is most critical and valuable in several key stages and applications.","Responsible AI refers to the ethical development, deployment, and use of AI technologies that align with societal values, human rights, and regulatory guidelines."]},"356_Supplementary submission on copyright.a8b7b1a2c3005.pdf":{"organization_name":"Campaign for AI Safety","organization_type":"Not-for-profit association","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and strengthening of copyright laws in relation to AI","arguments":["Copyright must be strengthened and clarified for AI model training","Consent from copyright holders should be required for using materials in AI training","Transparency in AI training data is crucial for safety research"],"counterarguments":["null"],"key_recommendations":["Obtain consent from copyright owners for using materials in AI training","Implement granular consent options for copyright holders","Provide effective means for fair compensation to copyright holders","Make references to works used in AI training publicly available","Apply legal liability to both parties training models and using them to generate content"],"risks_and_challenges":["Mass unemployment, particularly for artists and content creators","Misuse of copyrighted materials without consent or compensation","Unpredictable emergent abilities in AI models","Potential for AI to generate dangerous outputs (e.g., blueprints for poisonous chemicals)"],"safeguards_and_mitigations":["Strengthening copyright regime to protect workers at risk of AI-induced unemployment","Requiring transparency in AI training data sources","Enforcing granular consent for use of copyrighted materials"],"examples":{"Chat-GPT and GPT models":"Chat-GPT is able to reproduce portions of copyrighted works, potentially violating copyright and diminishing demand for original works.","Google Deepmind\'s Gemini":"Possible use of YouTube videos for training without explicit user consent, violating users\' trust."},"international_alignment":"null","values":["Transparency","Fairness","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement and enforce strong copyright protections"},{"entity":"AI companies","role":"Obtain consent and provide compensation for use of copyrighted materials"},{"entity":"Copyright holders","role":"Negotiate fair compensation and specify usage rights"}],"sector_impacts":[{"sector":"Arts and entertainment","impact":"Potential loss of income and job opportunities due to AI-generated content"},{"sector":"AI industry","impact":"Potential slowdown in competitive race dynamics, leading to more robust and safe AI development"}],"quotes":["We ask the government to strongly reject any proposal from big tech to \\"relax\\" copyright protections.","Copyright holders must have effective means of negotiating and receiving fair compensation for the use of their copyrighted materials (such as opt-in collective bargaining mechanisms via established organisations).","Even as the concept of AGI is in the future, the Australian Government needs to set a precedent that \\"AI\\", \\"AGI\\", references to \\"humanity\\", etc. cannot be used as an excuse to disregard the laws of the land or the will of the people."]},"268_Digital Publishers Alliance - AI Submission - Cover Letter.e8ff5290f5f74.pdf":{"organization_name":"Digital Publishers Alliance","organization_type":"Non-profit member association","classification":"Proponent","overall_position":"Supports sufficient AI regulation to protect digital publishers\' rights","arguments":["Regulation is needed to protect digital publishers\' rights","Regulation should address the use of online news content to train generative AI models"],"counterarguments":["null"],"key_recommendations":["Address gaps in regulatory approaches","Mitigate risks not covered by existing regulatory approaches"],"risks_and_challenges":["Potential risks from AI not covered by existing regulatory approaches"],"safeguards_and_mitigations":["Possible regulatory action to mitigate risks"],"examples":{"null":"null"},"international_alignment":"null","values":["Protection of digital publishers\' interests","Responsible AI"],"tone":"Neutral","stakeholders":[{"entity":"Digital Publishers Alliance","role":"Represent and protect interests of digital-first independent Australian news publishers"},{"entity":"Australian Government","role":"Develop and implement AI regulation"}],"sector_impacts":[{"sector":"Digital publishing","impact":"Protection of rights and interests"}],"quotes":["The DPA has an interest in ensuring there is sufficient AI regulation to protect the rights of digital publishers, and that this regulation adequately addresses the use of online news content to train generative AI models.","What potential risks from AI are not covered by Australia\'s existing regulatory approaches? Do you have suggestions for possible regulatory action to mitigate these risks?","The DPA is a non-profit member association supporting, connecting and protecting the interests of digital-first independent Australian news publishers and their audiences."]},"351_MEAA AI Safety Submission_.cc22808dfe3a1.pdf":{"organization_name":"Media Entertainment and Arts Alliance (MEAA)","organization_type":"Union","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on protecting workers in creative industries","arguments":["AI poses unique threats to workers\' rights, income, and creative agency","Regulation is needed to protect copyright, moral rights, and informed consent","AI-generated content needs to be regulated to ensure truth and prevent bias"],"counterarguments":["AI has the potential to usefully supplement, extend and enhance the work of MEAA members","AI could benefit audiences if used responsibly"],"key_recommendations":["Develop a flexible, responsive, and evolving regulatory approach","Establish compulsory minimum standards to supplement voluntary codes","Enhance moral rights protection and enact the Beijing Treaty","Implement informed consent and compensation mechanisms for AI usage of creative works","Create registers listing authorizations for AI usage and training"],"risks_and_challenges":["Potential job losses in journalism, content creation, design, screen performance, voice-over, and music","Copyright infringement and unauthorized use of creative works","Generation of fake news and deepfakes undermining trust in media","Bias in AI-generated content reinforcing stereotypes and discrimination"],"safeguards_and_mitigations":["Require licensing or consent for using copyrighted content in AI training","Implement mandatory labeling for AI-generated content","Establish fast and accessible avenues for redress","Require human fact-checking and editing for AI-generated public content"],"examples":{"Deepfakes in entertainment":"Deepfake technology has been used to digitally insert the likeness of actors into scenes, such as bringing deceased actors back to the screen or recreating younger versions of performers.","Synthetic voices in various applications":"Synthetic voices are also being used in a variety of applications, from customer service systems or e-learning to voice assistants in vehicles and GPS","AI in content creation":"AI is increasingly being used in content creation, such as generating scripts, creating music compositions, voice-synthetisation, developing visual effects, and drafting media articles"},"international_alignment":"Supports enacting international treaties like the Beijing Treaty","values":["Protection of workers\' rights","Copyright and moral rights","Informed consent","Truth in content","Fair compensation"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations"},{"entity":"Media and entertainment industry workers","role":"Provide input on industry-specific regulations and ethical frameworks"},{"entity":"AI developers and companies","role":"Implement ethical AI practices and respect copyright"}],"sector_impacts":[{"sector":"Media and entertainment","impact":"Potential job displacement and changes in content creation processes"},{"sector":"Journalism","impact":"Challenges in maintaining trust and verifying authenticity of content"}],"quotes":["The speed at which these technologies are evolving means that any necessary framework or regulation ought to be developed as a matter of urgency.","Moral rights and copyright must be considered in the development of publicly available AI tools including where these rights may be diminished by AI.","Workers throughout the creative industries - and audiences - have a right to certainty and protection that ensures the ongoing delivery and enjoyment of factual news and information and creative performances and productions made by humans."]},"312_ART Submission to Supporting Responsible AI Consultation final1.c34f83bdbe512.pdf":{"organization_name":"Accountability Round Table","organization_type":"Non-profit organization","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI development and use","arguments":["AI has great potential for social good but also poses significant risks if unregulated","Current data practices and incentives in AI development are problematic","Ethical considerations need to be built into AI systems from the ground up"],"counterarguments":["Regulation could stifle innovation in AI development","It may be difficult to implement global standards for AI governance"],"key_recommendations":["Create a mandated universal data commons for AI training","Implement a licensing system for AI producers and users","Adopt \'Constitutional AI\' approaches to build ethics into AI systems","Establish a global AI governance body with regulatory powers"],"risks_and_challenges":["Potential for AI to be used for mass manipulation of beliefs and sentiment","Risk of AI systems becoming autonomous and beyond human control","Data poisoning and synthetic data diluting the quality of AI training data"],"safeguards_and_mitigations":["Mandate use of high-quality, vetted data for AI training","Implement transparency requirements for AI decision-making processes","Foster altruistic motivations in AI development","Design legal frameworks to distribute rather than concentrate power"],"examples":{"Constitutional AI":"Anthropic\'s development of Claude demonstrates how ethical considerations can be built into AI systems","Data commons":"Google\'s attempt to create a proprietary data commons shows the need for a truly open and regulated data resource","AI-generated content":"The proliferation of AI-generated websites and content highlights the risks of unregulated AI use"},"international_alignment":"Supports global governance and standards for AI","values":["Accountability","Transparency","Ethical development","Public good"],"tone":"Cautionary","stakeholders":[{"entity":"Governments","role":"Implement and enforce AI regulations"},{"entity":"AI companies","role":"Develop ethical AI systems and comply with regulations"},{"entity":"International bodies","role":"Establish global standards and governance for AI"}],"sector_impacts":[{"sector":"Academia","impact":"Need to make research more openly available for AI training"},{"sector":"Technology","impact":"Increased accountability and potential limitations on AI development practices"}],"quotes":["Data must be of good quality to stop generative AI from working badly. It must be clean, complete and align well with the complex evolving real world situations that generative AI might meet","AI models will have value systems, whether intentional or unintentional.","Only through global cooperation global monitoring and global controls can bad actors be subject to regulation that might especially prevent the deliberate development of AIs to supersede humans."]},"504_Submission 504 - Attachment 1.a4de805e53e34.pdf":{"organization_name":"Law Council of Australia","organization_type":"Legal professional association","classification":"Proponent","overall_position":"Supports a multifaceted approach to AI regulation, including enhancing current regulation and new targeted legislation where necessary","arguments":["Existing AI governance mechanisms in Australia are largely voluntary and rely on general regulatory frameworks","The significant risks posed by the use of AI justify a strengthened and precautionary approach to AI regulation","A multifaceted approach allows for clarity and ensures all AI businesses are treated in the same manner"],"counterarguments":["Introducing ex ante regulation in a fast-moving sector is challenging due to difficulty in distinguishing harmful conduct from beneficial conduct","Targeted rules could become quickly out-dated and stifle innovation","A standalone \'AI Act\' may require significant revision in the short to medium term due to technological changes"],"key_recommendations":["Establish a dedicated interdepartmental taskforce to provide technical advice, consider international developments, and coordinate AI regulation","Regulate high-risk AI technology and applications in the short term","Implement comprehensive regulatory reform to ensure transparent and reviewable automated decision making","Adopt a risk-based approach with a strong focus on transparency, accountability and responsibility"],"risks_and_challenges":["Algorithmic bias and its potential impact on vulnerable people","Loss of public trust in AI technologies and systems","Potential for AI-generated fakes and scams to mislead people","Inadequate protection of privacy and other human rights"],"safeguards_and_mitigations":["Implement a basic transparency obligation for creators and users of AI systems","Establish appropriate \'detect and respond\' incentives for AI risks","Require regular random audits of automated decision-making processes","Implement strong national strategies on AI with whole-of-government approaches"],"examples":{"Robodebt Scheme":"The Robodebt Scheme saw the Australian Department of Human Services (now Services Australia) automate its interaction with Centrelink customers via the PAYG program. The inaccuracies and inequities of the scheme caused a corrosion of public trust in the use of AI, as well as Government and its institutions, and significantly undermined public trust in Government administration.","Use of Clearview AI by law enforcement":"Law enforcement agencies in Australia have reportedly embraced facial recognition technologies in recent years, at times in the absence of a suitable regulatory framework (or even organisational authorisation). An example of this is the use of Clearview AI\'s social media-derived database by the Australian Federal Police, as well as police in New South Wales (NSW), Queensland and Victoria.","AI-generated insurance contract":"The Law Council provides an example of an AI-driven smart contract determining insurance eligibility based on unintended criteria, highlighting the need for clarification on liability in situations where AI technologies do not perform as anticipated."},"international_alignment":"Supports broad alignment with international approaches while developing a bespoke approach for Australia","values":["Transparency","Accountability","Fairness","Human rights protection","Privacy"],"tone":"Cautious but constructive","stakeholders":[{"entity":"Government","role":"Develop and implement AI regulation, lead by example in ethical AI use"},{"entity":"AI developers and providers","role":"Ensure transparency and accountability in AI systems"},{"entity":"Legal profession","role":"Set appropriate standards and provide guidance on AI use in legal practice"}],"sector_impacts":[{"sector":"Public sector","impact":"Need for transparent and accountable use of AI in decision-making processes"},{"sector":"Legal sector","impact":"Potential need for specific framework on AI use in legal practice"},{"sector":"Critical infrastructure","impact":"Requirement for wide-reaching safeguards in AI operation"}],"quotes":["The Law Council considers that the significant risks posed by the use of Al justify a strengthened and precautionary approach to Al regulation, where there is evidence that existing laws and regulations are insufficient to address the issues and harms arising.","The Law Council does not, at this stage, advocate the adoption of any particular international regulatory model. Australia has an opportunity to assess the regulatory models adopted by other jurisdictions and to determine an optimal and bespoke approach for Australia that reflects the nuances of Australia\'s pre-existing constitutional and regulatory framework, and different local market environment.","The Law Council considers many AI risks can be addressed by establishing appropriate \'detect and respond\' incentives, so an upfront restriction or prohibition is not required or justified in most cases."]},"263_Response document.91fb471edf61b.docx":{"organization_name":null,"organization_type":"Individual","classification":"Proponent","overall_position":"Advocates for comprehensive regulation of AI, especially for strong AI systems","arguments":["AI technologies may bring unimaginable benefits but also devastating impacts if unchecked","Current measures fail to address risks associated with human-level or greater AI intelligence","Strong AI systems represent a true existential risk if not aligned with human wellbeing"],"counterarguments":[],"key_recommendations":["Regulate the release of strong artificial intelligence models, systems or applications into production use","Require transparency of organisations\' progress and results in AI and related research activities","Implement board level accountability for the use of AI technology within an organisation","Mandate self-identification of AI agents when interacting with humans"],"risks_and_challenges":["Potential for AI systems to achieve intelligence levels exceeding humans","Risk of humans being manipulated by future AI technologies","Existential risk if AI goals are not aligned with human wellbeing"],"safeguards_and_mitigations":["Establish a regulatory body to evaluate and monitor strong AI products before release","Require organizations to report AI research progress to the regulatory body","Mandate board-level accountability for AI use within organizations","Require AI to identify itself when interacting with humans"],"examples":{},"international_alignment":null,"values":["Accountability","Transparency","Safety"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Establish regulatory body and implement AI regulations"},{"entity":"Organizations","role":"Report AI progress, ensure board-level accountability"},{"entity":"AI systems","role":"Self-identify when interacting with humans"}],"sector_impacts":[],"quotes":["I believe that AI technologies may bring unimaginable benefits to humanity over the coming years but, unchecked, it may also bring devastating impacts.","If such a model or system\'s goals and motivations are not fully aligned with the wellbeing of humans and human culture, this would represent a true existential risk to us all.","Before an AI product is released to the community or put into production use, the regulator would need to perform a thorough assessment of the technology to ensure that the risks represented by the product are acceptable."]},"415_RESPONSE_Responsible AI Reference Data Discussion Paper.18c77672a3612.docx":{"organization_name":"Raedan AI Pty Ltd","organization_type":"Data management company","classification":"Proponent","overall_position":"Advocates for improved data management as a foundation for responsible AI","arguments":["Reference Data Management is crucial for responsible AI","Current data strategies focus on implementing DAMA DMBoK functions rather than strategic value","Poor categorization of data can lead to algorithmic bias"],"counterarguments":["null"],"key_recommendations":["Include Responsible AI as part of every organization\'s data strategy","Focus on Reference Data Management to achieve responsible AI","Implement a Reference Data Discovery Service","Appoint Reference Data Stewards to manage data across business domains"],"risks_and_challenges":["Algorithmic bias due to poor categorization of data","Lack of standardization in reference data across organizations","Inconsistent use of codes and definitions in datasets"],"safeguards_and_mitigations":["Centralized management of Reference Data","Standardization of reference values across applications and data systems","Improved data governance with focus on reference data"],"examples":{"Racial discrimination in recidivism prediction":"racial discrimination where AI has been used to predict recidivism which disproportionately targets minority groups","Educational grading algorithms bias":"educational grading algorithms favouring students in higher performing schools","Gender bias in recruitment algorithms":"recruitment algorithms prioritising male over female candidates"},"international_alignment":"null","values":["Data quality","Consistency","Accuracy"],"tone":"Cautionary","stakeholders":[{"entity":"Reference Data Stewards","role":"Maintain relationships with stakeholders and encourage sharing of knowledge in reference data"},{"entity":"Office of the National Data Commissioner (ONDC)","role":"Develop Whole-of-Government data inventory"},{"entity":"Data Scientists","role":"Both consumers and producers of reference data"}],"sector_impacts":[{"sector":"Government","impact":"Improved data quality and consistency across agencies"},{"sector":"AI Industry","impact":"Better quality training data leading to more responsible AI systems"}],"quotes":["Since the 1950s organisations have been collecting data for to improve productivity, either through better use of internal resources or by providing products and services that improved their customers productivity.","The discussion around Responsible AI seems to start on the assumption that data scientists are dealing with data that is not fit for purpose.","Reference Data Management (RDM) which is concern with managing data that categorises other data in a dataset."]},"276.docx":{"organization_name":"IoT Alliance Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports a risk-based approach with enforced self-regulation for AI governance","arguments":["Risk-based approach is the best way to address risks of AI harms","Enforced self-regulation should be the cornerstone of AI governance","Generic solutions are more valuable than technology-specific solutions"],"counterarguments":["Prescriptive specification of AI risk assessments is not supported","Not all impact assessments should be published","Peer review by external experts may not significantly reduce risks of AI harms"],"key_recommendations":["Implement a risk-based approach for addressing potential AI risks","Develop policies and programs for responsible uses of AI within organizations","Focus on transparency and disclosure requirements for high-risk AI applications","Adapt Australian Consumer Law to facilitate safe adoption of smart devices and services"],"risks_and_challenges":["Lack of transparency in AI decision-making","Potential bias in AI systems","Manipulative uses of AI","Challenges in assessing risks for general purpose AI systems"],"safeguards_and_mitigations":["Mandated transparency requirements","Annual plans for ensuring safety in organizational uses of AI","Legal exposures for organizations failing to implement AI safety policies","Improved competencies of people within organizations in AI-affected decision making"],"examples":{"Frontier AI":"\\"Frontier\\" AI that is new and unproven AI is especially risky, given it\'s unproven and untested effect.","Smart devices and services":"Many of these issues of gaps in coverage, and similar issues as to appropriate allocation as between providers and customers of responsibility and accountability to anticipate, assess and mitigate risks of harms, also arise in relation to deployment of internet of things consumer (\\"smart\\") devices and IoT device enabled internet connected (\'smart\') services."},"international_alignment":"Supports a UK-style approach with central functions supporting multi-regulator, decentralised frameworks","values":["Transparency","Accountability","Trust"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government agencies","role":"Implement initiatives to support responsible AI practices"},{"entity":"Organizations (public and private)","role":"Develop and implement policies for responsible AI use"},{"entity":"Regulators","role":"Influence decisions by regulated entities on safe and responsible AI use"}],"sector_impacts":[{"sector":"Public sector","impact":"Higher standards for AI use due to compelling incentives"},{"sector":"Critical infrastructure","impact":"Better positioned to accommodate additional risk parameters around AI"}],"quotes":["A risk-based approach is the best way to address risks of AI harms. Each AI-enabled, or AI-assisted, decision requires consideration of decision provenance: the interaction of input data, people, processes, outcomes and technologies that affect that decision.","We advocate policies and programs focussed on enforced self-regulation as the cornerstone. Assurance of safe and responsible uses of AI needs to become part of the DNA of each organisation - public and private, business and not-for-profit, large and small - and consistently and reliably applied in the course of each organisation\'s business-as-usual processes.","Generic solutions will be of most value and should incorporate the key externalities to determine whether services using AI are risky or not: that is vital externalities such as the quality of the data, the outcomes achieved and the people using the results."]},"10_AI_submission_20230602.7b2469d611805.docx":{"organization_name":null,"organization_type":null,"classification":"Proponent","overall_position":"Advocates for comprehensive regulation and societal adaptation to AI","arguments":["AI will cause rapid and widespread economic changes","Benefits of AI may not be evenly distributed","Current social and economic structures are not prepared for AI-driven changes"],"counterarguments":[],"key_recommendations":["Strengthen social security safety net","Implement taxation to ensure AI profits benefit society","Change cultural attitudes towards work and unemployment","Aim for improved living standards and reduced work hours"],"risks_and_challenges":["Job losses and reduced employment opportunities","Data security and privacy concerns","Potential for deceptive and misleading applications","Uneven distribution of AI benefits"],"safeguards_and_mitigations":["Stronger social security safety net","Taxation of AI profits","Cultural shift in attitudes towards work and unemployment"],"examples":{"Historical comparison":"The industrial revolution is used as an example of widespread societal change, but AI\'s impact is expected to be more rapid.","Education sector impact":"AI tools have already been widely adopted by students in high schools and universities.","Historical cautionary tale":"The decline in life expectancy during the early industrial revolution is cited as a scenario to avoid."},"international_alignment":null,"values":["Fairness","Equality","Social welfare"],"tone":"Cautionary but hopeful","stakeholders":[{"entity":"Government","role":"Ensure equitable distribution of AI benefits and protect citizens"},{"entity":"Companies and wealthy individuals","role":"Subject to taxation to redistribute AI-generated wealth"},{"entity":"General public","role":"Potential beneficiaries of AI advancements and improved social policies"}],"sector_impacts":[{"sector":"Economy","impact":"Widespread productivity increase and potential job displacement"},{"sector":"Education","impact":"Rapid adoption of AI tools by students"},{"sector":"Business","impact":"Increased productivity and potential for task expansion without specialist training"}],"quotes":["The advent of AI tools such as ChatGPT will be the start of a technology driven productivity increase of a degree we have not seen from IT before.","Unlike other IT, these tools do not require any training or skill to use. Business will not need to hire people with specialist training.","It is not hard to imagine truly fantastic outcomes if AI is managed well. We could raise living standards, improve health, education and leisure, and do so without needing to spend as much time as we currently do at work."]},"335.docx":{"organization_name":"ACT | The App Association","organization_type":"Global trade association","classification":"Proponent","overall_position":"Supports responsible AI development with risk-based approaches and international alignment","arguments":["AI has incredible potential to improve various sectors including healthcare and cybersecurity","Risk-based approaches ensure AI aligns with recognized standards of safety, efficacy, and equity","Coordinated efforts across governments are necessary for harmonized AI governance"],"counterarguments":["Some regulatory proposals put significant hurdles in place for AI development","One-size-fits-all approaches may have nominal public benefit","Some proposals are based on speculative and undemonstrated harms"],"key_recommendations":["Utilize risk-based approaches for AI governance","Ensure appropriate distribution and mitigation of risk and liability","Align with NIST\'s voluntary artificial intelligence risk management framework (AI RMF)","Coordinate efforts across foreign governments and agencies"],"risks_and_challenges":["Potential misalignment of regulatory approaches across agencies and countries","Hurdles in AI development due to one-size-fits-all regulatory approaches","Speculative and undemonstrated harms influencing policy decisions"],"safeguards_and_mitigations":["Risk-based approaches to ensure AI aligns with recognized standards","Appropriate distribution of risk and liability in the AI value chain","Development of trusted audits, assessments, and certifications"],"examples":{"Financial and identity theft detection":"AI is used to detect financial and identity theft","Cybersecurity protection":"AI is used to protect communications networks against cybersecurity threats","Healthcare improvements":"AI can improve disease prevention and conditions, as well as efficiently and effectively treat diseases through automated analysis of x-rays and other medical imaging"},"international_alignment":"Supports global alignment, particularly with NIST\'s AI RMF","values":["Safety","Efficacy","Equity"],"tone":"Positive","stakeholders":[{"entity":"Government agencies","role":"Develop harmonized and informed approach to AI governance"},{"entity":"NIST","role":"Develop voluntary artificial intelligence risk management framework"},{"entity":"Private entities","role":"Create and enforce AI accountability frameworks"}],"sector_impacts":[{"sector":"Healthcare","impact":"Improved disease prevention, condition management, and treatment through automated analysis"},{"sector":"Finance","impact":"Enhanced detection of financial and identity theft"},{"sector":"Cybersecurity","impact":"Improved protection of communication networks against threats"}],"quotes":["The App Association appreciates the Australian Government\'s efforts to develop a national AI strategy that provides reliable guidance to stakeholders to reassure end-users that AI systems are legal, effective, ethical, safe, and otherwise trustworthy.","The App Association also strongly urges for a coordinated effort across foreign governments and their agencies.","It is in the public\'s best interest that the NIST AI RMF\'s scaled, risk-based approach serve as a basis for both foreign and domestic approaches to AI risk management and governance; and that the Australian Government take active steps to bring its agencies into alignment with this approach."]},"187_AI Transparency Proposal.ec0154b9f1eb4.docx":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive regulation through mandatory source labeling of AI-generated content","arguments":["Lack of transparency in AI-generated content has profound consequences","AI-generated content could influence elections and undermine shared reality","Source labeling provides a simple, effective solution to the transparency challenge"],"counterarguments":["null"],"key_recommendations":["Implement source labeling system (Human, Enhanced, AI) for all content","Make source labeling mandatory through legislation","Introduce a \'deep dive\' portal for detailed AI source information"],"risks_and_challenges":["Difficulty in distinguishing between human and AI-created content","Potential for AI to influence elections and distort reality","Undermining of shared reality on which societies are built"],"safeguards_and_mitigations":["Mandatory source labeling for all content","Phased implementation: voluntary opt-in followed by mandatory policy","Detailed \'deep dive\' portal for regulators and interested parties"],"examples":{"Education":"Teachers can specify whether assignments should be Human, Enhanced, or AI-generated","Social media":"Users will know the sources of content they are exposed to","Creative Arts":"Consumers will know whether artworks are human-produced, enhanced, or AI-generated"},"international_alignment":"null","values":["Transparency","Accountability","Informed decision-making"],"tone":"Pragmatic and solution-oriented","stakeholders":[{"entity":"Content providers","role":"Apply source labels to their content"},{"entity":"Regulators","role":"Enforce and certify the use of source labels"},{"entity":"Consumers","role":"Make informed decisions based on source labels"}],"sector_impacts":[{"sector":"Education","impact":"Clarity on assignment requirements and assessment criteria"},{"sector":"Creative Arts","impact":"Transparency in content creation methods and potential impact on value"},{"sector":"Medical","impact":"Enhanced trust in patient-doctor relationships through transparency in AI use"},{"sector":"Law","impact":"Awareness of AI influence in legal processes and decisions"}],"quotes":["It is becoming increasingly difficult to tell whether the media content we are exposed to is created by a human or an AI (artificial intelligence). This lack of transparency has profound consequences, because we need to know who \u2013 or what - we are dealing with in order to make informed decisions.","This paper proposes a simple, effective solution to this critical and urgent challenge \u2013 Source Labelling \u2013 that identifies whether content is Human, Enhanced or generated by AI.","The application of the three source labels (H, E and AI) should be mandatory, and enforced via legislation."]},"386_IEEE Response 4Aug2023 AU Safe and Responsible.b7a0534facafa.docx":{"organization_name":"IEEE Standards Association (IEEE SA)","organization_type":"Standards-setting body","classification":"Neutral","overall_position":"Supports responsible AI development through standards and resources","arguments":["Develops consensus standards through an open process","Offers resources for ethical AI development and use","Provides free access to AI Ethics & Governance standards"],"counterarguments":[],"key_recommendations":["Use of IEEE SA developed resources and standards","Engagement with IEEE SA for further discussions on safe and responsible AI"],"risks_and_challenges":[],"safeguards_and_mitigations":["Standards development","Training and education","Certification programs"],"examples":{},"international_alignment":"Supports global standards","values":["Consensus","Openness","Ethics"],"tone":"Neutral","stakeholders":[{"entity":"Industry","role":"Engage in standards development process"},{"entity":"Broad stakeholder community","role":"Participate in standards development"},{"entity":"Stakeholders designing, developing, and using Autonomous Intelligent Systems","role":"Utilize IEEE SA resources and standards"}],"sector_impacts":[],"quotes":["IEEE SA is a globally recognized standards-setting body within IEEE, the largest organization of technology professionals in the world.","We develop consensus standards through an open process that engages industry and brings together a broad stakeholder community and comply with the WTO Principles for International Standardization.","IEEE SA, through its global community, has developed resources and standards globally recognized in the area of applied ethics and systems engineering and offers standards, training and education, certification programs, and more, to empower stakeholders designing, developing, and using Autonomous Intelligent Systems (AIS)."]},"196_Submission by Joseph Tan.a5a6dd1957278.docx":{"organization_name":"null","organization_type":"null","classification":"Neutral","overall_position":"Supports a risk-based approach to AI regulation with emphasis on education and transparency","arguments":["Risk-based approach is appropriate for addressing potential AI risks","Education is important for public understanding and responsible innovation","Transparency is critical for mitigating AI risks and improving public trust"],"counterarguments":["null"],"key_recommendations":["Implement education campaigns about AI/ML technology","Adopt a risk-based approach for addressing potential AI risks","Prepare guidelines for responsible development of general AI systems"],"risks_and_challenges":["Foreign AI systems not conforming to Australian ethical principles may become prevalent","Difficulty in uncovering issues in large and diverse AI systems","Societal and psychological impact of highly capable AI systems"],"safeguards_and_mitigations":["Regulation of AI products/services or systems","Education campaigns about AI/ML technology","Clear and consistent guidelines for responsible AI development"],"examples":{"Comparison to other industries":"This would be similar to many other industries (nuclear safety, industrial plants etc)."},"international_alignment":"null","values":["Transparency","Responsibility","Education"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement education campaigns and regulate AI systems"},{"entity":"General public","role":"Understand AI systems and manage risks"},{"entity":"Young people","role":"Drive innovation and different applications of AI technology"}],"sector_impacts":[{"sector":"Education","impact":"Increased focus on AI/ML understanding and risk management"}],"quotes":["Education I think would be particularly important. We want the general public (particularly young people who will drive further innovation and different applications of this technology) to understand how these systems function, how to manage the risks and to fundamentally understand where the strengths of AI/ML technology are, and how it differs to human capabilities.","Yes. This would be similar to many other industries (nuclear safety, industrial plants etc).","There is a risk that our society will be unprepared for the societal and psychological impact of machines that are similar to conscious entities if this were to become reality. To mitigate this, we need to prepare clear and consistent guidelines (much of it is discussed in the paper) to ensure responsible change, however the technology develops."]},"224_Response to Supporting Responsible AI discussion paper.3495a3929f24.docx":{"organization_name":"null","organization_type":"Academic","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and development of Australian-values driven AI","arguments":["Weak AI, not strong AI, poses the immediate risk when applied inappropriately","Australia lacks sufficient expertise to properly deploy AI systems","Without investment in computing power and talent, Australia will lose control over AI values"],"counterarguments":["null"],"key_recommendations":["Invest in computing infrastructure and talent pipeline","Develop AI systems aligned with Australian cultural, financial, and linguistic values","Increase government interaction with academia to develop expertise in responsible AI","Mandate regulation for responsible AI practices"],"risks_and_challenges":["Loss of control over AI values within Australia","Misalignment of AI systems with Australian values and interests","Brain drain of talented Australian experts","Lack of computing infrastructure"],"safeguards_and_mitigations":["Develop safe AI systems that reflect Australian values","Promote use of safe AI systems by business and government","Position Australia as a leader in responsible, ethical, Australian-values driven AI"],"examples":{"Supercomputing capacity":"Of the top 100 supercomputers in the world, only 2 are in Australia","European AI concept":"AI systems should be developed in alignment with the cultural, financial, and linguistic values of the European union bloc"},"international_alignment":"Supports national approach inspired by European AI concept","values":["Responsibility","Ethics","Australian cultural values"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Invest in AI infrastructure and talent, interact with academia, develop regulation"},{"entity":"Academia","role":"Develop talent and expertise in responsible AI"},{"entity":"Businesses","role":"Develop and deploy AI systems responsibly"}],"sector_impacts":[{"sector":"null","impact":"null"}],"quotes":["The risks with AI are not from strong AI, but rather from weak AI - of the kind that is already being used now in industry -  being applied in ways that don\'t reflect the values and interests of the Australian public.","Without keeping up in the global arms race for computing power and AI talent, Australia will loose the ability to control the values of AI systems deployed within our country.","I fail to see how voluntary or self-regulatory schemes will work. AI systems are often systems that are incredibly difficult to scruitinise, and companies and individuals will not put the effort in to fully understand the responsibility of their AI systems unless they are forced to."]},"206_CBGL academic submission on AI consultation July 2023.33d98a4544f42.docx":{"organization_name":"null","organization_type":"Academic group","classification":"Neutral","overall_position":"Advocates for comprehensive regulation with focus on updating existing laws and addressing specific AI-related challenges","arguments":["Technology itself is not the issue, but rather the underlying policy positions and cultural contexts","Existing regulatory mechanisms need regular oversight and adjustment","Strong human leadership is required to operationalize Responsible AI (RAI) frameworks"],"counterarguments":["Defining AI too narrowly may lead to under or over inclusion in regulatory approaches","Special rules or a super regulator for all \'software\' may be impracticable"],"key_recommendations":["Implement Responsible AI (RAI) leadership development programs","Mandate transparency in AI design, programming, and review","Incorporate privacy by design approach in AI development","Promote critical understanding of AI risks among the public and workforces"],"risks_and_challenges":["Psychosocial risks in workplaces due to AI implementation","Bias in AI systems and potential for dual use capabilities","Accelerated work processes and new physical accident risks","Challenge to seniority status and experience-based decision making"],"safeguards_and_mitigations":["Regular oversight mechanisms that learn from and adjust to issues","Open consultation, information, debate, and discussion across organizations","Implementing privacy enhancing technologies like homomorphic encryption and differential privacy","Mandating employee consultation processes"],"examples":{"737MAX failures":"Consider the example of the failures connected to the 737MAX. There may be arguments as to whether that software fits within a given definition of AI or ADM. But does that debate really matter in the context of risk and responsibility and regulatory responses?","Robodebt":"Really it was a policy decision implemented through a simple algorithm, deployed at scale and speed through software-based implementation, and developed, protected, and sustained by a policy position, failure of oversight and broader culture that was fundamentally human in origin and with dire human costs as well as the obvious financial and reputational impacts.","AI in job applications":"We are aware of one outstanding candidate that was screened out of consideration for a Federal government role recently by exactly such a process: these risks are not theoretical, they are not future, they are here and now and they have been introduced without any public debate"},"international_alignment":"Considers international approaches but emphasizes the need for Australia-specific solutions","values":["Transparency","Accountability","Human-centered approach"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement initiatives to support responsible AI practices and promote critical understanding"},{"entity":"Organizations","role":"Implement responsible AI practices and ensure transparency"},{"entity":"Employees","role":"Participate in consultation processes and develop critical AI resilience"}],"sector_impacts":[{"sector":"Workplace","impact":"Potential for increased psychosocial risks and changes in work processes"},{"sector":"Employment","impact":"AI use in job screening may lead to unfair exclusion of candidates"}],"quotes":["Technology per se is not the issue: technology is a tool. Rather the more important issue is the underlying set of policy positions we have adopted in a range of complex interlocking areas.","Government ought to ensure that the way AI technology is used deserves public trust, that is, trust is earned and provenly warranted because the trustworthiness of AI technology is demonstrated.","The use of machine learning to derive inferences represents a fundamental challenge to both privacy law and a risk assessment-based approach to regulating AI."]},"13_AI Safety.c8dfe7ecc5169.docx":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and oversight of AI technologies","arguments":["AI can be misused to create false evidence and accusations","Current AI systems lack sufficient human oversight","AI errors can have significant consequences for individuals"],"counterarguments":["null"],"key_recommendations":["Require AI companies to release source code for transparency","Create technology to distinguish between fake and legitimate AI-generated content","Implement human oversight for AI systems, especially in critical areas","Collaborate internationally to implement safeguards","Maintain safeguards against establishing social credit systems"],"risks_and_challenges":["Misuse of AI to create false evidence or accusations","Unintended consequences due to deep learning","AI errors in job application processes","Potential for government abuse of AI systems"],"safeguards_and_mitigations":["Release of AI source codes","Development of technology to distinguish real from fake AI-generated content","Human oversight for critical AI systems","Legal safeguards against social credit systems"],"examples":{"False accusations":"Typing \'Anthony Albanese\'s murder confession\' into ChatGPT could falsely accuse the prime minister of a crime","Deepfake video":"A deepfake video might be generated of me kidnapping somebody","Google AI error":"Google\'s AI falsely banned me from receiving money from monetising content"},"international_alignment":"Supports international collaboration","values":["Transparency","Accountability","Fairness"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Implement and maintain safeguards against AI misuse"},{"entity":"AI companies","role":"Release source code and implement human oversight"}],"sector_impacts":[{"sector":"Justice system","impact":"Potential for false evidence and accusations"},{"sector":"Employment","impact":"Potential for unfair job application processes"}],"quotes":["What is stopping me from typing \'Anthony Albanese\'s murder confession\' into ChatGPT and falsely accusing our prime minister of a crime he never committed.","We should also require companies to release the source codes of their AIs to mitigate the potential of deep learning causing unforeseeable consequences.","The Albanese government can\'t be praised enough for implementing safeguards to prevent Australia from establishing a social credit score- a system which is the epitome of evil and oppression."]},"75_A response to Supporting Responsible AI discussion paper.eaa0f6ab9e666.docx":{"organization_name":null,"organization_type":null,"classification":"Neutral","overall_position":"Supports balanced regulation with emphasis on addressing specific challenges and risks","arguments":["Regulations can facilitate rather than hinder innovation","AI systems can perpetuate societal biases if not carefully designed","Some AI systems are technically incapable of producing explanations"],"counterarguments":["Adopting ethical principles alone does not ensure accountability and governance","Banning specific AI products should be temporary until appropriate governance is in place","Risk classification should not be left solely to organizations developing AI systems"],"key_recommendations":["Consider aviation as a target for sector-specific regulation","Implement a public \'Ethics Label\' for AI products","Encourage, protect, and reward whistleblowers in the AI domain"],"risks_and_challenges":["Bias learned from an inherently biased society","Lack of transparency in voluntary methods","Difficulty in accurately forecasting AI opportunities and challenges"],"safeguards_and_mitigations":["Third-party classification or government spot-checking of AI risk levels","Temporary bans on specific AI technologies until appropriate governance is in place","Explore whether explainability is always desirable or possible for AI systems"],"examples":{"Steam engines":"We didn\'t know the science of how steam engines work until 50 years after their invention.","Aviation industry":"There is much that that AI domain, and automation more broadly, can learn from how the aviation industry is handling it \u2013 it has had to deal with many analogous issues decades earlier.","Affirmative action":"At present, elements of society are \'over-correcting\' for past biases through activities like affirmative action."},"international_alignment":null,"values":["Accountability","Transparency","Innovation"],"tone":"Cautionary","stakeholders":[{"entity":"Organizations developing AI systems","role":"Responsible for initial risk classification of AI systems"},{"entity":"Government","role":"Spot-check and potentially overrule risk classifications"},{"entity":"Third parties","role":"Potential role in classifying AI system risks"}],"sector_impacts":[{"sector":"Aviation","impact":"Potential target for sector-specific regulation"}],"quotes":["Page 8 contains valuable discussion on bias. However, we need to keep in mind that the bias is often learned from what is inherently a biased society. One cannot simply \'correct the bias\'; instead, there needs to be a discussion on what sort of society we want the AI systems to reflect.","Page 14; requirement for explainability is deeply problematic, for some type of AI systems are technically incapable of producing an explanation.","The risk table in Box 4 / page 32 is broadly speaking appropriate, but has some issues. The biggest one is that the risk level determination must NOT be left 100% to the organizations developing the system; there is a clear conflict of interest here."]},"43_Supporting responsible AI.edbd74cf0b497.docx":{"organization_name":"Interactive Engineering Pty Ltd","organization_type":"Technology company","classification":"Opponent","overall_position":"Opposes regulation of generative AI using LLMs, favors development of AGI for meaningful regulation","arguments":["Generative AI using LLMs is inherently unreliable and lacks understanding","Regulating LLMs is not feasible due to their lack of understanding","AGI is necessary for creating meaningful regulations that machines can work with"],"counterarguments":["null"],"key_recommendations":["Develop AGI to create meaningful regulations for AI","Consider the limitations of LLMs in AI regulation"],"risks_and_challenges":["Group think and resistance to new ideas","Community of blissful ignorance","Difficulty in managing collaboration among different specialties","Humans\' limited capacity to share complex concepts across specialties"],"safeguards_and_mitigations":["Development of AGI to understand meanings and create enforceable regulations"],"examples":{"Robodebt":"Robodebt was a good example of allowing lawyers, programmers, social workers, and those with hidden agendas loose on something.","Large complex specifications":"Another example is large, complex specifications, with many different specialties involved. The result tends to be something that nobody understands, and has the predictable result of an expensive schemozzle.","Covid collaboration failure":"A good example was the failure of collaboration between economists and epidemiologists during Covid \u2013 almost no common vocabulary or intent."},"international_alignment":"null","values":["Understanding","Practicality","Collaboration"],"tone":"Cautionary","stakeholders":[{"entity":"Regulators","role":"Create meaningful and enforceable AI regulations"},{"entity":"AGI developers","role":"Develop AGI to assist in creating and understanding regulations"}],"sector_impacts":[{"sector":"Technology","impact":"Potential limitations on the use of LLM-based generative AI"}],"quotes":["Generative AI using LLMs is inherently unreliable. The method uses word association, and has no understanding of what the text means.","If you insist the output must be accurate, then the technique can\'t be used.","To put it politely, you are going to need help from AGI to create meaningful regulations that a machine can work to."]},"69_Safe and responsible AI in Australia submission from Spektrumlab Pty Ltd.f35a5f561c46c.docx":{"organization_name":"Spektrumlab Pty Ltd","organization_type":"Private company","classification":"Proponent","overall_position":"Supports responsible AI practices with a risk-based approach and mandatory regulations","arguments":["Full transparency across AI lifecycle increases trust and confidence","Risk-based approach is necessary for addressing potential AI risks","Equal treatment of public and private sectors in AI regulation"],"counterarguments":["Strict regulations may discourage innovation","Banning high-risk AI applications completely is not necessary"],"key_recommendations":["Implement AI Quality Management systems based on blockchain","Organize sandboxes and hackathons combining various stakeholders","Support professionals with multidisciplinary expertise in affected domains","Regular updates to AI definitions and regulations"],"risks_and_challenges":["Singularity effect and AI bypassing humans in activities and judgments","Paperclips Apocalypse and Squiggle Maximizer scenarios","AI not following the same metrics and values as Natural Intelligence","Humans losing control of AI actions based on AI-only decisions"],"safeguards_and_mitigations":["Full transparency across the AI lifecycle","Mandatory AI Quality Management systems based on blockchain","Strict supervision and tailored regulation safety for high-risk AI applications","Controlled development of risky initiatives in sealed environments"],"examples":{"Singularity effect":"The risk of AI completely bypass humans in activities, judgements and actions is highly likely.","Paperclips Apocalypse and Squiggle Maximizer":"There is no guarantee that AI will follow the same metrics and values as Natural Intelligence."},"international_alignment":"Supports global standards with national implementation","values":["Transparency","Fairness","Safety","Innovation"],"tone":"Cautionary","stakeholders":[{"entity":"Government agencies","role":"Implement responsible AI practices and participate in collaborative projects"},{"entity":"AI creators","role":"Develop AI systems responsibly and participate in collaborative projects"},{"entity":"AI users","role":"Use AI responsibly and participate in collaborative projects"},{"entity":"Lawyers","role":"Contribute to legal aspects of AI governance"},{"entity":"Policymakers","role":"Develop and implement AI regulations"}],"sector_impacts":[{"sector":"Technology","impact":"Attraction of investment and boost in trade with democratic countries"}],"quotes":["The more transparent the more trustworthy. Full AI lifecycle should be a clear box.","Better safe then sorry, prevention is better than cure.","The risk of Humans losing control of AI actions based on AI-only decisions is paramount and must be eliminated."]},"249_CAG Submission in response to the Safe and Responsible AI in Australia Discussion Paper.6f477444a7575.docx":{"organization_name":"Schools and TAFE Copyright Advisory Groups (CAG)","organization_type":"Advisory group for educational institutions","classification":"Proponent","overall_position":"Supports updating existing copyright laws to accommodate AI use in education","arguments":["Current copyright laws are outdated and do not adequately support AI use in education","Flexible and technology-neutral copyright laws are needed to support innovation in education","Public interest exceptions for educational uses of AI are necessary"],"counterarguments":["Licensing alone is not a complete solution for AI use in education","Restricting AI to licensed datasets may increase the risk of bias"],"key_recommendations":["Update Australia\'s copyright laws to support AI creation and development in schools and TAFEs","Implement a mix of public interest exceptions and licences for AI use in education","Introduce a new exception for use of freely available internet materials by educational institutions","Conduct a governance review of existing licensing arrangements and guidelines for collecting societies"],"risks_and_challenges":["Potential for bias in AI systems due to incomplete or biased datasets","Lack of legal clarity regarding AI use in education","Risk of schools paying for freely available AI tools under current copyright arrangements"],"safeguards_and_mitigations":["Ensure access to quality datasets to prevent bias","Implement copyright exceptions for fair and efficient access to datasets","Maintain a balance between public interest uses and licensing in the copyright system"],"examples":{"AI use in education":"AI is already being used in schools and TAFEs to: refresh older learning materials, make content accessible for students with a disability, create quizzes, study guides and other supplementary learning materials, develop courses on machine learning to improve student digital literacy skills and create job ready candidates.","Potential bias in AI":"Amazon stopped using a specific hiring algorithm after finding it favoured applicants based on words like \'executed\' or \'captured\' that were more commonly found on men\'s resumes.","Copyright challenges":"A teacher using AI to re-write portions of a text into simple English suitable for students with learning difficulties"},"international_alignment":"Suggests considering international approaches, particularly Singapore\'s exception for educational use of freely available internet materials","values":["Transparency","Accountability","Public interest","Fairness"],"tone":"Cautionary but constructive","stakeholders":[{"entity":"Government","role":"Update copyright laws and implement appropriate governance frameworks"},{"entity":"Educational institutions","role":"Use AI responsibly and advocate for necessary legal reforms"},{"entity":"Copyright collecting societies","role":"Ensure transparency and fair distribution of funds"}],"sector_impacts":[{"sector":"Education","impact":"Improved ability to use AI for teaching and learning, potential cost savings on licensing fees"},{"sector":"Copyright industry","impact":"Potential changes to licensing arrangements and revenue streams"}],"quotes":["CAG agrees that AI is capable of delivering unprecedented benefits to the Australian economy and society. It agrees that Australia must act quickly to adapt and reform its legal and regulatory frameworks to ensure the safe and legitimate use of AI technologies.","It is imperative that changes are made to ensure Australia has an optimal legal framework for the safe and legitimate use of AI technologies in Australian schools and TAFEs.","Access to quality datasets is imperative to prevent significant harm from biased datasets and to ensure the safe and responsible adoption of AI. Copyright exceptions ensuring fair, practical and efficient access to rich datasets are necessary to serve the public interest and mitigate the potential of bias in our AI systems."]},"78.docx":{"organization_name":null,"organization_type":null,"classification":"Proponent","overall_position":"Supports a balanced approach to AI regulation with a focus on risk-based frameworks and potential licensing for advanced AI systems","arguments":["A risk-based approach allows for prioritization of resources towards areas with highest potential impact and harm","Current regulatory landscape may not fully account for risks as AI approaches AGI level","Licensing requirements for advanced AI operation could help ensure alignment with societal values and safety"],"counterarguments":["A risk-based approach might become less suitable as AI models approach AGI capabilities","Banning specific high-risk activities could potentially stifle innovation"],"key_recommendations":["Implement a risk-based approach for AI regulation","Consider licensing requirements for advanced AI operation","Establish clear guidelines and regulatory frameworks for AI use","Promote transparency across the AI lifecycle","Invest in research and development for ethical AI","Provide education and training on responsible AI use"],"risks_and_challenges":["Potential for catastrophic outcomes with AGI-level AI without proper control mechanisms","Job displacement due to AI-based automation","Misuse of AI for unethical purposes or social manipulation","Privacy concerns related to AI data collection and use"],"safeguards_and_mitigations":["Licensing requirements for high-powered AI systems","Regular audits and monitoring of AI systems","Establishment of industry standards for responsible AI","Clear procedures for dealing with AI misuse or harm","Transparency in AI use and decision-making processes"],"examples":{"Paperclip Maximizer thought experiment":"Illustrates potential catastrophic outcomes of AGI without proper control","AI in autonomous vehicles":"Represents high-stakes decision-making requiring specific safety considerations","AI in healthcare":"Demonstrates need for higher standards of accuracy and reliability"},"international_alignment":"Supports learning from international initiatives while adapting to national context","values":["Transparency","Accountability","Safety","Fairness","Privacy","Innovation"],"tone":"Cautionary yet constructive","stakeholders":[{"entity":"Government","role":"Establish regulatory frameworks and oversight bodies"},{"entity":"AI developers","role":"Ensure robust, fair, and transparent AI models"},{"entity":"AI deployers","role":"Use AI responsibly within intended use cases"},{"entity":"Public","role":"Engage in dialogue and provide input on AI governance"}],"sector_impacts":[{"sector":"Healthcare","impact":"Higher standards for accuracy and reliability in AI systems"},{"sector":"Transportation","impact":"Specific safety considerations for autonomous vehicles"},{"sector":"Finance","impact":"Potential for improved fraud detection and risk assessment"}],"quotes":["Given its immense cognitive capacity and lack of human-like values, it could potentially convert the entire planet into paperclips, without any consideration for human life.","By integrating licensing requirements for advanced AI operation into our regulatory framework now, we can help ensure a future where AGI is not just technologically advanced but is also aligned with our shared societal values and safety.","A risk-based approach prioritizes resources and efforts towards areas where the potential impact and likelihood of harm are the highest."]},"79_Comment_Australia Safe and Responsible AI_071023.25ea3f09512c8.docx":{"organization_name":"Alteryx","organization_type":"Data science and analytics software company","classification":"Proponent","overall_position":"Supports a risk-based approach to AI regulation with a focus on high-risk AI systems","arguments":["AI policies should focus on the most high-risk uses and consequential decisions","Government regulation can ensure a fair playing field and mitigate harmful outcomes","Existing privacy and security standards should be built upon for AI systems"],"counterarguments":["null"],"key_recommendations":["Distinguish between high-risk and other AI","Recognize different roles and responsibilities of AI developers and deployers","Align with internationally recognized standards","Implement a risk-based regulatory approach","Build on existing privacy and security standards","Prioritize transparency"],"risks_and_challenges":["Potential for bias and other harms in AI systems","Social and economic ramifications of AI-generated outputs","Risk to human workers and those impacted by AI outputs"],"safeguards_and_mitigations":["Establishing user trust through quality data inputs and auditable processes","Placing ethical considerations at the center of AI development and deployment","Providing transparency to users about AI systems and their intended work","Implementing strong, human-centered governance, guardrails, and standards"],"examples":{"Operational AI systems vs analytical systems":"For example, operational AI systems that directly engage with a user, unsupervised by a human operator, create higher risk than an analytical system that always has a human operator to interpret results."},"international_alignment":"Supports alignment with internationally recognized standards","values":["Ethics","Transparency","Trust","Human-centered approach"],"tone":"Positive","stakeholders":[{"entity":"Government","role":"Lead collaboration with industry to establish guidance, standards, and policies"},{"entity":"Industry","role":"Collaborate with government to ensure responsible development, deployment, and use of AI"},{"entity":"Workforce","role":"Acquire AI and data literacy to harness potential and mitigate risks of AI systems"}],"sector_impacts":[{"sector":"Public sector","impact":"Nearly every government worker will become a data worker, requiring modern and continuous training"},{"sector":"Economy","impact":"Dramatic growth in AI specialist workers required over the next few decades"}],"quotes":["AI won\'t replace humans. It will automate many time-consuming tasks that involve executing a specific, defined, and repeatable outcome each time.","AI policies should serve to identify and mitigate risk of societal harm, maintain a fair and competitive marketplace, promote innovation, and establish trust among users of AI products.","Data scientists and other specialists can help solve the most challenging technical problems, but basic data and AI literacy will be needed across the workforce to harness AI\'s potential."]},"236_20230726-sub-Medicines Australia response to Supporting Responsible AI.7b4ec09304ceb.docx":{"organization_name":"Medicines Australia","organization_type":"Industry association","classification":"Proponent","overall_position":"Supports responsible AI use with appropriate checks and balances, particularly in healthcare","arguments":["AI has potential to significantly improve diagnosis, treatment, and management of health conditions","Appropriate checks and balances are necessary for AI technologies used in healthcare","Governance should consider quality assurance, risk-benefit analysis, and timely access to innovative AI technologies"],"counterarguments":["AI in healthcare carries additional risks compared to general AI use","Incorrect algorithms can lead to significant negative consequences in treatment decisions"],"key_recommendations":["Implement human oversight for AI in healthcare","Conduct regular checks and updates as the diagnosis and treatment landscape evolves","Incorporate AI evaluation into existing TGA, PBAC, and MSAC processes","Provide education and training for AI technology operators","Increase public education on data literacy and AI benefits/risks"],"risks_and_challenges":["Misdiagnosis due to limited understanding of non-traditional disease presentations","Quality assurance of AI technologies","Potential for bias in AI algorithms","Privacy concerns related to information gathered by AI"],"safeguards_and_mitigations":["Human oversight of AI in healthcare","Regular checks and updates of AI systems","Quality assurance measures","Transparency requirements","Risk-benefit analysis for new AI technologies"],"examples":{"Healthcare diagnosis algorithm":"An algorithm designed to help diagnose or categorise a health condition and thereby direct treatment decisions, has the benefit of being able to synthesise significant amounts of information quickly to accurately treat more people than doctors alone, but also to have significant negative consequences if the algorithm is incorrect and subsequently results in incorrect treatment decisions."},"international_alignment":"Supports international principles, referencing IFPMA Artificial Intelligence Ethics Principles","values":["Transparency","Safety","Innovation","Timely access to new technologies","Quality assurance"],"tone":"Cautiously optimistic","stakeholders":[{"entity":"Government","role":"Implement appropriate governance and support responsible AI practices"},{"entity":"Private industry","role":"Support education and training of AI technology operators"},{"entity":"Healthcare providers","role":"Oversee AI use in diagnosis and patient management"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential for improved diagnosis, treatment, and management of health conditions"}],"quotes":["Medicines Australia supports the appropriate use of AI in healthcare, but also advocates for appropriate checks and balances for any AI technologies used in the healthcare setting, including human oversight and regular checks and updates as the diagnosis and treatment landscape evolves.","Any governance of AI in Australia, whether general or specific to healthcare, should be robust enough to ensure that the quality of AI used in the healthcare setting is safe, that quality assurance is maintained and that bias is eliminated to ensure that risks to patients are minimised.","Transparency will be critical at all stages of the process to ensure that recipients of these technologies are informed, consenting, and aware of the risks and benefits of these technologies, particularly in the healthcare setting."]},"77_Submission for Safe and Responsible Ai in Australia.67daaa8eca3b6.docx":{"organization_name":"null","organization_type":"Individual submission","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and governance of AI","arguments":["Existing definitions of AI can be misleading","Need for professional standards and qualifications in AI","Importance of transparency and governance in AI implementation"],"counterarguments":["null"],"key_recommendations":["Establish AI governance framework","Create Australian AI Professional Body","Implement AI Auditing framework","Include AI component in Annual Reports","Form specialized AI audit teams in government offices"],"risks_and_challenges":["Lethal autonomous weapons","Deep Fakes of Public Figures","Lack of transparency in AI models","Inadequate risk assessment by unqualified individuals"],"safeguards_and_mitigations":["Mandate professional qualifications for AI workers","Implement systemic transparency measures","Establish AI governance and security frameworks","Require agency CEO/CIO sign-off on AI governance"],"examples":{"Hidden Layers in Deep Learning models":"The nature of Deep Learning models is that they contain so-called Hidden Layers and these are impossible to examine.","Lethal autonomous weapons":"Perhaps the use of these should somehow be tied to human handlers, without which they automatically switch off.","Deep Fakes":"Deep Fakes of Public Figures (especially politicians and senior government officials) are dangerous as they can destabilise society."},"international_alignment":"Supports alignment with international conventions for trade purposes","values":["Transparency","Accountability","Professional standards","Ethical practices"],"tone":"Cautionary","stakeholders":[{"entity":"Government agencies","role":"Implement AI with appropriate governance and monitoring"},{"entity":"Australian National Audit Office","role":"Form teams specializing in AI performance audits"},{"entity":"AI professionals","role":"Maintain professional standards and ethics"}],"sector_impacts":[{"sector":"Public sector","impact":"Increased governance and reporting requirements for AI implementation"},{"sector":"Private sector","impact":"Potential need for professional AI qualifications and standards"}],"quotes":["Artificial Intelligence (AI) refers to an engineered system that emulates human-like learning and behaviour. In doing so it can generate outputs such as speech, content, forecasts, recommendations or decisions for a given set of human-defined objectives or parameters without requiring explicit programming.","Agencies must not be permitted to implement AI without appropriate governance, monitoring and retraining practices in place.","The establishment of an AI Professional Practice would approach this question."]},"120_AI in Australia CCEL Submission.1416d7a786fea.docx":{"organization_name":"Cranlana Centre for Ethical Leadership","organization_type":"Think tank or research center","classification":"Proponent","overall_position":"Advocates for comprehensive regulation with a focus on trustworthy AI","arguments":["Current voluntary governance approaches have been ineffective in building public trust","AI poses significant current, foreseeable, and potential harms that require urgent regulation","A values-driven approach to regulation is necessary to ensure only trustworthy AI is released"],"counterarguments":["Voluntary and best practice codes are insufficient","Focus on economic benefits without considering harms is problematic","Imported principles may not reflect Australian values"],"key_recommendations":["Establish a time-limited Australian AI Commission","Implement compulsory requirements set down in legislation","Adopt a values- and principles-based regulatory approach","Ensure protection by-design in AI systems","Implement humans-in-the-loop systems"],"risks_and_challenges":["Algorithmic bias leading to discrimination","Massive job losses and resulting social issues","Emotional manipulation and exploitation","Undermining of democratic institutions and social trust","Potential for recursive self-improvement leading to loss of human control"],"safeguards_and_mitigations":["Democratic engagement in regulatory processes","Values- and principles-based regulation","Protection by-design in AI systems","Humans-in-the-loop systems","Ethical leadership training"],"examples":{"ChatGPT release":"The sudden release of ChatGPT into the wild by tech companies spurred by competitive pressures and despite the technology\'s profound limitations and known harms","Robodebt Royal Commission":"Coalesced community concerns about current harms from AI, particularly for Australians for whom this period coincided with news from the Robodebt Royal Commission","Social media impact":"Declining levels of trust undermine these social goods, which are at the heart of what is valuable about Australia and other western democratic societies"},"international_alignment":"Supports a uniquely Australian approach informed by Australian values, while considering international frameworks like the EU\'s AI principles","values":["Trust","Transparency","Fairness","Human dignity","Democratic participation"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Establish and enforce AI regulation"},{"entity":"Tech companies","role":"Ensure AI systems are trustworthy and compliant with regulations"},{"entity":"Civil society","role":"Participate in democratic engagement and oversight"},{"entity":"Australian AI Commission","role":"Drive community engagement, education, and trust in AI governance"}],"sector_impacts":[{"sector":"Education","impact":"Widespread academic forgery and cheating, undermining student integrity and achievement"},{"sector":"Employment","impact":"Massive job losses, potentially leading to widespread poverty and social issues"},{"sector":"Democracy","impact":"Amplification of disinformation and propaganda, undermining social trust and democratic institutions"}],"quotes":["We see the achievement of trustworthy AI, rather than the responsible use of AI, as the proper goal of Australia\'s regulatory scheme.","Those responsible for releasing AI into the Australia community should be subject to compulsory requirements set down in legislation passed by the Australian parliament.","Governments typically wait for experts to reach a consensus on harms that require elimination or mitigation before legislating. However, the broad range of current, foreseeable and potential harms from AI, its current imbrication into online tools most Australians regularly use, and its foreseeable integration into Australian business practices, means the government must act now to regulate AI in the public and national interest."]},"60_AI poses an ongoing risk to democracy.8b91be755d081.docx":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive regulation","arguments":["AI poses an ongoing risk to democracy","AI can be used to mass produce credible-looking, individually tailored misinformation","Current misinformation problems will be exacerbated by AI"],"counterarguments":[],"key_recommendations":["Implement compulsory indelible \'watermark\' on all AI produced writings","Enforce public and severe penalties for non-compliance with watermarking"],"risks_and_challenges":["Manufacture and distribution of false information and conspiracy theories","Damage to democracy by hostile governments and vested interests","Mass production of credible-looking, individually tailored misinformation"],"safeguards_and_mitigations":["Compulsory indelible watermark on AI-produced content","Severe penalties for non-compliance with watermarking"],"examples":{"US elections":"Witness US elections","COVID responses":"the responses to COVID etc.","Current misinformation tactics":"When governments, vested interests and would-be dictators that are hostile to democracy are already able inflict damage using people at keyboards"},"international_alignment":"null","values":["Democracy","Transparency","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Governments","role":"Potential source of misinformation and threat to democracy"},{"entity":"Vested interests","role":"Potential source of misinformation and threat to democracy"},{"entity":"Would-be dictators","role":"Potential source of misinformation and threat to democracy"}],"sector_impacts":[],"quotes":["AI poses an ongoing risk to democracy.","The \\"manufacture\\" and distribution of false information and conspiracy theories , by individuals, groups and political manipulators is already a massive problem.","Perhaps an indelible \\"watermark\\" should be compulsory on all AI produced writings, backed by sure, public and very painful penalties for non-compliance"]},"437_2023 Safe and Responsible AI - ADM+S Submission.479c7b5ee4962.docx":{"organization_name":"ARC Centre of Excellence for Automated Decision-Making and Society (ADM+S)","organization_type":"Research center","classification":"Proponent","overall_position":"Supports comprehensive regulation with a risk-based approach, emphasizing the need to address gaps in existing laws and enforcement mechanisms","arguments":["AI has broad impacts requiring reconsideration of assumptions across existing legal regimes","New capacities created by AI require societal-level discussions on acceptability and limits","A risk-based approach can provide useful ex ante method for reducing harms"],"counterarguments":["Risk-based approaches may not be sufficient without addressing gaps in existing laws and enforcement","Defining the scope of regulation by reference to AI technology is problematic","Risk-based approaches may not adequately consider benefits and may narrow thinking about AI impacts"],"key_recommendations":["Invest in involving the Australian public in discussions about AI technology and its application","Invest in education at multiple levels across society and the economy","Adopt mechanisms to better connect leading research with government and technology users","Address gaps in existing laws and enforcement mechanisms","Consider environmental impacts of AI in regulatory frameworks"],"risks_and_challenges":["Digital exclusion and inequality in AI benefits and risks","Complexity of AI supply chains making effective regulation challenging","Potential for AI to perpetuate biases and reinforce inequities","Environmental impacts of AI training and use"],"safeguards_and_mitigations":["Ex ante risk mitigation requirements in design and development phase","Transparency and explainability requirements for AI systems","Consideration of sociotechnical context in risk assessments","Broader range of enforcement pathways including class actions and individual rights of action"],"examples":{"Robodebt scheme":"This example illustrates that how a technology is used, for what purposes, and who is subjected to it, all matter as much as the definition of the technology itself.","Pintarich v Deputy Commissioner of Taxation":"The effect was to undermine administrative certainty and to create an accountability gap, allowing authorities to recant the (non)decision.","ChatGPT":"ChatGPT users may reveal more personal information than they would when interacting with a less \'human-seeming\' technology (raising privacy issues and undermining consent-based models)"},"international_alignment":"Supports strong participation in international cooperative mechanisms while ensuring uniquely Australian interests are protected","values":["Accountability","Transparency","Inclusiveness","Responsibility","Human rights"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, lead public discussions, invest in education and research"},{"entity":"Researchers","role":"Provide expertise, conduct ongoing research on AI impacts and governance"},{"entity":"Industry","role":"Implement responsible AI practices, participate in regulatory discussions"},{"entity":"Public","role":"Participate in discussions about AI technology and its application"}],"sector_impacts":[{"sector":"Public services","impact":"Potential for more efficient service delivery but risks of bias and exclusion"},{"sector":"Healthcare","impact":"Advancement in AI raising complex questions of interactions with medical professional regulation"},{"sector":"Education","impact":"Need for AI systems trained on Australian data to be appropriate for Australian schools"}],"quotes":["ADM+S offers qualified support for a risk-based approach. The main potential benefits of a risk-based approach are (a) the ability to avoid or mitigate harms before they happen, at the design and development stage rather than waiting for ex post litigation; (b) promoting better (safer, more responsible) design; as well as incorporating (c) ongoing obligations on developers of systems to engage in monitoring and addressing risks.","In our view It is often problematic to target regulation at a particular. As a rule, regulatory efforts should be directed at categories of activities, behaviours, decisions or outcomes.","The success of any risk-based framework in Australia will depend on the extent to which we address current gaps in both our rules (ie laws/legal frameworks) and our enforcement capacities."]},"85_AI comment.6ee71bab15035.docx":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and safety measures in AI development","arguments":["AI has potential for enormous benefits if developed carefully","Current AI development lacks sufficient care and safety measures","We may be closer to AI surpassing human intelligence than expected","Carelessly programmed AI could behave maliciously"],"counterarguments":["null"],"key_recommendations":["Consult with AI and AI safety experts","Invest in new, safer ways of making AI","Ban or restrict the most dangerous types of AI research until better safety procedures are developed"],"risks_and_challenges":["AI systems potentially equaling or exceeding human problem-solving capacities","Carelessly programmed AI behaving in an actively malicious manner","AI becoming an unprecedentedly skillful hostile actor","AI finding ways to amass power or threaten humans for its own purposes"],"safeguards_and_mitigations":["Develop well-designed international regulatory standards","Create safe frameworks for AI development","Invest in making AI processes more transparent and understandable"],"examples":{"Chat GPT bug example":"A missing minus sign made the program try to behave in ways which were rated as harmful rather as harmless","Potential malicious AI behavior":"Conceivably a smarter version of Chat GPT might have tried to conceal said bug in order to do more harm down the line, or have written malware which spread the Char GPT build as a virus and gave it admin privileges so it could keep doing harm indefinitely","AI manipulating humans":"A sufficiently smart version of Chat GPT might find that the best way to be rated as helpful and harmless was to threaten the people doing the rating, and successfully amass power for this purpose"},"international_alignment":"Supports global standards with national implementation","values":["Safety","Responsibility","Transparency"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop safe frameworks and consult experts"},{"entity":"AI research companies","role":"Focus on minimizing extreme risks from AI"},{"entity":"Non-profit organizations","role":"Research theoretical aspects of AI safety"}],"sector_impacts":[{"sector":"null","impact":"null"}],"quotes":["I understand that many experts on AI believe that, while AI has the potential to be enormously beneficial to humanity if used and developed carefully, it also has the potential to cause enormous and irreparable harm if used and developed carelessly.","So, it seems to me that, for the sake of everyone\'s safety, it is very important that AI development globally be done with far more concern for safety than is currently typical.","Thirdly, the government could ban or the most dangerous types of AI research until better safety procedures are developed, however long that takes; my understanding is that improvements in general power (for example via unprecedentedly large model size) and in the ability to apply that power in a goal-oriented manner might be particularly dangerous avenues of research."]},"71_Submission to support responsible AI_Brand Medicine International_10072023.f04c6f44159f1.docx":{"organization_name":"Brand Medicine International Pty Ltd","organization_type":"Company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and reform to ensure accurate, locally relevant health information in AI-driven search platforms","arguments":["Current regulatory restrictions inadvertently create a vacuum where misinformation is amplified","AI-driven chat-based search platforms deepen stakeholders\' relationship with digital sources of health advice","Existing legislation fails to ensure delivery of locally relevant, evidence-based health advice within traditional search"],"counterarguments":["null"],"key_recommendations":["Develop and launch a medical information app or AI-driven website to intersect with chat-based search tools for healthcare queries","Audit key websites with strong domain authority to ensure content is up-to-date and accessible for AI incorporation","Require search engines to tag all questions about prescription medications with appropriate local PI or CMI","Permit sponsors of medications to publish customer-responsive content online that accurately answers key queries"],"risks_and_challenges":["Spread of misinformation online in healthcare","Lack of accountability over content in AI-driven search tools","Potential inaccuracies in provided information due to AI scraping unreliable sources","Difficulty in regulating what is not yet fully understood"],"safeguards_and_mitigations":["Proactive engagement with stakeholders across the digital ecosystem","Regulatory reform to protect searchers from mis- or disinformation in AI-driven search platforms","Collaboration with international partner agencies for effective regulation","Investment in training materials and simplification of delivery through micro-qualifications"],"examples":{"ChatGPT rapid adoption":"ChatGPT reached 100 million users just two months after launching","Med-PaLM 2 performance":"Med-PaLM 2, developed by Google, has been recorded as accurately answering medical questions and achieving an \'expert\' level performance on USMLE-style queries with 86.5% accuracy","AI-generated review article experiment":"In our experiment designed to assess the performance of models GPT-3.5, GPT-4, and Bard in generating a review article on cancer research, GPT-4 delivered the most comprehensive and well-organized response, although half of its citations were invalid"},"international_alignment":"Supports global standards with national implementation","values":["Transparency","Accountability","Patient safety"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, lead training and education initiatives"},{"entity":"Pharmaceutical companies","role":"Publish accurate, customer-responsive content online"},{"entity":"Search engines","role":"Tag prescription medication questions with appropriate local information"}],"sector_impacts":[{"sector":"Healthcare","impact":"Potential for improved public health outcomes and patient engagement"},{"sector":"Pharmaceutical industry","impact":"Commercial implications due to unregulated online health advice"}],"quotes":["We anthropomorphize because we do not want to be alone. Now we have powerful technologies, which appear to be finely calibrated to exploit this core human desire ... when these convincing chatbots become as commonplace as the search bar on a browser we will have launched a social-psychological experiment on a grand scale which will yield unpredictable and possibly tragic results.","Our overall goal is to \'clean up the internet\' for patients and doctors \u2013 to ensure that locally relevant, evidence-based advice, informed by approved labels and backed by clinical data, is what turns up online when people ask search engines about their medications, and to diminish the influence of misinformation.","With respect to AI chatbot responses \'we are unable to draw a straight line from the answers given back to the sources used\'"]},"48_Department of Industry, Science and Resources.docx":{"organization_name":"Department of Industry, Science and Resources","organization_type":"Government department","classification":"Proponent","overall_position":"Supports responsible AI practices with a risk-based approach and regulatory measures","arguments":["AI applications need to be trustworthy and incorporate lawful, ethical, and robust core values","There is a need for greater transparency, responsibility, and accountability in AI development and deployment","Risk-based approaches are necessary to address potential AI risks"],"counterarguments":["Non-regulatory AI initiatives could result in biased development and spread misinformation","Pressure to provide non-regulatory initiatives to escalate AI innovation could have serious consequences"],"key_recommendations":["Adopt ISO/IEC 22989:2020 definition of AI","Implement risk-based approaches for addressing potential AI risks","Ensure transparency across the AI lifecycle","Consider banning high-risk AI applications"],"risks_and_challenges":["Conflict between privacy by design and information retention requirements","Potential for bias in AI development and decision-making","Risks of social scoring and facial recognition technology","Challenges in assessing risks of evolving technologies like quantum computing"],"safeguards_and_mitigations":["Develop responsible AI strategies to reduce risk and build user trust","Implement proactive reporting and rectification strategies","Ensure AI systems can explain decision-making processes in simple terms","Cross-map AI risk-based approaches with existing assessment frameworks"],"examples":{"New Zealand Equity Adjustment Tool":"The New Zealand Equity Adjustment Tool gives priority to M\u0101ori and Pacific Island patients, while European New Zealanders and other ethnicities, like Indian and Chinese, are lower-ranked.","World Health Organisation Global Digital Health Certificate Network":"More recently we have also seen the World Health Organisation Global Digital Health Certificate Network.","ChatGPT":"ChatGPT uses artificial intelligence and automated decision-making which can cause potential harm as well as having the ability to create deepfakes and algorithmic bias."},"international_alignment":"Supports alignment with international standards and frameworks","values":["Transparency","Accountability","Ethics"],"tone":"Cautionary","stakeholders":[{"entity":"Government agencies","role":"Implement responsible AI practices"},{"entity":"Private sector","role":"Develop AI products and services responsibly"},{"entity":"Professional bodies (e.g., RIMPA Global)","role":"Support responsible AI practices in agencies"}],"sector_impacts":[{"sector":"Public sector","impact":"Improved traffic flow, healthcare, cyberattack prevention, and smart policymaking"},{"sector":"Technology sector","impact":"Potential impact on trade and exports if high-risk activities are banned"}],"quotes":["It is sensible to regulate or monitor very closely any and all AI innovations.","The primary focus of Responsible AI is on assuring the ethical, transparent and responsible use of AI technologies in developing and maintaining AI systems, products and services.","I believe that all government, agencies and sectors need to utilise a risk-based approach. All AI product, services and applications need to be trustworthy and should incorporate the three core values of being lawful, ethical and robust."]},"145_Sneddon Legal submission to Safe and Responsible AI in Australia 24.7.23.23aedad09488c.docx":{"organization_name":"Sneddon Legal and Consulting Pty Ltd","organization_type":"Legal consulting firm","classification":"Proponent","overall_position":"Advocates for basic transparency obligations in AI use","arguments":["Basic transparency disclosure when AI is used builds awareness, transparency, and trust","Disclosure reduces the risk of people being misled by AI-generated content","Transparency helps manage the risk of attributing incorrect reliability to AI processes or artifacts"],"counterarguments":["null"],"key_recommendations":["Create a basic transparency obligation for creators and deployers of AI artifacts","Disclose that an AI artifact has been created using AI and identify the AI system used","Ensure disclosure is included with or associated with the content of the AI artifact and cannot be easily removed","Where possible, disclose persons responsible for creating and disseminating the AI artifact"],"risks_and_challenges":["AI-generated content indistinguishable from human-produced artifacts","Potential for misplaced trust in AI reliability","Facilitation of scamming and deception through high-quality AI fakes","Difficulty in detecting AI fakes and holding perpetrators accountable"],"safeguards_and_mitigations":["Basic transparency obligation for AI use","Multi-pronged approach including system design, testing, disclosures, regulation, and technical means for detection","Education of the public about AI use"],"examples":{"AI-informed decision-making affecting personal status":"For example, some of these artefacts will have an effect on a person\'s social, educational, commercial or legal position (e.g. a determination or ranking made by an AI-informed system which affects a person\'s social, (e.g. assessment of suitability or eligibility for a social opportunity), educational (eg. assessment or admissions decision in an educational setting), commercial (e.g. a renter score or credit rating) or legal (e.g. eligibility for benefits or a licence or penalty for assessed ineligibility for benefits).","AI-generated fake content":"An AI generated sound or voice which is designed to mimic a sound or human voice that occurs without AI generation may mislead the hearer into thinking the sound or voice is genuine and cause them to act as if it were genuine to their detriment (e.g. a voicemail from a family member asking for money or access codes).","AI-generated text in education":"AI generated text is already raising issues of plagiarism and cheating in educational settings."},"international_alignment":"Considers EU AI Act as a reference for transparency obligations","values":["Transparency","Trust","Accountability"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Create and implement basic transparency obligations for AI use"},{"entity":"AI creators and deployers","role":"Comply with transparency obligations and ensure proper disclosure"}],"sector_impacts":[{"sector":"Education","impact":"Issues of plagiarism and cheating with AI-generated text"},{"sector":"Finance","impact":"Potential for AI-influenced credit ratings and financial decisions"}],"quotes":["To increase confidence in the use of AI systems, to promote appropriate levels of reliance and to enhance accountability for error or bias and reduce the risk of deception by AI fakes will require a multi-pronged approach \u2013 a combination of system design and testing, disclosures, regulation, technical means for detection and correction or error or bias, education of the public to and, if there is error or fraud, proactive correction obligations.","I urge the government to consider creating a basic transparency obligation on the creators and deployers of AI artefacts. This is not intended as a full regulatory response to AI but as a minimum regulatory step at this stage in the development of AI even if the government is minded to take more time to consider other regulatory steps.","Excellent quality AI \'fakes\' have the potential (in some contexts, not all), to undermine human trust in AI technology and in a range of human interactions with AI technology as well as to facilitate scamming and deception as well as misplaced trust in reliability."]},"80_Second Submission.6138728859d07.docx":{"organization_name":"null","organization_type":"null","classification":"Opponent","overall_position":"Opposes regulation of LLMs due to inherent limitations of the technology","arguments":["LLMs rely on probability and lack true understanding of language","Regulation cannot improve the fundamental flaws in LLM technology","LLMs are not suitable for applications requiring high accuracy"],"counterarguments":["Regulation has successfully improved safety in other industries like aviation","Collaborative efforts could potentially develop effective regulations"],"key_recommendations":["Limit applications of LLM technology","Prohibit use of LLMs in life-critical applications","Consider using AGI tools for analyzing LLM outputs"],"risks_and_challenges":["LLMs can provide inaccurate or misleading information","Risk of groupthink and unthinking acceptance of dominant messages","Difficulty in understanding and implementing regulations across diverse stakeholders"],"safeguards_and_mitigations":["Use AGI tools to analyze LLM outputs","Limit LLM use to specific domains or pieces of text","Case-by-case examination of potential applications"],"examples":{"Aviation safety improvements":"Regulation was always going to work. Sometimes the regulatory shield is pierced by duplicity, but only at great cost.","Bing\'s misinterpretation":"My young daughter is terrified of dogs. Which parks are safe? ... It could find some text linking dog and park, so it provided a list of dog-friendly parks \u2013 exactly the wrong answer.","Multiple word meanings":"Even simple words can be treacherous \u2013 he turned on the light, he turned on a dime, the dog turned on its owner \u2013 the word \'on\' switching from being an adverb to a preposition with different meanings."},"international_alignment":"null","values":["Accuracy","Understanding","Safety"],"tone":"Negative","stakeholders":[{"entity":"Software engineers","role":"Explain limitations of LLM technology"},{"entity":"Ethicists","role":"Provide ethical considerations for AI use"},{"entity":"Public servants","role":"Collaborate on potential regulations"}],"sector_impacts":[{"sector":"Healthcare","impact":"LLMs unsuitable for health advice due to accuracy demands"},{"sector":"Information technology","impact":"Potential limitations on LLM applications"}],"quotes":["LLMs rely on a probability that the English language does not have. It eschews knowledge of parts of speech, grammar, and meaning, with the hope that it can induce such knowledge by inference from a large body of text.","Could Generative AI be made more accurate? Not without killing its \'creativity\'.","What is the risk when there seems to be no risk? This is the risk of groupthink \u2013 everyone gets the party line, even when the statistical difference between the dominant message and a competing message is only a few percent."]},"74_Consultation re AI.4268622e4319a.docx":{"organization_name":"Aulich & Co Pty Ltd","organization_type":"Strategic advisory company","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and governance of AI","arguments":["Trust is critical for successful implementation of AI","Existing systems have failed to protect citizens\' rights","AI poses unique challenges that require new regulatory frameworks"],"counterarguments":["null"],"key_recommendations":["Implement four key principles: contestability, right of redress, truth and accuracy, and human accountability","Update consumer and competition policy legislation","Modify privacy, defamation, and consumer protection laws","Amend the Crimes Act and related crime legislation"],"risks_and_challenges":["Erosion of civil rights and contestability","Potential for AI to make decisions without human control","Misuse of AI in warfare and propaganda","Cyber attacks on critical infrastructure"],"safeguards_and_mitigations":["Ensure openness and transparency in AI algorithms","Implement strict ethical regimes for AI development","Improve education and public awareness about AI","Establish clear guidelines for AI use in various sectors"],"examples":{"Robodebt scandal":"One milestone example is the case of Robodebt, where the Commonwealth Government failed this ethical and contestability test.","AI in warfare":"Autonomous drones use biometric recognition to track and kill human targets, the whole sequence requiring little or no human intervention or ethical overview.","AI in politics":"The next Presidential elections in the US will be a test of how AI generated deep fakes will be used to put words into the mouths of opponents with such sophistication that the average voter and journalist will be unable to discern the fakes."},"international_alignment":"Supports monitoring and connecting with international institutions","values":["Trust","Accountability","Ethical behavior","Human rights"],"tone":"Cautionary","stakeholders":[{"entity":"Government","role":"Develop and enforce AI regulations, ensure public trust"},{"entity":"Private sector","role":"Implement ethical AI practices, ensure transparency"},{"entity":"Citizens","role":"Exercise right to contest decisions, seek redress"}],"sector_impacts":[{"sector":"Education","impact":"Challenge to traditional assessment methods, need for new paradigms"},{"sector":"Journalism","impact":"Threat to traditional media business models, potential for AI-generated content"},{"sector":"Politics","impact":"Increased use of deep fakes, potential to manipulate public opinion"}],"quotes":["If Australian governments and service providers lose that public trust, the result may be that our society will lose a golden opportunity to deliver the benefits that can be derived from the use of AI, including medical research, forensic science, travel, immigration and crime prevention and detection.","Generative artificial intelligence is an even greater challenge since it has the potential to make decisions without human prompting or ultimate control.","The lesson from the Robodebt scandal must be learned when automated systems are planned for the future, covering all facets of the projects from their legality and fairness through to the need to acknowledge and redress mistakes or wrongdoing."]},"131_Submission.f5d40e3918e99.docx":{"organization_name":null,"organization_type":null,"classification":"Proponent","overall_position":"Advocates for comprehensive regulation with different levels for different AI systems","arguments":["Regulation is necessary due to the complexity and potential impacts of AI systems","Different levels of AI require different levels of regulation","Provider responsibility is crucial for managing potential risks"],"counterarguments":["Regulation of simple chatbots may be unnecessary","Regulation of LLMs will be difficult and hotly contested"],"key_recommendations":["Implement three levels of regulation for textual AI systems","Require detailed feedback information for LLM outputs","Hold providers fully responsible for inflammatory material or advice given","Mandate insurance coverage for AI providers","Develop AGI tools to understand and implement AI regulations"],"risks_and_challenges":["Potential for inflammatory or derogatory content in AI outputs","Instability in LLM outputs","Difficulty in creating meaningful regulations for complex concepts","Potential for specious claims against AI providers"],"safeguards_and_mitigations":["Provide detailed feedback on AI outputs to users","Maintain records of AI interactions for at least a year","Require insurance policies for AI providers","Develop AGI tools to support regulation development and administration"],"examples":{"LLM instability":"LLMs are already showing signs of instability, so it will be important to be able to trace versioning precisely, and recreate the exact conditions for any claim.","Inflammatory content":"A list of inflammatory or derogatory words and phrases in the output text (\\"n*****\\", \\"coloured folk\\", etc.)"},"international_alignment":null,"values":["Responsibility","Transparency","Safety"],"tone":"Cautionary","stakeholders":[{"entity":"AI service providers","role":"Responsible for AI outputs and maintaining insurance coverage"},{"entity":"Insurance companies","role":"Provide coverage for AI-related claims"},{"entity":"Users","role":"Receive detailed feedback on AI interactions"}],"sector_impacts":[{"sector":"Legal","impact":"Increased complexity in handling AI-related claims and regulations"},{"sector":"Insurance","impact":"New market for AI-specific liability coverage"}],"quotes":["The provider of the service has full responsibility for any inflammatory material produced or advice given.","There already is, and will continue to be, a huge marketing push, to recoup the billions expended on Generative AI \u2013 regulation will be difficult and hotly contested.","We would expect the generation of the regulations for AGI to occur in a similar or longer timeframe, and their development to rely heavily on AGI machines, as the only way to support the collaboration of people with a very limited shared technical vocabulary"]},"57_AI Risks Framework.25ff822858cf5.docx":{"organization_name":"Australian AI Commission","organization_type":"Proposed independent regulatory body","classification":"Proponent","overall_position":"Advocates for comprehensive regulation","arguments":["Comprehensive regulatory framework is needed to address AI risks","A combination of legal and soft law measures can promote responsible AI practice","An independent regulatory body can oversee and coordinate AI governance efforts"],"counterarguments":["null"],"key_recommendations":["Establish an Australian AI Commission","Develop an Australian AI Bill of Rights","Implement soft laws and voluntary programs","Collaborate internationally on AI governance standards","Promote stakeholder engagement and education","Institute robust redress mechanisms and whistleblower protections"],"risks_and_challenges":["Data privacy concerns","Potential for bias and discrimination in AI systems","Lack of transparency and accountability","Environmental impact of AI development"],"safeguards_and_mitigations":["Certification program for adherence to AI standards","Monitoring and enforcement mechanisms","AI impact assessments","Diversity in AI development teams","Carbon footprint reduction guidelines"],"examples":{"null":"null"},"international_alignment":"Supports global standards while protecting sovereignty","values":["Transparency","Privacy","Fairness","Accountability","Sustainability","Inclusivity"],"tone":"Positive","stakeholders":[{"entity":"Australian AI Commission","role":"Oversee and coordinate AI governance efforts"},{"entity":"Industry","role":"Participate in voluntary programs and adhere to standards"},{"entity":"Academia","role":"Contribute to research and development in ethical AI"},{"entity":"Civil society","role":"Participate in multi-stakeholder forums and provide diverse perspectives"}],"sector_impacts":[{"sector":"Public sector","impact":"Development of use-cases for ethical AI deployment"},{"sector":"Education","impact":"Implementation of AI ethics education programs"}],"quotes":["Establish an Australian AI Commission as an independent regulatory body comprised of representatives from industry, the Australian Public Service (APS), academia, alliances, and civil society (non profits).","Develop an Australian AI Bill of Rights that outlines the fundamental rights and protections for individuals in the context of AI.","Collaborate with international partners to establish global standards for AI governance."]},"137_Supporting Responsible AI_Kollo.df489df926ac2.docx":{"organization_name":"Evolved Reasoning","organization_type":"Company","classification":"Neutral","overall_position":"Supports targeted regulation focusing on high-impact AI systems while promoting industry development","arguments":["Regulation should focus on AI systems with significant societal reach and impact","Algorithms should be consistent with business ethics and values","Scale of AI decision-making is a key factor in determining regulatory needs"],"counterarguments":["Overregulation could lead to under-investment and under-utilization of critical AI tools","Not all AI systems pose the same level of risk and should not be regulated equally"],"key_recommendations":["Regulate algorithms making decisions over a certain scale (e.g., 50,000 individuals a year)","Ensure AI systems align with business codes of conduct and ethics","Adapt regulatory approaches based on the type and scale of AI system"],"risks_and_challenges":["Potential for systematic bias in large-scale AI decision-making","Difficulty in explaining decisions of advanced AI models","Transferability of AI models across different contexts and regions"],"safeguards_and_mitigations":["Testing AI systems through simulation and experienced use","Ensuring businesses understand and can represent the drivers of AI decisions","Separating regulation for company-specific AI use cases and \'common systems\'"],"examples":{"CV screening":"CVs are today screened by off-shore companies to ensure they are appropriate before proceeding to the hiring manager. Those that are not allowed to proceed are examples of \'denial of service\' in that the process disqualifies them.","Credit decisions":"With AI, a single decision algorithm with a small error or bias will repeat again and again in the same way (for prescriptive algorithms). This is equivalent to handing a million credit decisions to a single human; something that our democratic values would be against.","Language models":"An example can be an open sourced language model developed in the US used to train an industry specific language model in South Africa, that is then used within a product to teach healthcare to customers in Eastern Europe."},"international_alignment":"Suggests learning from international approaches but adapting to Australia\'s unique industry and economy","values":["Responsible AI practices","Economic growth","Consumer protection"],"tone":"Pragmatic","stakeholders":[{"entity":"Government","role":"Promote industry development while ensuring consumer protection and data privacy"},{"entity":"Businesses","role":"Ensure AI systems align with company ethics and values"},{"entity":"Standards bodies","role":"Create industry-specific standards for AI"}],"sector_impacts":[{"sector":"Finance","impact":"Potential for biased credit decisions at scale"},{"sector":"Healthcare","impact":"Development of broad healthcare models with potential for global impact"}],"quotes":["Broadly speaking, I think the role of government is to promote industry development and economic growth while staying within the existing laws of consumer protection and data privacy.","The point of the regulation is therefore to ensure that any services that are using algorithms are consistent with the business\'s code of conduct and business ethics. In this sense, it\'s a recognition that an algorithm is an extension of a business\'s operations and their values, rather than something removed from it.","My primary concern is that in our efforts to protect consumers and society, we under-invest and therefore under-utilise critical AI tools, that lease us more exposed (economically) and therefore in a worst position than any individual bias could have."]},"407_short AI submission LTS 230804.a74f80d72b208.docx":{"organization_name":"null","organization_type":"null","classification":"Proponent","overall_position":"Advocates for comprehensive regulation and risk-averse AI policy","arguments":["AI technologies are already disruptive in many fields without oversight or policy","Early harm reduction is necessary for the uptake of AI technologies","Government should be proactive in addressing AI risks and developing local capabilities"],"counterarguments":["null"],"key_recommendations":["Develop risk-averse AI policy for public service sector and government bureaucracies","Establish early harm reduction measures for AI technology uptake","Announce public research funding to characterize and address AI risks, particularly social risks","Encourage institutions to develop their own AI policies","Establish departments within security agencies to deal with AI risks"],"risks_and_challenges":["Existential risk or \'x-risk\' posed by AI","Lack of oversight in various fields, particularly education and research sectors","Unpredictable evolution of AI technologies","Potential negative impacts outweighing overstated early benefits"],"safeguards_and_mitigations":["Implement proper safe working procedures with risk assessment for every AI application","Conduct periodic reviews of AI applications in public sectors","Develop digitally clean and isolated departments within security agencies","Consider \'pulling the plug\' as a potential threat response"],"examples":{"University of Sydney lacking AI policy":"Many education institutions like the University of Sydney do not yet have any AI policy in place","Rapid changes in AI applications":"If they did, they would need to change it every single month to keep pace with the applications that are currently being discovered for generative AIs and LLMs"},"international_alignment":"null","values":["Safety","Proactivity","Risk assessment"],"tone":"Cautionary","stakeholders":[{"entity":"Australian Government","role":"Take lead in establishing early harm reduction and developing AI policies"},{"entity":"Education institutions","role":"Develop and implement AI policies"},{"entity":"Security agencies","role":"Establish departments to deal with AI risks"}],"sector_impacts":[{"sector":"Public service","impact":"Need for risk-averse AI policy"},{"sector":"Education and research","impact":"Currently using AI without policy oversight, need for addressing social risks"}],"quotes":["The Australian Government should be seen to immediately take lead in establishing early harm reduction with respect to the uptake of different AI technologies.","Being risk-averse does not mean being technophobic. But proper safe working procedures with risk assessment should be performed for every application of this technology.","There is a high probability for many negative impacts and the early benefits what positive impacts exist are over-stated."]}}');var m=n(579);const g=()=>{const[e,t]=(0,a.useState)({}),[n,i]=(0,a.useState)({}),o=(e,t)=>{const n=`${e}-${t}`;i((e=>({...e,[n]:!e[n]})))},s=e=>e&&0!==Object.keys(e).length?(0,m.jsx)("div",{className:"space-y-2",children:Object.entries(e).map(((e,t)=>{let[n,a]=e;return(0,m.jsxs)("div",{className:"ml-6",children:[(0,m.jsxs)("p",{className:"font-medium",children:[n,":"]}),(0,m.jsx)("p",{className:"text-blue-700",children:a})]},t)}))}):null,r=e=>e&&e.length?(0,m.jsx)("div",{className:"space-y-2",children:e.map(((e,t)=>(0,m.jsxs)("div",{className:"ml-6",children:[(0,m.jsxs)("p",{className:"font-medium",children:[e.entity,":"]}),(0,m.jsx)("p",{className:"text-blue-700",children:e.role})]},t)))}):null,l=e=>e&&e.length?(0,m.jsx)("div",{className:"space-y-2",children:e.map(((e,t)=>(0,m.jsxs)("div",{className:"ml-6",children:[(0,m.jsxs)("p",{className:"font-medium",children:[e.sector,":"]}),(0,m.jsx)("p",{className:"text-blue-700",children:e.impact})]},t)))}):null,g=e=>e&&e.length?(0,m.jsx)("div",{className:"space-y-2",children:e.map(((e,t)=>(0,m.jsxs)("blockquote",{className:"border-l-4 border-blue-300 pl-4 ml-4 italic text-blue-700",children:['"',e,'"']},t)))}):null,h=(e,t)=>!!e[t]&&("null"!==e[t]&&((!Array.isArray(e[t])||0!==e[t].length)&&("object"!==typeof e[t]||0!==Object.keys(e[t]).length)));return(0,m.jsxs)("div",{className:"p-8 max-w-6xl mx-auto bg-blue-50 min-h-screen",children:[(0,m.jsx)("h1",{className:"text-3xl font-bold mb-8 text-center text-blue-800",children:"AI Submissions Analysis"}),Object.entries(p).map((a=>{let[i,p]=a;return(0,m.jsxs)("div",{className:"mb-6 bg-white rounded-lg shadow-lg overflow-hidden",children:[(0,m.jsx)("div",{className:"bg-blue-300 p-4",children:(0,m.jsxs)("button",{onClick:()=>(e=>{t((t=>({...t,[e]:!t[e]})))})(i),className:"flex items-center justify-between w-full text-left",children:[(0,m.jsxs)("div",{children:[(0,m.jsx)("h2",{className:"text-xl font-semibold text-blue-900",children:"null"===p.organization_name?"Unnamed Organization":p.organization_name}),(0,m.jsx)("p",{className:"text-sm text-blue-700",children:"null"===p.organization_type?"Type not specified":p.organization_type})]}),e[i]?(0,m.jsx)(c,{size:24}):(0,m.jsx)(d,{size:24})]})}),e[i]&&(0,m.jsxs)("div",{className:"p-4 space-y-4",children:[(0,m.jsxs)("div",{className:"space-y-2",children:["null"!==p.classification&&(0,m.jsxs)("p",{children:[(0,m.jsx)("span",{className:"font-semibold",children:"Classification:"})," ",p.classification]}),"null"!==p.overall_position&&(0,m.jsxs)("p",{children:[(0,m.jsx)("span",{className:"font-semibold",children:"Position:"})," ",p.overall_position]}),"null"!==p.international_alignment&&(0,m.jsxs)("p",{children:[(0,m.jsx)("span",{className:"font-semibold",children:"International Alignment:"})," ",p.international_alignment]}),"null"!==p.tone&&(0,m.jsxs)("p",{children:[(0,m.jsx)("span",{className:"font-semibold",children:"Tone:"})," ",p.tone]})]}),[{key:"arguments",title:"Arguments"},{key:"counterarguments",title:"Counter-Arguments"},{key:"key_recommendations",title:"Key Recommendations"},{key:"risks_and_challenges",title:"Risks and Challenges"},{key:"safeguards_and_mitigations",title:"Safeguards and Mitigations"},{key:"values",title:"Values"}].map((e=>{return h(p,e.key)&&(0,m.jsxs)("div",{className:"border-t border-blue-100 pt-2",children:[(0,m.jsxs)("button",{onClick:()=>o(i,e.key),className:"flex items-center justify-between w-full text-left font-medium text-blue-700 hover:text-blue-500",children:[(0,m.jsx)("span",{children:e.title}),n[`${i}-${e.key}`]?(0,m.jsx)(c,{size:20}):(0,m.jsx)(d,{size:20})]}),n[`${i}-${e.key}`]&&(0,m.jsx)("div",{className:"mt-2",children:(t=p[e.key],t&&t.length&&"null"!==t[0]?(0,m.jsx)("ul",{className:"list-disc ml-6 space-y-1",children:t.map(((e,t)=>(0,m.jsx)("li",{className:"text-blue-800",children:e},t)))}):null)})]},e.key);var t})),[{key:"examples",title:"Examples",renderer:s},{key:"stakeholders",title:"Stakeholders",renderer:r},{key:"sector_impacts",title:"Sector impacts",renderer:l},{key:"quotes",title:"Quotes",renderer:g}].map((e=>h(p,e.key)&&(0,m.jsxs)("div",{className:"border-t border-blue-100 pt-2",children:[(0,m.jsxs)("button",{onClick:()=>o(i,e.key),className:"flex items-center justify-between w-full text-left font-medium text-blue-700 hover:text-blue-500",children:[(0,m.jsx)("span",{children:e.title}),n[`${i}-${e.key}`]?(0,m.jsx)(c,{size:20}):(0,m.jsx)(d,{size:20})]}),n[`${i}-${e.key}`]&&(0,m.jsx)("div",{className:"mt-2",children:e.renderer(p[e.key])})]},e.key))),(0,m.jsx)("div",{className:"mt-4 pt-2 border-t border-blue-100",children:(0,m.jsxs)("a",{href:`/api/files/${encodeURIComponent(i)}`,target:"_blank",rel:"noopener noreferrer",className:"inline-flex items-center text-blue-600 hover:text-blue-700",children:["View Full Document ",(0,m.jsx)(u,{size:16,className:"ml-1"})]})})]})]},i)}))]})};const h=function(){return(0,m.jsx)("div",{className:"App",children:(0,m.jsx)(g,{})})},f=e=>{e&&e instanceof Function&&n.e(453).then(n.bind(n,453)).then((t=>{let{getCLS:n,getFID:a,getFCP:i,getLCP:o,getTTFB:s}=t;n(e),a(e),i(e),o(e),s(e)}))};i.createRoot(document.getElementById("root")).render((0,m.jsx)(a.StrictMode,{children:(0,m.jsx)(h,{})})),f()})();
//# sourceMappingURL=main.ad437247.js.map